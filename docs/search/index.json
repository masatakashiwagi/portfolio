[{"content":"はじめに 以前から気になっていたOSSの Airbyte というELに特化したData Integrationツールを使ってみたかったので，今回はこれを使って以前Embulkで実装していたスプレッドシートからBigQueryへのデータ同期処理と同じことができるか試してみた話になります．\n Airbyteは良い感じのUIがあるので，UIをポチポチしながら設定していきます．\nAirbyteとは？  AirbyteはOSSのETLツールですが，特にExtractとLoadに注力しているツールになります．豊富なデータソース（Source）とターゲットソース（Destination）に対応していて，これらを設定することでデータを簡単に連携することができます．Transform部分は内部的にはdbtを使ってハンドリングしているみたいです．（Transformations with SQL (Part 1/3)）\n提供形態としては，OSSとマネージドサービス（クラウド版: 有料）があり，ローカルをはじめ，AWS/GCP/Azureと各種クラウドサービスでデプロイすることができます（参考: Deploying Airbyte Open Source）．\nConnectorは既に用意されているもの（airbyte/airbyte-integrations/connectors/）もあれば，独自で作成することもできます．\nスプレッドシートからBigQueryに連携 基本的には，Tutorialに沿って進めていきます．まずはgit cloneしてUIを立ち上げます．\ngit clone https://github.com/airbytehq/airbyte.git cd airbyte docker compose up  docker composeを実行したら，http://localhost:8000 でUIにアクセスすることができます．\nここからはUIの世界で全て完結することができます！\nSource 案内に従って進めると，まずプルダウンから今回のデータソースであるGoogle SheetsをSource typeとして選択します．右側にSetup Guideがあるので，見ながら設定できて親切設計だなと感じました．\n以下の設定を埋めていきます．\n Source name  「Google Sheets」としています   Authentication  GCPのサービスアカウントを設定します．JSONファイルの中身をコピーして貼り付けます cat ~/.gcp/hoge_service_account.json | pbcopyでクリップボードにコピーするとやりやすいです   Row Batch Size  デフォルトの200にしています   Spreadsheet Link  対象となるスプレッドシートのURLを設定します 事前にサービスアカウントでのアクセスを許可しておく必要があります    Destination 次に，プルダウンからターゲットソースであるBigQueryをDestination typeとして選択します．\n以下の設定を埋めていきます．\n Destination name  「BigQuery」としています   Project ID  GCPにアクセスして現在使っているプロジェクトIDを設定します   Dataset Location  「US」で良さそう？   Default Dataset ID  BigQueryのデータセットとして作成されるIDになります   Loading Method  Standard InsertsとGCS Stagingの2種類あります Standard Inserts  SQL INSERTで直接アップロードする方法で，非効率的なためGCS Stagingを推奨しています（今回はこちらを選択）   GCS Staging  ファイルにレコードを書き込み，そのファイルをGCSにアップロードし，その後COPY INTOテーブルを使用してファイルをアップロードする方法（GCSのバケットなどの情報が必要になります）     Service Account Key JSON (Required for cloud, optional for open-source)  GCPのサービスアカウントを設定します．JSONファイルの中身をコピーして貼り付けます   Transformation Query Run Type (Optional)  interactiveとbatchの2種類あります   Google BigQuery Client Chunk Size (Optional)  デフォルトの15にしています    Connection 最後に，Connectionのセットアップを行います．設定したSourceとDestinationをセットし，Replication frequency（同期頻度）, Destination NamespaceやPrefixなどを決めていきます．\n Transfer  Replication frequency  手動実行やスケジュール実行を選択できる     Streams  Destination Namespace  Mirror source structure, Destination default, Custom formatの3種類あります   Destination Stream Prefix (Optional)  必要に応じて付与します     Normalization \u0026amp; Transformation  Raw data (JSON) Normalized tabular data 上記どちらかを選択しますが，Raw dataだとJSONのままデータが格納されるので，Normalized tabular dataで良いと思います    個人的に同期するスプレッドシートの各シートがそれぞれ表示されて，どれを同期するか選択して決められるというのが感動しました 🎉\n今回は1シートだけ連携することにし，Custom Transformの処理はせずに単純にデータをそのまま連携していきます．\n実行結果のログは以下のような感じSync Succeededとなっています．\nAirbyte側は大丈夫そうなので，BigQueryの方も確認してみると，ちゃんと入ってるので問題なさそうです！\nおわりに 今回は，OSSのAirbyteを使ったスプレッドシートからBigQueryへのデータ同期を行ってみました．AirbyteはUIが用意されていて，直感的に操作できる+設定も簡単でデータ同期の体験としてとても良かったです！データソースが豊富なのもメリットとして大きいと思います．\nExtract \u0026amp; Loadのみしか使えていないので，次はdbtを理解してTransformも追加して処理を実行してみたいと思います．あとは他のデータパイプラインツールとの比較とかも出来たら楽しそうかなと思いました．\n","date":"2022-09-29","permalink":"https://masatakashiwagi.github.io/portfolio/post/data-integration-airbyte/","tags":["Dev","Data"],"title":"AirbyteでスプレッドシートのデータをBigQueryに連携"},{"content":"はじめに きっかけは@kaggle_araisanさんのこちらのTweetを見かけて，面白そうな取り組みだなーと感じたので，この機会に僕もやってみようと思ったのが始まりです！\nこういうのは，誰かと一緒にやったり，みんなが見ているところで宣言した方が続けられると思うので，僕も乗っかる形でTweetしました．（やると言った手前途中で投げ出せない力学に持っていくやり方笑）\n夏休みでいい機会なのでTry something new for 30 daysを再開してみようと思いました\n\u0026mdash; Hidehisa Arai (@kaggle_araisan) August 10, 2022  期間は，8月10日から31日までの22日間で，毎日違ったテック記事や書籍または論文などを読みました．\n実際のコンテンツは以下のリンクから眺めて頂ければと思います．\n 8月末までのテック備忘録日記  今回はNotionでページを作ってそれをシェアする形で公開しています．この辺りNotionは簡単に外部公開できるのと，階層でページを増やしていけるのでコンテンツを集約しやすいなと常々思っています👏\nやってみた感想 率直に言うと結構しんどかった笑，でもインプットを集中して出来た部分もあり楽しかったです！\n子供がまだ小さいというのもあり，育児があったりで上手く時間を捻出するのが難しいタイミングも正直ありました😇\nコンテンツとしては，最近自分が興味ある領域「MLOps, レコメンド, 検索」を中心に技術記事などを読んだのですが，ストックしていて読めていなかった記事や積読していた書籍を読み始めることができたといった面でプラスだったなと感じています．忙しくて後回しになってしまいがちですが，この業界は移り変わりも激しいので，常にある程度の最新の情報をインプットしておかないとなーと思うところもあるので，集中して取り組めたのは良かったかなと思います．\n一方で，勢いで始めた部分もあったので，1日で読めるぐらいの程良いコンテンツを探すのに時間をくった部分もありました．あとは，1日で完結するところにこだわってしまったので，論文や書籍などでページ数がそれなりにあるものをしっかり読む！といったことが上手くできなかったので，そこは反省かなと思っています．\nそういった場合は，数日に分けて読んだ部分までの紹介や感想を書けば良いかなと思うので，次回はその辺も考えて取り組みたいと思います．今回の取り組みでは読めていない記事でかなりの分量のものもあったので，その辺りも読んでいけたらと思いました．\nあと，Tweetもしましたが，今回はインプットしか出来ていない状態で，手を動かしてゴニョゴニョするみたいなところはなかったので，次回はコードを書く系も取り入れて見るのも良さそうかなと感じた次第です😆\n最後にコンテンツの内訳は，大体以下のような感じになっています（タグは複数タグ付いているものもあります）．\n MLOps関連: 6~8件ぐらい レコメンド: 7件 検索: 4件  おわりに 今回は約20日ぐらい実施したわけですが，集中して似た記事を読んだりするとより考えが洗練されてきたり，理解が進みやすくなるので，短期集中でインプットするのも良いなと強く思いました！\nあとは，日常だと業務に追われる部分もあると思いますが，やっぱりインプットする時間は1日の中でもしっかり確保して割いた方が引き出しが増えたりするので，良いなと思います．\n次は冬休みぐらいにまた「2022年冬休みのテック備忘録日記」でやろうかなと思います笑\n","date":"2022-09-02","permalink":"https://masatakashiwagi.github.io/portfolio/post/summer-tech-diary-2022/","tags":["Poem"],"title":"2022年夏休みのテック備忘録日記"},{"content":"はじめに 少し時間が空いてしまいましたが，初めての育休（今は育業というみたいですがw）を2月末〜5月のGW明けまで2ヶ月ちょい取得した感想とオススメしたいものを紹介していきたいと思います！\n育児休業とは？ そもそも育児休業とは何かというと，厚生労働省のページを確認すると以下のように書かれています．\n 育児休業制度\n子が１歳（一定の場合は、最長で２歳）に達するまで（父母ともに育児休業を取得する場合は、子が１歳２か月に達するまでの間の１年間＜パパ・ママ育休プラス＞）、申出により育児休業の取得が可能 また、産後８週間以内の期間に育児休業を取得した場合は、特別な事情がなくても申出により再度の育児休業取得が可能＜パパ休暇＞\n 基本的には子供が1歳になるまでに取得することが可能で，この期間中は育児休業給付金が支給され，休業開始時のお給料の67％（休業開始から6か月経過後は50％）が支給されます．また，社会保険料も免除されます．社会保険料はその月に1日でも育業期間が被っていれば，その月の社会保険料が免除されるので月跨ぎで取得するのが個人的にオススメです！\nあとは育業期間後も育児と仕事の両立支援制度が用意されていて，特に子の看護休暇が時間単位で取得できるようになった（令和3年1月1日から）ので病院とかで抜ける場合に半日や1日全部休む必要がなくなり良いかなと思います．\nなぜ育休を取得しようと思ったのか そもそもなぜ育休を取得しようと思ったのか書いておくと，\n 赤ちゃんの成長はすごく早いので，小さい頃の姿をしっかり記憶に残したかった  既に忘れつつあります\u0026hellip;😇   初めての経験でもあるので，わからないことだらけの中，それを妻一人に任せっきりにするのは違うなという気持ち（もちろん全て一人でして貰うとかではないが，日中は全てお願いになってしまう） 妻自身がメンタル的に強い方ではないので，産後うつとかそういった話を聞くとメンタル面をしっかりサポートしてあげたいという気持ちがあった 自分が携わっているサービス柄，ママの気持ちだったりを少しでも理解することで今までとは違う視点で物事を見ることができるだろうし，サービス開発などにそういった部分を活かせたら良いなという漠然とした想いもあった  上記のような思いと，次の子供が奇跡的に産まれて来てくれるか分からないというのもあり，人生の中で一度は経験できると良いなと思って今回のタイミングで取得しました．この辺りは流産を2度経験していて，妊娠してから出産するまで容易なことではないというのを体感していたというのもあります．\n育休を取得してみて 育休期間中はTwitterで毎日一言日記を付けていたので，その当時の感想を知りたい方はそちらを見て頂ければと思います！\n育休1日目\n今日から育休がスタート！\n昨日までは妻実家で義父母も居たので、比較的余裕があったけど、今日からは2人だけで余裕が無くなってる。\n娘だけでなく、妻のケアもしていきたい。\n\u0026mdash; asteriam (@asteriam_fp) February 28, 2022  Podcastの方でも育休時の感想を話しているので，良ければこちらも聴いてみて下さい（宣伝です笑）！\n 育休を取得して思ったのは，取れる人は是非取って欲しいなという気持ちです．特に最初の1~2ヶ月は3時間おきにミルクを上げる必要があるため，夜中でも関係なくそれが毎日続くので体力的にもしんどいのと，初めてというのもあり，十二分に赤ちゃんの動きに対して反応してしまうので精神的にも疲れてしまうことがあり，これを1人でやり切るのは本当に大変だと思います．なので，夫婦で交代しながらミルクを上げたり，寝かしつけることができるとお互い少しは休める時間を取れるかなと思います．\n自分で子供を育ててみて（まだ数ヶ月ですが笑），改めて自分の親の偉大さを感じました笑\n育休期間を通してミルクを適温で調整する能力をかなり高めることは出来たかなと思います笑．湯冷しのお水と沸騰したお湯をいい感じに配合して，適温で100mlとか150mlのミルクを作ることが出来ます😆\nあとはTwitterでも書いたりしていましたが，新しい発見も多く，赤ちゃんの足の温度で眠たいかどうかを理解出来たりもします（足があったかくなるとそろそろ眠たくなるという合図）．\nオススメのもの紹介（順不同）  スワドル  生後数ヶ月の時に一番役に立ったのは，スワドルかなと思います．これはUetaさんのブログ記事で知ったのですが，買って最高に良かったです！赤ちゃんはモロー反射で抱っこした状態からベッドなりお布団に置いた時や寝ている時に起きてしまうことが多々あるので，これを着せて寝かすと腕が動かしづらくなり，モロー反射で起きないようになるのでオススメです！ LOVE TREE - スワドルアップ   ぴよログ  赤ちゃんの生活記録を付けるならぴよログがオススメです．これはミルクの量や睡眠時間，おしっこやうんちなどの赤ちゃんに関するログを取ることができます．最初はミルクの量や回数など成長の目安になる部分もあるので，これを履歴として確認できるのは良いと思います！ ぴよログ   BabyBoon  iOSでしか使えないみたいですが，BabyBoonはホワイトノイズを始め，30種類以上の環境音などの赤ちゃんにとって穏やかで快適な音があるので寝かしつけの際に重宝します！ BabyBoon   バウンサー  買ってはないのですが，知り合いから譲り受けたバウンサーもとても良かったです！常に抱っこしておくことは難しいこともあるので，バウンサーに置くと自動的に揺らしてくれるので新生児の時はとても助かりました．ただ，寝返りを打つようになってからは危ない場面もあり使わないようになりました．   ニオイポイ  オムツを処理したい場合に，そのままゴミ箱に入れると匂いが気になったり，普段のゴミと赤ちゃんのオムツゴミを分けたい時に，ニオイポイはとても使えると思います！カートリッジを買い足すことでゴミ袋は補充することが出来ます． Aprica - ニオイポイ   CuboAi  ベビーモニターは最近まで買うか悩んでいたのですが，少し高価ですが買いました！買った感想としてはもっと早く買っても良かったなーという感想です．子供が1人で先に別の部屋で寝かす時に（親はまだ寝る時間じゃない時），例えば仰向けで寝ていると顔が布団で覆われていたり，心配な場面が出ると思うのですが，ベビーベッドに取り付けることで寝ている様子がハッキリとわかるので安心して他のことが出来るようになりました！ CuboAi    おわりに 今5ヶ月ほど経過しましたが，毎日ハイハイでマットの上を歩き回っていて少しの間も目を離せない感じに成長してくれてます😅 まだ，自分で座ることはできないので，コテンとバランスを崩して倒れてしまって頭を打ったりするのをヒヤヒヤしながら見守ってます笑．あとは手足を活発に動かすので，たまに踵落としを食らわされたりもしています😇\nなにはともあれ健康にすくすくと成長してくれることを願ってます！\nP.S. 勤めている会社の関係もあり，今回育休を取得したことでテレビ朝日の取材を受ける機会がありました．オンラインでの取材だったのですが，ABEMA Morningの方でインタビューの様子が放送されたみたいで，探せばYouTubeにアップされたその映像を見れると思います笑．あとは記事でも一部取り上げて頂いたみたいです．\n","date":"2022-07-26","permalink":"https://masatakashiwagi.github.io/portfolio/post/first-childcare-leave/","tags":["Poem"],"title":"初めての育児休業（休暇）を取得した感想"},{"content":"はじめに 2ヶ月以上ご無沙汰になってしまいましたが，久しぶりのテックブログになります．今回はタイトルにもあるように，Github ActionsとCodePipelineを使ってマージトリガーでStep Functionsのパイプラインを動かすCI/CDを構築したお話になります．\n今回のモチベーションは，3つほどあります．\n ML系のモデル学習パイプラインの構築とDryRun的なものを毎回手動で実行するのがいいかげんめんどくさくなってきた 人数が少ないMLチームだと担当者が対応できない場合に，属人化したものを代わりにオペレーションするのが大変でオペミスが発生する可能性がある  この辺りはドキュメント整備やチーム内での共有といった部分を整理しておく必要があるのは理解しつつ\u0026hellip;   CI/CD周りの設定含めてもう少し知識を付けてMLOpsのレベルを上げたかった  ML系プロジェクトにおいて，CI/CD整備の優先度が低かったり，そもそもソフトウェアエンジニアに比べてこの辺りの経験や知識が豊富でないということで後回しにされがちですが，MLOpsを考える上でCI/CDは大事なファクターの1つなのでしっかり取り組むべきだと思います（CI/CDの自動化はGoogleが定義しているMLOps level 2: CI/CD pipeline automationに相当する部分）．また，少人数チームの場合は尚のこと，人手をかけられない+属人化を排除する意味でも取り入れていくのが良いかなと思います．\n以下のリポジトリにソースコードなどを置いてあります．\n CI/CDパイプラインの構成 ※ はじめに，CodePipelineの設定やIAMロールの必要な権限は詳細に説明しないのでご了承くださいませ．公式ドキュメントや巷にある詳細な説明がされているブログをご覧下さい．\n今回のゴールはGithubのブランチマージから最終的にStep Functionsのパイプラインを動かすところまでになります．本来はStep Functions内でMLのモデル学習を行うパイプラインを構築しますが，今回はサンプルとしてSageMaker ProcessingJobを単発で動かすだけになります．\n今回構築したCI/CDパイプラインは以下のような感じになります．\nディレクトリ構成は以下になります．\n. ├── .github │ └── workflows │ └── sam-codepipeline.yaml ├── .gitignore ├── README.md ├── async-processing ├── cicd-pipeline │ ├── README.md │ ├── config │ │ ├── buildspec.yml │ │ └── dev-codepipeline-ver1.json │ ├── container │ │ ├── Dockerfile │ │ ├── app │ │ │ └── src │ │ │ ├── hello.py │ │ │ └── logger.py │ │ ├── docker-compose.yml │ │ ├── requirements.lock │ │ └── requirements.txt │ └── sam │ ├── env │ │ ├── dev │ │ │ ├── samconfig.toml │ │ │ └── template.yaml │ │ └── prod │ │ ├── samconfig.toml │ │ └── template.yaml │ └── statemachine │ └── sample-ml-pipelines-ver1.asl.json └── teamaya  流れを説明していくと，\n  Github Actionsパート\n sam-codepipeline.yaml name: sam-stepfunctions-codepipeline on: pull_request: branches: - dev - main types: [opened] # paths: # - 'cicd-pipeline/config/dev-codepipeline-ver1.json' # - './github/workflows/sam-codepipeline.yaml' workflow_dispatch: jobs: Build-Deploy-SAM: name: Build \u0026amp; Deploy SAM for Pipeline runs-on: ubuntu-latest timeout-minutes: 5 steps: - name: Checkout uses: actions/checkout@v2 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ap-northeast-1 # samによるdev/prod環境のAWSリソース更新 - name: Build SAM \u0026amp; Deploy SAM run: | if ${{ github.base_ref == 'dev' }}; then cd sam/env/dev sam build sam deploy --fail-on-empty-changeset --no-confirm-changeset elif ${{ github.base_ref == 'main' }}; then cd sam/env/prod sam build sam deploy --fail-on-empty-changeset --no-confirm-changeset else echo \u0026quot;Invalid branch name.\u0026quot; exit 1 fi Update-CodePipeline: name: Update Codepipeline for Step Functions runs-on: ubuntu-latest timeout-minutes: 5 needs: Build-Deploy-SAM steps: - name: Checkout uses: actions/checkout@v2 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ap-northeast-1 - name: Update Codepipeline run: aws codepipeline update-pipeline --pipeline file://config/codepipeline-ver1.json    Pull ReqestがOpenしたタイミングでGithub Actionsが走る AWS Serverless Application Model (SAM)のBuildとDeployを行う  マージ先のブランチに応じて，切り替わるようにしています．devマージではdev用のリソースを作成し，mainマージではprod用のリソースを作成します．   AWS CLIを使ってCodePipelineのaction configurationを更新する  Step FunctionsのStateMachineArnがMLのモデル学習のバージョンによって変更されることがあるので，後続のCodePipelineで動かす対象のStep Functionsを更新します．      CodePipelineパート\n buildspec.yml version: 0.2 env: variables: ENV: \u0026quot;dev\u0026quot; REPOSITORY_NAME: \u0026lt;ECRのリポジトリ名\u0026gt; IMAGE_TAG: \u0026quot;latest\u0026quot; REGION: \u0026quot;ap-northeast-1\u0026quot; parameter-store: AWS_ACCOUNT_ID: \u0026quot;/CodeBuild/common/AWS_ACCOUNT_ID\u0026quot; AWS_ACCESS_KEY_ID: \u0026quot;/CodeBuild/common/AWS_ACCESS_KEY_ID\u0026quot; AWS_SECRET_ACCESS_KEY: \u0026quot;/CodeBuild/common/AWS_SECRET_ACCESS_KEY\u0026quot; phases: pre_build: commands: # - echo Login to Docker # - docker login --username $AWS_ACCESS_KEY_ID --password $AWS_SECRET_ACCESS_KEY - echo Set ECR repository URI - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com - aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $REPOSITORY_URI build: commands: - echo Build started - echo Building the Docker Image - docker build -t $REPOSITORY_URI/$REPOSITORY_NAME:$IMAGE_TAG container post_build: commands: - echo Login to Amazon ECR - aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $REPOSITORY_URI - echo Pushing the Docker Image to ECR started - docker push $REPOSITORY_URI/$REPOSITORY_NAME:$IMAGE_TAG    PRがdev/mainにマージされたタイミングでAWSで事前に設定しているCodePipelineが走る  事前にCodePipeline上で組んでいるフローが動く   CodeBuildが起動し，Docker ImageのBuildが行われ，ImageをECRにpushします Step Functionsが起動し，パイプラインが走る  ECRに登録しているImageを使って，SageMaker ProcessingJobが動く     細かい部分で言うと，AWS_ACCOUNT_ID, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEYなどの機密情報は，パラメータストア(AWS Systems Manager Parameter Store)へ登録しておき，それを参照する形で使うようにしています．    AWS Serverless Application Model AWS Serverless Application Model (SAM)とはAWSでサーバーレスアプリケーションを簡単に構築することがフレームワークになります．CloudFormationの拡張でCloudFormationで利用できるリソースはSAMでも使用することができます．YAMLもしくはJSON形式のテンプレートを使って簡単に環境構築ができますし，CLIも提供されてます．詳しくは公式の「AWS Serverless Application Model (AWS SAM) とは」を見て頂ければと思います．\n設定するファイルは2つあります．\n samconfig.toml template.yaml  dev環境の設定ファイルを見ていくと，\n  samconfig.toml\n samconfig.toml version = 0.1 [default] [default.deploy.parameters] stack_name = \u0026quot;dev-sample-codepipeline\u0026quot; s3_bucket = \u0026quot;aws-sam-cli-managed-samclisourcebucket-dev-sample-codepipeline\u0026quot; s3_prefix = \u0026quot;dev-sample-codepipeline\u0026quot; region = \u0026quot;ap-northeast-1\u0026quot; confirm_changeset = true capabilities = \u0026quot;CAPABILITY_IAM\u0026quot; disable_rollback = true    こちらのファイルは1つのファイルにdevとprodの両方の設定を実装することもできますが，今回は環境毎にファイルを分けています．（後述するテンプレートと一緒に管理する必要があるが，微妙に環境毎で変数が違う部分もあるのでこのファイルも分けて管理した方が良いかなと思い分けています） [default.deploy.parameters]はsam deployコマンドが実行された時に渡される引数になります 注意としては，s3_bucketは事前に作成しておかないとデプロイした際にS3 Bucket does not exist.といったエラーが発生します．  S3には，ビルドしたテンプレートファイルとリソースファイルが保存されます      template.yaml\n template.yaml AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; Transform: AWS::Serverless-2016-10-31 Description: \u0026gt; Create Resource - StepFunctions - EventBridge Parameters: EnvironmentVariable: Description: 環境変数 Type: String Default: dev VersionVariable: Description: バージョン番号 Type: String Default: ver1 StepFunctionsExecutionRole: Description: Step Functionsの実行ロール Type: String Default: arn:aws:iam::\u0026lt;AWSアカウントID\u0026gt;:role/StepFunctionsExecutionRole SageMakerProcessingImage: Description: SageMakerのProcessingJobを動かすImage Type: String Default: \u0026lt;AWSアカウントID\u0026gt;.dkr.ecr.ap-northeast-1.amazonaws.com/\u0026lt;ECRのリポジトリ名\u0026gt;:latest Resources: # =======Step Functions for ProcessingJob======== # DevMLPipelinesStateMachine: Type: AWS::Serverless::StateMachine Properties: Name: !Sub ${EnvironmentVariable}-sample-ml-pipelines-${VersionVariable} DefinitionUri: ../../statemachine/sample-ml-pipelines-ver1.asl.json DefinitionSubstitutions: ProcessingJobRole: !Ref StepFunctionsExecutionRole ProcessingImage: !Ref SageMakerProcessingImage ProcessingEnvironment: !Ref EnvironmentVariable Role: !Ref StepFunctionsExecutionRole Events: Schedule: Type: Schedule Properties: Description: パイプライン用のスケジューラー Enabled: False Name: !Sub ${EnvironmentVariable}-sample-ml-pipelines-${VersionVariable} Schedule: \u0026quot;cron(0 16 * * ? *)\u0026quot;    JSONではなく，YAML形式で書けるので良きですね！ Parametersブロックでは，値を変数化できるので，共通設定やdev/prodで動的に変わる部分だったりを書いておくと使い回しやすいかなと思います． Resourcesブロックでは，Parametersブロックで定義した変数を!Refや!Subで使うことができます．  !Subは値の一部に変数を使用したい時に使うことができます．   Roleの設定もできますが，今回は事前に設定しておいたIAMロールを使用しています． 今回はStep Functionsだけを定義したので，定義ファイルsample-ml-pipelines-ver1.asl.jsonを次に見ていきます．    sample-ml-pipelines-ver1.asl.json\n sample-ml-pipelines-ver1.asl.json { \u0026quot;Comment\u0026quot;: \u0026quot;Sample ML pipelines\u0026quot;, \u0026quot;StartAt\u0026quot;: \u0026quot;SageMaker-Hello-World\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;SageMaker-Hello-World\u0026quot;: { \u0026quot;Comment\u0026quot;: \u0026quot;Hello Worldを出力する\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::sagemaker:createProcessingJob.sync\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;RoleArn\u0026quot;: \u0026quot;${ProcessingJobRole}\u0026quot;, \u0026quot;ProcessingJobName.$\u0026quot;: \u0026quot;States.Format('{}', $$.Execution.Name)\u0026quot;, \u0026quot;AppSpecification\u0026quot;: { \u0026quot;ImageUri\u0026quot;: \u0026quot;${ProcessingImage}\u0026quot;, \u0026quot;ContainerEntrypoint\u0026quot;: [ \u0026quot;python3\u0026quot;, \u0026quot;/opt/program/src/hello.py\u0026quot; ] }, \u0026quot;ProcessingResources\u0026quot;: { \u0026quot;ClusterConfig\u0026quot;: { \u0026quot;InstanceCount\u0026quot;: 1, \u0026quot;InstanceType\u0026quot;: \u0026quot;ml.t3.medium\u0026quot;, \u0026quot;VolumeSizeInGB\u0026quot;: 10 } }, \u0026quot;Environment\u0026quot;: { \u0026quot;PYTHON_ENV\u0026quot;: \u0026quot;${ProcessingEnvironment}\u0026quot; }, \u0026quot;StoppingCondition\u0026quot;: { \u0026quot;MaxRuntimeInSeconds\u0026quot;: 86400 } }, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;FailState\u0026quot; } ], \u0026quot;End\u0026quot;: true }, \u0026quot;FailState\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Fail\u0026quot;, \u0026quot;Cause\u0026quot;: \u0026quot;Error\u0026quot;, \u0026quot;Error\u0026quot;: \u0026quot;Error\u0026quot; } } }    JSON形式で書かれたStep Functionsの定義ファイルになります． Hello Worldを出力するだけの内容になっていますが，それをSageMaker ProcessingJobで動かしています．今回は単純な処理ですが，MLモデルを構築するためのパイプラインをここに実装すれば，その内容がSAMで構築されます． パイプラインを構築する際は，Step FunctionsのWorkflow studioで直感的なGUIで簡単に作成できるので，それで作成した後にJSONの定義ファイルをDLすれば同じものを本番環境用にサクッと構築することができます． ProcessingJobを動かすImageはCodeBuild時にECRにpushしたものを使っています． また，ProcessingJobNameは一意でないとエラーになるので，Contextオブジェクトの$$.Execution.Nameを使用しています．    AWS CodePipeline CodePipelineは複数のステージというものが用意されていて，それを繋ぎ合わせて一連のパイプラインを構築しCI/CDを自動化するものになります．詳しくは公式の「AWS CodePipeline とは」を見て頂ければと思います．\n今回の場合，Source→Build→Execute(Invoke)のような流れになっています．\n Source: Githubのブランチマージトリガーで起動する Build: Docker ImageのBuildとECRへのPushを行う Execute(Invoke): Step Functionsを起動する   処理が正常に終了すると，Step Functionsの実行結果とCloudWatch Logsのログは以下のようになります．   CodePipelineを動かす時の注意としては，権限周りのエラーがよく発生するのでCodePipelineから何を動かす必要があるかをチェックして動かすアクションの権限を与えてやる必要があります．  今回の場合:  CodePipelineでは，CodeBuildとStep Functionsを動かすためのポリシーが必要 CodeBuildでは，ECRとSystemsManagerを操作するためのポリシーが必要 Step Functionsでは，SageMakerの操作とECRへのアクセスを行うポリシーが必要   エラー周りは参考に挙げたブログが役に立ちました．    おわりに 今回はGithub ActionsとCodePipelineを使ってStep Functionsを動かすCICDパイプラインを構築したお話でした．\nStep Functionsの中身をMLモデル学習のパイプラインとすれば，Github ActionsのPR Openとブランチマージをトリガーとして，CodePipelineが走ることで，一連の流れをCICDで実現できることになります．\nここにテストを追加したり，例えば，ECSでML-APIが動いている場合には，Step Functionsのパイプラインが正常終了した後に，承認プロセスを入れてECSのタスク定義とサービスの更新を入れることで，デプロイまで持っていくことができるかなと思います！\nもう少し発展させることでより良い開発体験が生み出すことができるのと，MLOpsの成熟度も上がって運用の自動化も一歩前進すると考えています．\n参考  CodeBuild・CodePipelineを使ってデリバリーパイプラインを導入（AWSでWebアプリ構築 part3） Solve - Policy has Prohibited field Principal AWS Error Troubleshooting AWS CodeBuild Troubleshooting CodePipeline  ","date":"2022-07-18","permalink":"https://masatakashiwagi.github.io/portfolio/post/implement-cicd-github-actions-codepipeline-for-stepfunctions/","tags":["Dev","AWS","MLOps"],"title":"Github ActionsとCodePipelineを使ってStep Functionsを動かすCI/CDを構築"},{"content":"はじめに 2週間ぐらい間が空きましたが，先月の4月19日にこちらのMLOps勉強会で登壇させて頂いた時の感想ポエムを書こうと思います．\nところでMLOps勉強会とは？\nコミュニティーの説明を引用させて貰うと\u0026hellip;\n AI/機械学習技術をビジネス適用した後に必要となる多くのチャレンジについて理解を深めていく事、また業界横断の技術コミュニティを作ることを目的としています。\nまた、先駆けてMLOpsに取り組まれている事例の講演など、MLOpsに関する最先端の情報を共有する会を開催していきます。\n 日本でのMLOpsに関する事例などをそれらに取り組んでいる方々と共有する会で，今回僕は以前のブログポストで紹介した「MLOps Practices」という日本でのMLOpsの事例を収集して整理したWebサイトを公開したという内容で登壇させて頂きました．\nこのブログを書いてTwitterで発信したところ想定以上の反応を頂いて，その中で運営の方からお声がけ頂いて登壇する流れになりました．\n 登壇前まで まず登壇までの話をしておくと，お話を頂いてから登壇日までは十分時間があったものの，丁度育休中だったこともあり，育児やら家事やらで思ったよりもあっという間に1日が過ぎて行って，資料の構成だったりスライドの作成だったりの時間を確保するのが難しく，気づけば登壇日まであと1週間になってた時は少し焦りました😅\nこの辺は毎日少しづつ時間を確保すれば良かったなーと計画性の無さを実感しました\u0026hellip;（余談ですが，実は個人プロジェクトのコードを書くことに夢中になっててスライド作成に取り掛かれてませんでしたw）\nでも，我ながら生後1~2ヶ月の子供が居る中で妻と交代しながらの育児とはいえ，登壇承諾して資料作成とかして頑張ったなーと思いました笑\n登壇してみて 当日はオフラインで18時開催だったので，それまでに子供をお風呂に入れて事前準備を終えたり，登壇中の育児は妻にお願いしたりして対応していました．\n小さいお子さんが居る家庭は似たような感じなのか気になったので，誰かまたお話し聞かせて下さいー！\nさてさて実際に登壇した感想としては，オンライン発表はこちらが一方的に喋る感じになるので，聴いてくれている人の顔や反応を見ることができないのが残念だし難しいなと感じました．自己紹介の一発目にボケをかましたので，皆さんの失笑具合を見れなかったのは残念です🤣\nあとどうしても淡々と話す感じになるので，話すスピードが速かったりとかに気づきにくいとも思いました．発表はオフラインでしたいけど，小さい子供が居ると今回のようなオンラインで助かるなとも思い，難しいですね．\n内容に関しては，実務での取り組みとは少し違う部分がメインになるので，聴いている人の興味としてどうなのかな？と思い，おまけとして実務で取り組んでいる事例も軽く紹介する形にしました．メインパートは継続的な取り組みが必要とされるMLOpsにおいて，それらに興味がある人にとってどうゆうのを参考にすればいいかをサポートする形で海外の事例集を紹介しつつ，日本版でそういった事例集がないので，それを作りたい気持ちでWebsiteを作成して公開した話をする流れにしました．備忘録がてらに用意したもので色々と反響頂けたのは嬉しかったです！\nまた，発表後の質問やアンケート結果を見る限り，実際に皆さんからも好意的な内容が多く嬉しい気持ちでした！😆（受けた質問をメモしておけば良かったのですが，していなかったのでもし録画が公開されたらこのブログに回答する形で追記したいと思います．）\nおわりに 自分自身まだまだ満足したMLOpsの取り組みが出来ているわけではない \u0026amp; 知らない部分も多いので，他社さんの事例を参考にしながら，個人的に関心が強い「機械学習の社会実装と継続的な仕組み作り」にチャレンジして行ければと思います．\nそして，日本のMLOpsコミュニティー界隈を少しでも盛り上げたい気持ちから，この度MLOps勉強会の運営に参加させて頂くことになりました！情報発信や企画など進めて行ければと思うので，今後ともよろしくお願いします🙏\n参考  MLOps勉強会  ","date":"2022-05-04","permalink":"https://masatakashiwagi.github.io/portfolio/post/my-thoughts-on-speaking-at-the-mlops-study/","tags":["Poem"],"title":"MLOps勉強会で登壇した感想"},{"content":"はじめに Continuous Machine Learning (CML) という機械学習モデルのCIツールを個人プロジェクトで導入したので，その紹介をしようと思います．\n機械学習プロジェクトでテストを考える時に，大きく3つあると思っています．\n ソフトウェアのテスト: 単体テストや結合テストなどコードが意図通りの挙動を示すかどうかを確認するテスト  ツール: pytest, unittest \u0026hellip;など   機械学習モデルのテスト: 機械学習モデルが正常に動作するか，評価指標でスコアを得ることができるかなどを確認するテスト  ツール: CML (with DVC) \u0026hellip;など   データのテスト: データが想定しているスキーマに従っているか，データの分布や範囲が意図したデータかなどを確認するテスト  ツール: dbt, Dataform \u0026hellip;など    このうち今回は，「機械学習モデルのテスト」に着目してこれをどのように実施するかを紹介しています．\n※ 今回，DVCは使っていません．機械学習モデルは使用するデータに依存する部分がかなり大きいので，DVCと組み合わせて実施するのが望ましいですが，今回は簡易的なテストデータを用意して，CMLを実施しています．\nContinuous Machine Learning (CML) とは？ Continuous Machine Learning (CML) は，Iterative.aiが開発している機械学習プロジェクトのCI/CDを実現するOSSであり，特徴としては以下のような点があります．\n モデルの学習と評価の結果をレポート（markdown形式）にして自動生成できる（メトリクスや図など） Github Actionsと連携して，Pull Requests時に自動的に実行する仕組み 任意のクラウド環境で実験を実行することが可能  これは例えば，次のような課題を解決するためのサポートになると思います．\n PoCでデータサイエンティストが作成したコードをプロダクション環境に載せた際に，モデルの学習と評価が適切に行われているかをどのように確認するか？ モデルが前回から向上していることをどのように確認するか？（これについてはDVCとの連携も必要） モデルの再学習の際にも同じようなチェックが行われるのか？ etc\u0026hellip;  CMLはPRベースでモデルの学習と評価を行い，適切な意思決定に繋げるのに役立つツールだと思います．\n個人プロジェクトへの導入 前回のブログポストのこちらの章で少し触れた部分になります．\n機械学習によるモデル作成を行っているtasks.pyのif __name__ == \u0026quot;__main__\u0026quot;:以下がCML用に用意したコードになります．\n if __name__ == \"__main__\":以下のコードを抜粋 if __name__ == \u0026quot;__main__\u0026quot;: def plot_yy(y_valid, y_pred, metrics, savepath): \u0026quot;\u0026quot;\u0026quot;Vizualize the results using yy-plot \u0026quot;\u0026quot;\u0026quot; y_max = np.max(y_valid) y_min = np.min(y_valid) # calculate max and min of y_pred predict_y_max = np.max(y_pred) predict_y_min = np.min(y_pred) # use the smallest and largest value of either of both y_valid and y_pred # as the range of the vertical axis horizontal axis axis_max = max(y_max, predict_y_max) axis_min = min(y_min, predict_y_min) # margin of 5% of the length axis_max = axis_max + (axis_max - axis_min) * 0.05 axis_min = axis_min - (axis_max - axis_min) * 0.05 plt.figure(figsize=(10, 6)) plt.subplots_adjust(wspace=0.2, hspace=0.3) plt.scatter(y_pred, y_valid, c='r', s=50, zorder=2, edgecolors=(0, 0, 0), alpha=0.6) plt.plot([axis_min, axis_max], [axis_min, axis_max], c=\u0026quot;#1560bd\u0026quot;) plt.xlabel('Predict Values', fontsize=20) plt.ylabel('True Values', fontsize=20) plt.title(r'RMSE=%.2f' % (metrics), fontsize=15) plt.tick_params(labelsize=20) plt.tight_layout() plt.grid(True) plt.savefig(savepath, dpi=100, bbox_inches='tight', pad_inches=0.1) plt.close() def plot_residual(y_valid, y_pred, savepath): residual = y_pred - y_valid xmax = np.max(y_pred) + (np.max(y_pred) - np.min(y_pred)) * 0.05 xmin = np.min(y_pred) - (np.max(y_pred) - np.min(y_pred)) * 0.05 plt.figure(figsize=(10, 6)) plt.subplots_adjust(wspace=0.2, hspace=0.3) plt.scatter(y_pred, residual, c='r', s=50, zorder=2, edgecolors=(0, 0, 0), alpha=0.6) plt.hlines(y=0, xmin=xmin, xmax=xmax, color='#1560bd') plt.title('Residual Plot', fontsize=20) plt.xlabel('Predict Values', fontsize=20) plt.ylabel('Residuals', fontsize=20) plt.tick_params(labelsize=20) plt.tight_layout() plt.grid(True) plt.savefig(savepath, dpi=100, bbox_inches='tight', pad_inches=0.1) plt.close() params = { \u0026quot;model_id\u0026quot;: \u0026quot;sample_test\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;test_diabetes\u0026quot;, \u0026quot;features\u0026quot;: [\u0026quot;age\u0026quot;, \u0026quot;bmi\u0026quot;, \u0026quot;bp\u0026quot;, \u0026quot;s1\u0026quot;, \u0026quot;s2\u0026quot;, \u0026quot;s3\u0026quot;, \u0026quot;s4\u0026quot;, \u0026quot;s5\u0026quot;, \u0026quot;s6\u0026quot;], \u0026quot;target\u0026quot;: \u0026quot;target\u0026quot; } dataset_path = 'test/data/' + params['dataset_id'] + '.csv' df = pd.read_csv(dataset_path) result = train(df, params) rmse = result['metrics']['rmse'] # Record the metrics outfile = \u0026quot;data/metrics.txt\u0026quot; if not os.path.isdir(\u0026quot;data\u0026quot;): os.mkdir(\u0026quot;data\u0026quot;) with open(outfile, \u0026quot;w\u0026quot;) as f: f.write(\u0026quot;RMSE: \u0026quot; + f\u0026quot;{rmse:.2f}\u0026quot; + \u0026quot;\\n\u0026quot;) # Plot results y_valid = result['y_true'] y_pred = result['y_pred'] savepath_yy = \u0026quot;data/yy_plot.png\u0026quot; plot_yy(y_valid, y_pred, metrics=rmse, savepath=savepath_yy) savepath_residual = \u0026quot;data/residual_plot.png\u0026quot; plot_residual(y_valid, y_pred, savepath=savepath_residual)   こちらのコードがレポートにする処理になります．metrics.txtというテキストファイルを一時的に作成し，そこにメトリクスの結果を書き込みます．また，作成した回帰モデルによる実測-予測プロットの図（Y-Yプロット）や残差プロットの図をpngファイルでこちらも一時的に保存し，レポートに出力します．\n# Record the metrics outfile = \u0026quot;data/metrics.txt\u0026quot; if not os.path.isdir(\u0026quot;data\u0026quot;): os.mkdir(\u0026quot;data\u0026quot;) with open(outfile, \u0026quot;w\u0026quot;) as f: f.write(\u0026quot;RMSE: \u0026quot; + f\u0026quot;{rmse:.2f}\u0026quot; + \u0026quot;\\n\u0026quot;) # Plot results y_valid = result['y_true'] y_pred = result['y_pred'] savepath_yy = \u0026quot;data/yy_plot.png\u0026quot; plot_yy(y_valid, y_pred, metrics=rmse, savepath=savepath_yy) savepath_residual = \u0026quot;data/residual_plot.png\u0026quot; plot_residual(y_valid, y_pred, savepath=savepath_residual)  レポートに出力したいファイルを用意できたら，CMLを使うためにcml.yamlファイルを.github/workflows以下に作成します．公式のユースケースを参考にしても良いと思います．\n以下は，今回のプロジェクトで実行したCMLになります．\nname: train-my-model on: push: paths: - 'async-processing/app/consumer/tasks.py' pull_request: branches: - dev jobs: train-model: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: iterative/setup-cml@v1 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.8' - name: Train model env: repo_token: ${{ secrets.GITHUB_TOKEN }} S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }} S3_PATH_NAME: ${{ secrets.S3_PATH_NAME }} S3_MODEL_PATH_NAME: ${{ secrets.S3_MODEL_PATH_NAME }} run: | cd async-processing docker compose up -d docker compose exec -T consumer python3 consumer/tasks.py # Create CML report echo \u0026quot;## Metrics\u0026quot; \u0026gt;\u0026gt; report.md cat app/data/metrics.txt \u0026gt;\u0026gt; report.md echo \u0026quot;## Plots\u0026quot; \u0026gt;\u0026gt; report.md echo \u0026quot;### YY-plot\u0026quot; \u0026gt;\u0026gt; report.md cml-publish app/data/yy_plot.png --md --title 'YY Plot' \u0026gt;\u0026gt; report.md echo \u0026quot;### Residual-plot\u0026quot; \u0026gt;\u0026gt; report.md cml-publish app/data/residual_plot.png --md --title 'Residual Plot' \u0026gt;\u0026gt; report.md cml-send-comment report.md  CMLが実行されるタイミングとして，tasks.pyが変更された時とdevブランチにPRが作成された時の2つを設定していますが，こちらは適宜状況に合わせるのが良いと思います．今回は単純なRandomForestのモデルなので簡単に短時間で回すことができますが，画像系のモデル学習など学習に時間もリソースもかかる場合（特にクラウド環境でCMLを動かす場合），コード修正の度に実行されるのは適切でないかもしれません．\nrunパートは，docker compose upで立ち上げたコンテナ環境内でtasks.pyを実行して，出力されたテキストファイルと図をmarkdown形式のファイルに出力しています．ここで，echoを挟むことでheaderを付けたりもできます．\n cml-publish: レポートに画像を表示させるコマンド cml-send-comment: githubのPRにコメントととしてmarkdownレポートを作成するコマンド  参考: CML - Command Reference\n結果はこのような形のレポートになります．\n実際使ってみた感想 個人的に良いと思う部分と微妙だなと思う部分を挙げておきます．\n 良い点  作成した学習の結果や評価をPR上で議論できる点  認識のズレなどを議論できるかなと思います   モデルの結果を見て，デプロイするかどうかなど意思決定に繋げることができる点 再現性を一定担保することができる点   微妙な点  必要なものを出力してからレポート作成する必要がある点  結果のテキストファイルや図を出力しておく必要があるので，そこが面倒だったり，何を出すかの検討も必要   どういった基準でCMLを実行するか  どのタイミングでCMLを実行するか？（push時？merge時？） 重たいモデルを実行する場合，全てのデータで学習させてテストすべきか？ etc\u0026hellip;      微妙な点として挙げた内容も一部は，利用する側で決めるべきルールやポリシーだったりするので，ここはどういった情報があれば「意思決定」をする上で判断材料となるのかを整理することでクリアになる部分かもしれないです．\nおわりに 今回は，機械学習モデルのためのテストとして，CMLという機械学習プロジェクトでCI/CDを行うツールを使ってみたのでその紹介になります．\nまた，今回は使っていないですが，DVCという同じIterative.aiが開発しているデータのバージョン管理を行うツールをCMLと組み合わせて使う方法もあるので，ここも次回実験して使ってみたいと思います．これを使うことで前回の結果との差分なども見ることができるので，より良い「CI/CD for ML」が実現できるかなと思います．\nP.S. Twitterでコメント頂いたので，追記しておきます．\n確かに，モデルの学習をPush時やPR時に毎回回すのは大変なので，学習時に適用するのではなく，学習済みのモデルに対して適当なサブセットのデータを用意してそれに対する推論結果をレポート出力するのが軽量で試しやすそうだなと思いました．\nプルリクコメントでモデルの評価結果を出力してくれるGitHub Action。学習まで回すのは大変そうなので、サブセットの推論結果くらいに留めた方が良さそう？ https://t.co/5QkaHvQYxt\n\u0026mdash; ken_jimmy (@ken_jimmy) May 2, 2022 参考  Continuous Machine Learning (CML) Data Version Control (DVC) masatakashiwagi/teamaya/async-processing  ","date":"2022-05-01","permalink":"https://masatakashiwagi.github.io/portfolio/post/build-continuous-machine-learning/","tags":["MLOps","Machine Learning"],"title":"CMLを使ったMLモデルのCI/CD"},{"content":"はじめに 最近，機械学習を使ったアプリケーションのバックエンドでどういった処理を行ってモデル作成などを行っているか気になったので，モデル作成時によく行われる非同期処理をFastAPIとRabbitMQを用いて検証したお話になります．\n機械学習のようなモデル作成に時間がかかる場合，モデル作成を行うリクエストに対して，その情報を受け取ったというレスポンスだけを先に返し，実際の処理は非同期で行われることが多いと思います．この処理をRabbitMQというOSSのmessage queuing serviceを用いて実施した紹介になります．\nあと個人的にRPC (Remote Procedure Call) やPublish/Subscribeの仕組みを理解したいという気持ちもありました．\n以下のリポジトリにソースコードなどを置いてあります．\n 概要として，以下のような流れで処理を見ていきました．\n FastAPIにjson形式でリクエストをPOSTする（モデル作成するためのメッセージ情報） /trainというエンドポイントにまずはデータをPOSTする モデルの学習が行われモデルをS3に保存する モデルの作成が完了したら，次に/predictというエンドポイントにデータをPOSTする 学習済みのモデルをS3からロードし，POSTされたデータに対して予測確率を返す  Message Queuing Serviceとは？ メッセージキューはProducerと呼ばれるクライアントアプリケーションが作成したメッセージを受け取り，メッセージが溜まっていく仕組みです．Producer側から見ると，メッセージキューにメッセージを配信します．このメッセージを処理する役割として，Consumer (Worker) と呼ばれる別のアプリケーションがあり，ConsumerはQueueに接続し，処理するメッセージを受信します．処理し終わったら，返信用のメッセージをクライアント側に送信することもできます．（Queueに入れられたメッセージは，Consumerが取り出すまで保存されます．）\nまた，メッセージキューにはExchangeと呼ばれる機能があり，どのメッセージをどのように送るかを設定する機能もあります．（厳密には，Producerは直接Queueに送信するのではなく，Exchangeに送信することになります）\n実際にこのメッセージキューの役割を担うものをBrokerと呼んだりします．サービスとしてRabbitMQやRedisなどがあり，マネージドサービスではAmazon Simple Queue Service (SQS) があります．\n参考: What is message queuing?\nRabbitMQを使った実装 今回はRabbitMQを使って実装しました．RabbitMQはOSSのMessage Brokerで動作が速く軽量で，複数のメッセージングプロトコルをサポートしています．いくつかの言語で実装可能ですが，pythonで扱う場合には，pikaというライブラリを使うことになります．\n例えば，Celeryのような分散タスクキューツールを使うことで非同期処理をより簡単に実装できますが，Celery自体はメッセージキューを構築することはできないため，RabbitMQやRedisのようなBrokerが必要になります．今回はこのあたりのpub/subの仕組みを理解するためにCeleryは使わずにRabbitMQのpythonライブラリであるpikaを使って実装することにしました．\nRabbitMQはdockerコンテナで立ち上げていて，definitions.jsonという定義ファイルを事前に用意することでそのスキーマに基づいてRabbitMQを立ち上げることができます．このファイルはコンテナ起動時に読み込まれることになります．\n definitions.json { \u0026quot;rabbit_version\u0026quot;: \u0026quot;3.9.14\u0026quot;, \u0026quot;rabbitmq_version\u0026quot;: \u0026quot;3.9.14\u0026quot;, \u0026quot;product_name\u0026quot;: \u0026quot;RabbitMQ\u0026quot;, \u0026quot;product_version\u0026quot;: \u0026quot;3.9.14\u0026quot;, \u0026quot;users\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;guest\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;guest\u0026quot;, \u0026quot;hashing_algorithm\u0026quot;: \u0026quot;rabbit_password_hashing_sha256\u0026quot;, \u0026quot;tags\u0026quot;: \u0026quot;administrator\u0026quot;, \u0026quot;limits\u0026quot;: {} } ], \u0026quot;vhosts\u0026quot;: [{ \u0026quot;name\u0026quot;: \u0026quot;/\u0026quot; }], \u0026quot;permissions\u0026quot;: [ { \u0026quot;user\u0026quot;: \u0026quot;guest\u0026quot;, \u0026quot;vhost\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;configure\u0026quot;: \u0026quot;.*\u0026quot;, \u0026quot;write\u0026quot;: \u0026quot;.*\u0026quot;, \u0026quot;read\u0026quot;: \u0026quot;.*\u0026quot; } ], \u0026quot;topic_permissions\u0026quot;: [], \u0026quot;parameters\u0026quot;: [], \u0026quot;global_parameters\u0026quot;: [], \u0026quot;policies\u0026quot;: [], \u0026quot;queues\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;queue.model.train\u0026quot;, \u0026quot;vhost\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;durable\u0026quot;: true, \u0026quot;auto_delete\u0026quot;: false, \u0026quot;arguments\u0026quot;: { \u0026quot;x-queue-type\u0026quot;: \u0026quot;classic\u0026quot; } }, { \u0026quot;name\u0026quot;: \u0026quot;queue.model.predict\u0026quot;, \u0026quot;vhost\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;durable\u0026quot;: true, \u0026quot;auto_delete\u0026quot;: false, \u0026quot;arguments\u0026quot;: { \u0026quot;x-queue-type\u0026quot;: \u0026quot;classic\u0026quot; } } ], \u0026quot;exchanges\u0026quot;: [], \u0026quot;bindings\u0026quot;: [] }   RabbitMQには丁寧なTutorialsがあるので，それを読むと理解が進むと思います！\nシステム構成 非同期処理を行うシステムの構成は図のようになります．Producer/Broker/Consumerとコンテナを3つ用意しています．\n図の右下にあるResult Storesはタスクの処理結果を保存するためのものになります．Result StoresにはPostgreSQLやMySQLなどのDBを使用することもできますし，Redisも使用することができます．RedisはBrokerとしても使用することができるので，両方を1つで担うことが可能です．今回はモデルをS3に保存するだけとして，処理結果をDBに保存したりはしていないです．\n Producer: 機械学習タスクを行うためにメッセージをポストするコンテナ（=FastAPI） Broker: メッセージキューの役割を担うコンテナ（=RabbitMQ） Consumer: タスクを実際に実行するコンテナ Storage: モデルを保存するストレージ（=S3）  docker-compose.ymlは以下のような構成になります．\n docker-compose.yml version: '3.8' # Common definition x-template: \u0026amp;template volumes: - ~/.gcp:/root/.gcp:cached - ~/.aws:/root/.aws:cached - ./app:/opt/program:cached env_file: - .env environment: TZ: Asia/Tokyo LANG: 'ja_JP.UTF-8' restart: always tty: true services: producer: # FastAPI for producer container_name: producer build: context: . ports: - 5000:5000 command: [\u0026quot;uvicorn\u0026quot;, \u0026quot;main:app\u0026quot;, \u0026quot;--reload\u0026quot;, \u0026quot;--host\u0026quot;, \u0026quot;0.0.0.0\u0026quot;, \u0026quot;--port\u0026quot;, \u0026quot;5000\u0026quot;, \u0026quot;--access-log\u0026quot;] depends_on: - rabbitmq \u0026lt;\u0026lt;: *template consumer: container_name: consumer hostname: consumer build: context: . command: [\u0026quot;python3\u0026quot;, \u0026quot;consumer/consumer.py\u0026quot;, \u0026quot;--num_threads\u0026quot;, \u0026quot;2\u0026quot;] depends_on: - rabbitmq \u0026lt;\u0026lt;: *template rabbitmq: image: rabbitmq:3.9-management container_name: rabbitmq hostname: rabbitmq restart: always volumes: # - ./app/rabbitmq/etc:/etc/rabbitmq/rabbitmq - ./app/rabbitmq/etc/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf - ./app/rabbitmq/etc/definitions.json:/etc/rabbitmq/definitions.json - ./app/rabbitmq/data:/var/lib/rabbitmq - ./app/rabbitmq/logs:/var/log/rabbitmq - ~/.aws:/root/.aws:cached ports: # AMQP protocol port - 5672:5672 # HTTP management UI - 15672:15672 environment: TZ: Asia/Tokyo LANG: 'ja_JP.UTF-8' env_file: - .env # networks: # default: # external: # name: teamaya-network-async   今回のシステムのディレクトリ構成は以下になります．少し冗長な構成になっていますが，./app/producerと./app/consumer配下にProducerとConsumerの処理を行うスクリプトがあります．\n. ├── Dockerfile ├── README.md ├── app │ ├── consumer │ │ ├── base.py │ │ ├── consumer.py │ │ └── tasks.py │ ├── logger.py │ ├── main.py │ ├── producer │ │ ├── base.py │ │ ├── producer.py │ │ └── schema.py │ ├── rabbitmq │ │ └── etc │ │ ├── definitions.json │ │ └── rabbitmq.conf │ └── test │ ├── __init__.py │ ├── conftest.py │ ├── data │ │ └── test_diabetes.csv │ └── unit │ ├── __init__.py │ └── test_tasks.py ├── docker-compose.yml ├── requirements.lock └── requirements.txt  Producerの実装 それぞれのファイルの説明をしておくと，\n base.py: RabbitMQに接続するための初期化やConsumerにメッセージを送信するための処理を実装したファイル producer.py: base.pyのクラスを継承して，個別のタスクに合わせて送信するメッセージの実行するAPIを実装したファイル schema.py: データの入出力のスキーマを定義したファイル  FastAPIでは入出力をPydanticというライブラリを用いてData validationを行います．型ヒントを利用するためのスキーマ定義になります     producer.py import ast import uuid from fastapi import APIRouter from logger import get_logger from producer.base import BaseProducer, QueueNames, RepQueueNames from producer.schema import ApiSchemaPredict, ApiSchemaTrain, ProducerResult LOGGER = get_logger() router = APIRouter(prefix='', tags=[\u0026quot;producers\u0026quot;]) class ProducerTrain(BaseProducer): def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames): BaseProducer.__init__(self, queue_name, rep_queue_name) def run(self, params: ApiSchemaTrain): \u0026quot;\u0026quot;\u0026quot;Run to send message to train consumer Args: params (ApiSchemaTrain): schema for train \u0026quot;\u0026quot;\u0026quot; model_id = str(uuid.uuid4()) message = { \u0026quot;model_id\u0026quot;: model_id, \u0026quot;dataset_id\u0026quot;: params.dataset_id, \u0026quot;features\u0026quot;: params.features, \u0026quot;target\u0026quot;: params.target } # self.send_message_to_consumer(message) LOGGER.info(\u0026quot;Produce message for train.\u0026quot;) response = self.send_message_to_consumer(message) response = ast.literal_eval(response.decode()) LOGGER.info(f\u0026quot;Reply Response from consumer: {response}\u0026quot;) return ProducerResult(message=response) class ProducerPredict(BaseProducer): def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames): BaseProducer.__init__(self, queue_name, rep_queue_name) def run(self, params: ApiSchemaPredict): \u0026quot;\u0026quot;\u0026quot;Run to send message to predict consumer Args: params (ApiSchemaPredict): schema for predict \u0026quot;\u0026quot;\u0026quot; message = { \u0026quot;model_id\u0026quot;: params.model_id, \u0026quot;dataset_id\u0026quot;: params.dataset_id, \u0026quot;input_data\u0026quot;: params.input_data } LOGGER.info(\u0026quot;Produce message for predict.\u0026quot;) response = self.send_message_to_consumer(message) response = ast.literal_eval(response.decode()) LOGGER.info(f\u0026quot;Reply Response from consumer: {response}\u0026quot;) return ProducerResult(message=response) @router.post(\u0026quot;/train\u0026quot;, response_model=ProducerResult, name=\u0026quot;train\u0026quot;) async def train(params: ApiSchemaTrain) -\u0026gt; ProducerResult: \u0026quot;\u0026quot;\u0026quot;Train model\u0026quot;\u0026quot;\u0026quot; return ProducerTrain(queue_name='queue.model.train', rep_queue_name='queue.reply.train').run(params) @router.post(\u0026quot;/predict\u0026quot;, response_model=ProducerResult, name=\u0026quot;predict\u0026quot;) async def predict(params: ApiSchemaPredict) -\u0026gt; ProducerResult: \u0026quot;\u0026quot;\u0026quot;Predict model\u0026quot;\u0026quot;\u0026quot; return ProducerPredict(queue_name='queue.model.predict', rep_queue_name='queue.reply.predict').run(params)   ProducerTrainとProducerPredictはタスク実行用のメッセージを送るキューのqueue_nameとConsumer側からの返信用のキューであるrep_queue_nameの2つを引数に取ります．実際の学習や予測処理を行う部分はConsumer側で実装しています．\n base.py import json import os import uuid from typing import Literal import pika from logger import get_logger LOGGER = get_logger() # Possible values as queue name QueueNames = Literal['queue.model.train', 'queue.model.predict'] RepQueueNames = Literal['queue.reply.train', 'queue.reply.predict'] class BaseProducer: def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames): self.queue_name = queue_name self.rep_queue_name = rep_queue_name self.pika_params = pika.ConnectionParameters( host=\u0026quot;rabbitmq\u0026quot;, port=os.getenv('RABBITMQ_PORT', 5672), connection_attempts=10, heartbeat=0 ) self.connection = pika.BlockingConnection(self.pika_params) self.channel = self.connection.channel() LOGGER.info('Pika connection initialized.') result = self.channel.queue_declare(queue=self.rep_queue_name, exclusive=True) self.callback_queue = result.method.queue self.channel.basic_consume(queue=self.callback_queue, on_message_callback=self.on_response, auto_ack=True) def on_response(self, ch, method, props, body): if self.corr_id == props.correlation_id: self.response = body def run(self): raise NotImplementedError() def send_message_to_consumer(self, message: dict): \u0026quot;\u0026quot;\u0026quot;Send message Args: message (dict): message info \u0026quot;\u0026quot;\u0026quot; self.response = None self.corr_id = str(uuid.uuid4()) message_json = json.dumps(message) self.channel.basic_publish( exchange=\u0026quot;\u0026quot;, routing_key=self.queue_name, body=message_json, properties=pika.BasicProperties( content_type='application/json', delivery_mode=2, # make message persistent reply_to=self.callback_queue, correlation_id=self.corr_id ) ) LOGGER.info(f\u0026quot;Sent message. [q] '{self.queue_name}' [x] Body: {message_json=}\u0026quot;) while self.response is None: self.connection.process_data_events() self.close() return self.response def close(self): self.channel.close() self.connection.close()     __init__関数:\n RabbitMQのサーバーと接続するためにpika.BlockingConnection()でhost, portなどのパラメータを渡してインスタンス化を行う ConsumerからのReply用にrep_queue_nameに指定したキュー名でcallback_queueを作成 basic_consumeではsubscribeするキューが存在すればそれを実行    send_message_to_consumer関数:\n メッセージをjson.dumpし，basic_publishのbodyにつめてExchangeに送る    Consumerの実装 それぞれのファイルの説明をしておくと，\n base.py: RabbitMQに接続してキューにあるメッセージを受信し，処理を実行するベースファイル  callback部分はtasks.pyで実装しています   consumer.py: スレッド数を決めるnum_threadsをコマンドライン引数に取り，コンテナ上ではこのファイルが実行されます tasks.py: 機械学習によるモデル作成や学習済みモデルをロードして予測を行う処理を実装したファイル  callbackメソッドに実行したい処理を実装します if __name__ == \u0026quot;__main__\u0026quot;:以下にはContinuous Machine Learning (CML) で利用するCT用の処理を実装しています．（CMLに関しては別でブログを書こうと思います）     tasks.py import json import os import sys import traceback from typing import Any, Dict import matplotlib.pyplot as plt import numpy as np import pandas as pd import pika from logger import get_logger from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from base import BaseConsumer, EvalMetrics, QueueNames LOGGER = get_logger() S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME') S3_PATH_NAME = os.getenv('S3_PATH_NAME') S3_MODEL_PATH_NAME = os.getenv('S3_MODEL_PATH_NAME') class TrainConsumer(BaseConsumer): def __init__(self, queue_name: QueueNames): BaseConsumer.__init__(self, queue_name) def callback(self, ch, method, props, body): params = self.body2dict(body) payload = { 'status': 'TASK_RECEIVED', 'model_id': params['model_id'] } response = json.dumps(payload) ch.basic_publish( exchange='', routing_key=props.reply_to, properties=pika.BasicProperties(correlation_id=props.correlation_id), body=response ) # ch.basic_ack(delivery_tag=method.delivery_tag) self.download_from_s3(S3_BUCKET_NAME, S3_PATH_NAME, 'data/', params['dataset_id'] + '.csv') LOGGER.info(\u0026quot;Download dataset from S3.\u0026quot;) dataset_path = 'data/' + params['dataset_id'] + '.csv' df = pd.read_csv(dataset_path) LOGGER.info(\u0026quot;Read csv file and transform to dataframe.\u0026quot;) try: result = train(df, params) # save model model_path = 'data/model.pkl' self.save_model(result['model'], model_path) LOGGER.info(\u0026quot;Save trained model to local.\u0026quot;) # upload model to cloud storage model_id = params['model_id'] self.upload_to_s3(S3_BUCKET_NAME, S3_MODEL_PATH_NAME + f'{model_id}/', 'data/', 'model.pkl') LOGGER.info(\u0026quot;Upload trained model to S3.\u0026quot;) LOGGER.info(\u0026quot;TASK_COMPLETED\u0026quot;) except Exception as e: _, _, tb = sys.exc_info() LOGGER.error( f\u0026quot;Exception Error: {e} || Type: {str(type(e))} || Traceback Message: {traceback.format_tb(tb)}\u0026quot;) LOGGER.error(\u0026quot;TASK_ERROR\u0026quot;) class PredictConsumer(BaseConsumer): def __init__(self, queue_name: QueueNames): BaseConsumer.__init__(self, queue_name) def callback(self, ch, method, props, body): params = self.body2dict(body) model_id = params['model_id'] self.download_from_s3(S3_BUCKET_NAME, S3_MODEL_PATH_NAME + f'{model_id}/', 'data/', 'model.pkl') LOGGER.info(\u0026quot;Download model file from S3.\u0026quot;) model_path = 'data/model.pkl' model = self.load_model(model_path) LOGGER.info(\u0026quot;Load model for prediction.\u0026quot;) try: result = predict(model, params) payload = { 'status': 'TASK_COMPLETED', 'pred_proba': result['pred_proba'] } response = json.dumps(payload) except Exception as e: _, _, tb = sys.exc_info() LOGGER.error( f\u0026quot;Exception Error: {e} || Type: {str(type(e))} || Traceback Message: {traceback.format_tb(tb)}\u0026quot;) payload = { 'status': 'TASK_ERROR', 'pred_proba': None } response = json.dumps(payload) ch.basic_publish( exchange='', routing_key=props.reply_to, properties=pika.BasicProperties(correlation_id=props.correlation_id), body=response ) # ch.basic_ack(delivery_tag=method.delivery_tag) def train(df: pd.DataFrame, params: dict) -\u0026gt; Dict[str, Any]: \u0026quot;\u0026quot;\u0026quot;Train machine learning model (RandomForestRegressor) Args: df (pd.DataFrame): dataset for training model params (dict): parameters for training \u0026quot;\u0026quot;\u0026quot; features = params['features'] target = params['target'] X, y = df[features], df[target].values # train/test split X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42) LOGGER.info(\u0026quot;Start model training.\u0026quot;) # machine learning model: RandomForestRegressor reg_model = RandomForestRegressor(max_depth=3, random_state=42, n_estimators=100) reg_model.fit(X_train, y_train) LOGGER.info(\u0026quot;Model fit for training.\u0026quot;) # evaluate model pred = reg_model.predict(X_valid) # evaluate metrics eval_metrics = EvalMetrics() rmse = eval_metrics.rmse_score(y_valid, pred) LOGGER.info(\u0026quot;Evaluate metrics=RMSE for valid dataset : %.3f\u0026quot; % rmse) LOGGER.info(\u0026quot;Finish model training.\u0026quot;) result = { 'y_pred': pred, 'y_true': y_valid, 'metrics': {'rmse': rmse}, 'model': reg_model } return result def predict(model: object, params: dict) -\u0026gt; Dict[str, Any]: \u0026quot;\u0026quot;\u0026quot;Prediction for dataset using trained model Args: model (object): trained model params (dict): parameters for prediction Returns: float: predict probability \u0026quot;\u0026quot;\u0026quot; input_data = params['input_data'] pred_proba = model.predict(pd.DataFrame([input_data])) result = { 'pred_proba': pred_proba[0] } return result    学習パート  今回，機械学習モデルは何でもよかったので，RandomForestで回帰を行う処理にしています 学習済みモデルはS3に保存しているので，この処理を実行する場合は.envファイルに自身で利用しているAWSのバケット情報などを載せて下さい    S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME') S3_PATH_NAME = os.getenv('S3_PATH_NAME') S3_MODEL_PATH_NAME = os.getenv('S3_MODEL_PATH_NAME')  モデル学習時のメタ情報もDBに残しておくのが良いと思いますが，今回はその部分は実装していないです🙏\n 予測パート  S3に保存したモデルをロードして，与えらたデータに対して予測を行います モデルIDは学習時に発行されたUUIDをコピーして貼り付ける必要があるのですが，出力されたログから拾うのでちょっといけてないですがモックなのでご勘弁を\u0026hellip;     consumer.py from concurrent.futures import ThreadPoolExecutor import click import tasks @click.command() @click.option(\u0026quot;--num_threads\u0026quot;, type=int, help='the number of threads', default=1) @click.option(\u0026quot;--max_workers\u0026quot;, type=int, help='the number of max workers', default=None) def main(num_threads: int, max_workers: int): # Consumer execution with ThreadPoolExecutor(max_workers=max_workers) as executor: for _ in range(num_threads): for task in [ tasks.TrainConsumer(queue_name='queue.model.train'), tasks.PredictConsumer(queue_name='queue.model.predict') ]: executor.submit(task.run) if __name__ == \u0026quot;__main__\u0026quot;: main()   引数に指定したスレッド数に応じてConsumerが複数立ち上がります．\n実行結果 docker compose upでコンテナを起動して，http://localhost:5000/docsにアクセスするとSwaggerによる表示がされます．FastAPIはデフォルトでOpenAPIを自動生成してくれ，SwaggerやReDocで表示することができます．\nこの辺は個人的にとても便利だなと思っていて，データを簡単にGET/POSTすることで動作を確認することできます．\n Swaggerの画面   ReDocの画面  学習編 /trainにリクエストをPOSTします．事前にS3に保存したデータセット名をdataset_idに，使用する特徴量（説明変数）をfeaturesに，目的変数をtargetに指定します．\ncurl -X 'POST' \\ 'http://localhost:5000/train' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;features\u0026quot;: [\u0026quot;age\u0026quot;, \u0026quot;bmi\u0026quot;, \u0026quot;bp\u0026quot;, \u0026quot;s1\u0026quot;, \u0026quot;s2\u0026quot;, \u0026quot;s3\u0026quot;, \u0026quot;s4\u0026quot;, \u0026quot;s5\u0026quot;, \u0026quot;s6\u0026quot;], \u0026quot;target\u0026quot;: \u0026quot;target\u0026quot; }'  出力されるログは以下のような感じになります．\n 4行目はProducerがキューに対して送信したメッセージ（Consumerに渡したい情報） 7行目（中略後1行目）はConsumerからのReplyメッセージ 最後のConsumerからのログは学習処理を実行中に出力されるログ  producer | [2022-04-24 15:49:09] [ INFO] Created channel=1 producer | [2022-04-24 15:49:09] [ INFO] Pika connection initialized. producer | [2022-04-24 15:49:09] [ INFO] Produce message for train. producer | [2022-04-24 15:49:09] [ INFO] Sent message. [q] 'queue.model.train' [x] Body: message_json='{\u0026quot;model_id\u0026quot;: \u0026quot;c7632288-442d-44c5-9102-31ccda2af6b7\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;features\u0026quot;: [\u0026quot;age\u0026quot;, \u0026quot;bmi\u0026quot;, \u0026quot;bp\u0026quot;, \u0026quot;s1\u0026quot;, \u0026quot;s2\u0026quot;, \u0026quot;s3\u0026quot;, \u0026quot;s4\u0026quot;, \u0026quot;s5\u0026quot;, \u0026quot;s6\u0026quot;], \u0026quot;target\u0026quot;: \u0026quot;target\u0026quot;}' consumer | [2022-04-24 15:49:09] [ INFO] Convert message to dict type. ~中略~ producer | [2022-04-24 15:49:09] [ INFO] Reply Response from consumer: {'status': 'TASK_RECEIVED', 'model_id': 'c7632288-442d-44c5-9102-31ccda2af6b7'} producer | INFO: 172.24.0.1:57222 - \u0026quot;POST /train HTTP/1.1\u0026quot; 200 OK rabbitmq | 2022-04-24 06:49:09.367236+00:00 [info] \u0026lt;0.5900.0\u0026gt; closing AMQP connection \u0026lt;0.5900.0\u0026gt; (172.24.0.4:46266 -\u0026gt; 172.24.0.2:5672, vhost: '/', user: 'guest') consumer | [2022-04-24 15:49:09] [ INFO] Found credentials in shared credentials file: ~/.aws/credentials consumer | [2022-04-24 15:49:09] [ INFO] Download dataset from S3. consumer | [2022-04-24 15:49:09] [ INFO] Read csv file and transform to dataframe. consumer | [2022-04-24 15:49:09] [ INFO] Start model training. consumer | [2022-04-24 15:49:09] [ INFO] Model fit for training. consumer | [2022-04-24 15:49:09] [ INFO] Evaluate metrics=RMSE for valid dataset : 53.039 consumer | [2022-04-24 15:49:09] [ INFO] Finish model training. consumer | [2022-04-24 15:49:09] [ INFO] Save trained model to local. consumer | [2022-04-24 15:49:10] [ INFO] Upload trained model to S3. consumer | [2022-04-24 15:49:10] [ INFO] TASK_COMPLETED  予測編 /predictにリクエストをPOSTします．model_idを元に学習済みモデルをS3からロードします．input_dataには，モデルに入力するデータをdict形式で特徴量とその値という組で渡します．\n※ model_idは学習編のログ出力にあるmodel_idを使用する必要があります．\ncurl -X 'POST' \\ 'http://localhost:5000/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;model_id\u0026quot;: \u0026quot;c7632288-442d-44c5-9102-31ccda2af6b7\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;input_data\u0026quot;: { \u0026quot;age\u0026quot;: 0.038076, \u0026quot;bmi\u0026quot;: 0.061696, \u0026quot;bp\u0026quot;: 0.021872, \u0026quot;s1\u0026quot;: -0.044223, \u0026quot;s2\u0026quot;: -0.034821, \u0026quot;s3\u0026quot;: -0.043401, \u0026quot;s4\u0026quot;: -0.002592, \u0026quot;s5\u0026quot;: 0.019908, \u0026quot;s6\u0026quot;: -0.017646 } }'  出力されるログは以下のような感じになります．\n 4行目はProducerがキューに対して送信したメッセージ（Consumerに渡したい情報） 5行目のConsumerからのログは予測処理を実行中に出力されるログ （中略後1行目）はConsumerからのReplyメッセージで，予測結果が入っています  producer | [2022-04-24 18:00:16] [ INFO] Created channel=1 producer | [2022-04-24 18:00:16] [ INFO] Pika connection initialized. producer | [2022-04-24 18:00:16] [ INFO] Produce message for predict. producer | [2022-04-24 18:00:16] [ INFO] Sent message. [q] 'queue.model.predict' [x] Body: message_json='{\u0026quot;model_id\u0026quot;: \u0026quot;c7632288-442d-44c5-9102-31ccda2af6b7\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;input_data\u0026quot;: {\u0026quot;age\u0026quot;: 0.038076, \u0026quot;bmi\u0026quot;: 0.061696, \u0026quot;bp\u0026quot;: 0.021872, \u0026quot;s1\u0026quot;: -0.044223, \u0026quot;s2\u0026quot;: -0.034821, \u0026quot;s3\u0026quot;: -0.043401, \u0026quot;s4\u0026quot;: -0.002592, \u0026quot;s5\u0026quot;: 0.019908, \u0026quot;s6\u0026quot;: -0.017646}}' consumer | [2022-04-24 18:00:16] [ INFO] Convert message to dict type. consumer | [2022-04-24 18:00:16] [ INFO] Download model file from S3. consumer | [2022-04-24 18:00:16] [ INFO] Load model for prediction. ~中略~ producer | [2022-04-24 18:00:16] [ INFO] Reply Response from consumer: {'status': 'TASK_COMPLETED', 'pred_proba': 208.6445780005619} rabbitmq | 2022-04-24 09:00:16.565636+00:00 [info] \u0026lt;0.8348.0\u0026gt; closing AMQP connection \u0026lt;0.8348.0\u0026gt; (172.24.0.4:46664 -\u0026gt; 172.24.0.2:5672, vhost: '/', user: 'guest') producer | INFO: 172.24.0.1:57624 - \u0026quot;POST /predict HTTP/1.1\u0026quot; 200 OK  おわりに FastAPIとRabbitMQを用いてWebAPI形式で，機械学習タスクの非同期処理を行う検証をしました．非同期処理だったり，RPCやPub/Subの仕組みを少しは理解できたかなと思います．\n今回はDBにメタデータを保存したりDB周りの処理は実装していないので，この辺も時間があれば実装できればと思います\u0026hellip;\n非同期処理を行う上でメインの役割を果たしたRabbitMQについてもコメントすると，OSSで簡単に非同期処理を行える便利な技術だと思います．Producer/Exchange/Queue/Consumerの関係性もTutorialsの図などでイメージしやすくなるので，サンプルコードを見ながら比較的容易に実装することができました．\n一方で，ConsumerからProducerにReplyメッセージを送る場合に，どのように実装すればいいかが分かりづらく，個人的にはハマりポイントでした．\nあと，なにげにFastAPIもほとんど使ったことなかったので良い勉強になりました！\n最後に今後やりたいことについて列挙しておくと\u0026hellip;\n AWS SQSを使った非同期処理の実装 Celeryを使った非同期処理の実装 BrokerとしてRedisを用いた実装 DBを使ったメタデータの保存 etc\u0026hellip;  参考  FastAPI RabbitMQ  RabbitMQ Tutorials   Asynchronous message-based communication Deep Learning: Scaling your neural networks with containerization and a message broker katanaml/katana-skipper  ","date":"2022-04-24","permalink":"https://masatakashiwagi.github.io/portfolio/post/async-ml-processing/","tags":["Dev","Machine Learning"],"title":"FastAPIとRabbitMQを用いた機械学習タスクの非同期処理"},{"content":"はじめに 最近個人的にMLOpsという領域に高い関心があって，日々情報収集をしたりしているのですが，各社が実務で取り組んでいる事例やMLOpsの各領域で使用されている技術やツールなど個別のTipsが点在していて見つけるのが大変だなという印象があります．\n特に実際のユースケースが整理されていると嬉しいなーという気持ちから，じゃあ自分で整理すればいいのでは？と思い「MLOps Practices」というWebsiteを作って公開することにしました．\nMLOps Practicesとは？ 海外には，ApplyingML や Awesome MLOps というWebsite/Repositoryがありトピック毎に各社の事例が載っていたり，色々と網羅的に整理されておりサーベイの参考になるものがあります．\nこれと似たものがあったらいいなーと思って軽く探したところ，見当たらなかったので，それだったら自分のリファレンス用も兼ねて作ってみようかなという気持ちから MLOps Practices というWebsiteを作成し，公開しました．\nKnowledgeには，各社で実際に導入されていた or されている事例を整理しています．特に会社のテックブログやイベントでの登壇資料を紹介するようにしています．これは実運用されているケースを大事にしたいと思ったので，その部分にフォーカスを当てています．\nTips（こちらは未整備）には，各ツールの使い方など個人ブログなどからも収集しようと考えてますが，数が膨大になるので要検討という感じです．\nまだまだ始めたばかりで十分に整理網羅出来ていないですが，細々と続けていきたいと思います．もし，「手伝ってもいいよー！」という方が居ましたら是非お声がけ下さい！一緒に進めて行けたらと思います．\nそれ以外にも追加の事例などありましたら，Issueを作成後にPRを出して貰えたら大変嬉しいです！\nおわりに 徐々に内容を充実させて行ければと思うので，今後も定期的に更新していきたいと思います！また，一緒に更新 \u0026amp; 運用していってくれる人も募集中ですので，興味があればご連絡下さい！\nRepositoryにスター追加して貰えると励みになりますので，よろしくお願いします🙇\n\n各社のMLOpsに対する実践事例を収集して整理するRepositoryを作ってWebsiteに公開したので，良ければご利用くださいー！https://t.co/JMOQGuDtf0\n\u0026mdash; asteriam (@asteriam_fp) January 22, 2022  参考  ApplyingML Awesome MLOps MLOps Practices  Website: https://masatakashiwagi.github.io/mlops-practices/ Repository: https://github.com/masatakashiwagi/mlops-practices    ","date":"2022-02-05","permalink":"https://masatakashiwagi.github.io/portfolio/post/start-mlops-practices-project/","tags":["Poem","MLOps"],"title":"各社のMLOps事例を集めたMLOps PracticesというWebsiteを公開しました"},{"content":"はじめに Step FunctionsでSageMakerのリソースを特にカスタムコンテナイメージで使う場合，単純にInstanceTypeとして\u0026quot;ml.g4dn.xlarge\u0026quot;などのGPUマシンを設定するだけではGPUを使った学習はできなくて，使いたいDockerfileに少し手を加える必要があります．\n今回は備忘録も兼ねてDockerfileの中身を紹介しながら，GPU環境での動作確認をしたいと思います．\n以下の手順で動作確認を行っています．\n ローカルでDockerfileと動作確認用のPythonスクリプトを作成する ローカルでbuildし，AWS ECRにbuildしたイメージをpushする pushしたイメージのURIをSageMaker Training JobのTrainingImageに指定する Step Functionsを実行する CloudWatch Logsを確認する  今回の動作確認フローは以下の図のようなイメージです．\n自作の「Dockerfile.gpu」ファイル 今回はTensorflow-gpuのベースイメージを使っています．そのイメージに「nvidia-docker」を追加でインストールすることでStep FunctionsでGPU用のカスタムコンテナイメージを使ってSageMakerを動作させることができます．\nTensorFlow Docker Imagesから好きなgpu用のイメージを選択して下さい．今回は「tensorflow/tensorflow:2.6.1-gpu」を使用することにします．Optional Featuresにも記載されていますが，nvidia-dockerが必要だよとのことなので，この通りにします．\n -gpu tags are based on Nvidia CUDA. You need nvidia-docker to run them. NOTE: GPU versions of TensorFlow 1.13 and above (this includes the latest- tags) require an NVidia driver that supports CUDA 10. See NVidia\u0026rsquo;s support matrix.\n 大事な部分は以下の4行になります．以前はこちらの記事にも書かれていますが，nvidia-container-toolkitをインストールする必要があったみたいですが，今はnvidia-docker2をインストールすることで，nvidia-container-toolkitも一緒にインストールされるみたいで，よりシンプルになっています．\nRUN distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list RUN sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-docker2  今回使用したDockerfile全体は以下になります．\n# Dockerfile.gpu FROM tensorflow/tensorflow:2.6.1-gpu # Set some environment variables. # PYTHONUNBUFFERED keeps Python from buffering our standard # output stream, which means that logs can be delivered to the user quickly. ENV PYTHONUNBUFFERED=TRUE # PYTHONDONTWRITEBYTECODE keeps Python from writing the .pyc files which # are unnecessary in this case. ENV PYTHONDONTWRITEBYTECODE=TRUE # DEBIAN_FRONTEND prevent from stoping docker build with tzdata ENV DEBIAN_FRONTEND=noninteractive RUN apt-get -y update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ curl \\ sudo \\ libmecab-dev \\ python3.8 \\ python3-distutils \\ python3-six \\ git \\ file \\ wget \\ \u0026amp;\u0026amp; apt-get clean \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # ここが今回のポイントで，nvidia-dockerをインストールします RUN distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list RUN sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-docker2 # 必要なライブラリーをrequirements.lockを使ってインストールします COPY requirements.lock /tmp/requirements.lock RUN python3 -m pip install -U pip \\ \u0026amp;\u0026amp; python3 -m pip install -r /tmp/requirements.lock \\ \u0026amp;\u0026amp; python3 -m pip install sagemaker \\ \u0026amp;\u0026amp; python3 -m pip install sagemaker-training \\ \u0026amp;\u0026amp; rm /tmp/requirements.lock \\ \u0026amp;\u0026amp; rm -rf /root/.cache # Timezone jst RUN ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime # Locale Japanese ENV LC_ALL=ja_JP.UTF-8 # Set up the program in the image ENV PROGRAM_DIR=/opt/program COPY app $PROGRAM_DIR WORKDIR $PROGRAM_DIR ENV PATH=\u0026quot;/opt/program:${PATH}\u0026quot; RUN chmod +x $PROGRAM_DIR/hello_gpu.py CMD [\u0026quot;python3\u0026quot;]  また，SageMakerでGPUを認識しているかを確認するためのPythonスクリプトは以下になります．\n# hello_gpu.py import tensorflow as tf from tensorflow.python.client import device_lib def main(): # TensorflowのGPU確認 print(f'GPUs Available: {tf.test.is_gpu_available()}') print(f\u0026quot;Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\u0026quot;) print(f'{device_lib.list_local_devices()}') if __name__ == \u0026quot;__main__\u0026quot;: main()  これらのファイルを用意して，ローカルでbuildを行います，buildした後はそのイメージをECRにpushすることで，Step Functionsで使用することができます．\n※ ECRへのpush方法や設定は今回割愛します．\nStep Functionsの設定 Step Functionsとは，AWSが提供する各種サービスを組み合わせたパイプラインを構築するためのワークフローサービスになります．機械学習向けのアクションも用意されていて，今回使用するSageMaker Training Jobもその一つになります．\n設定はyamlファイルのような形式でAWSのコンソール画面上で打ち込んでいきます．今回実施する内容の記述は以下の通りになります．\n{ \u0026quot;Comment\u0026quot;: \u0026quot;Check GPU env\u0026quot;, \u0026quot;StartAt\u0026quot;: \u0026quot;Hello-GPU\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;Hello-GPU\u0026quot;: { \u0026quot;Comment\u0026quot;: \u0026quot;GPUの動作確認\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::sagemaker:createTrainingJob.sync\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;RoleArn\u0026quot;: \u0026quot;\u0026lt;SageMaker Training Jobの実行権限がアタッチされているロール\u0026gt;\u0026quot;, \u0026quot;TrainingJobName\u0026quot;: \u0026quot;sample-training-job\u0026quot;, \u0026quot;AlgorithmSpecification\u0026quot;: { \u0026quot;EnableSageMakerMetricsTimeSeries\u0026quot;: true, \u0026quot;TrainingImage\u0026quot;: \u0026quot;\u0026lt;アカウントID\u0026gt;.dkr.ecr.ap-northeast-1.amazonaws.com/sample:latest-gpu\u0026quot;, \u0026quot;TrainingInputMode\u0026quot;: \u0026quot;File\u0026quot; }, \u0026quot;EnableInterContainerTrafficEncryption\u0026quot;: true, \u0026quot;EnableManagedSpotTraining\u0026quot;: true, \u0026quot;Environment\u0026quot;: { \u0026quot;SAGEMAKER_PROGRAM\u0026quot;: \u0026quot;/opt/program/hello_gpu.py\u0026quot; }, \u0026quot;ResourceConfig\u0026quot;: { \u0026quot;InstanceCount\u0026quot;: 1, \u0026quot;InstanceType\u0026quot;: \u0026quot;ml.g4dn.xlarge\u0026quot;, \u0026quot;VolumeSizeInGB\u0026quot;: 20 }, \u0026quot;StoppingCondition\u0026quot;: { \u0026quot;MaxRuntimeInSeconds\u0026quot;: 12345, \u0026quot;MaxWaitTimeInSeconds\u0026quot;: 12345 } }, \u0026quot;End\u0026quot;: true } } }  細かい設定内容に関しては，CreateTrainingJob というドキュメントを参考下さい．\nここで，Environment（環境変数）の「SAGEMAKER_PROGRAM」について説明しておきます．この変数に指定したプログラムはTraining Jobのエントリーポイントにすることができます．\n元々は以下のコマンドが実行されるのですが（train.pyがあればそれが対象となる），実行したいプログラムのパスを指定することで任意のプログラムを実行することができます．\n docker run \u0026lt;イメージ\u0026gt; train\n ただし，実行権限を与えておく必要があるので，Dockerfile内でRUN chmod +x $PROGRAM_DIR/hello_gpu.pyとしています．\nあとは，実行結果をCloudWatch Logsで確認して，以下の内容がログに出力されていればOKです．\n GPUs Available: True Num GPUs Available: 1\n おわりに 今回は，Step FunctionsでSageMakerのTraining Jobをカスタムコンテナイメージを使ってGPU環境で動かす方法を紹介しました．この方法を使えば，深層学習などのGPU環境を必要とした学習もパイプラインに組み込むことが可能になります．また，Training Jobを使った学習ができれば，実験結果はSageMaker Experimentsに保存されるので，再現性を担保することもできます．\nStep Functionsでカスタムコンテナイメージを使ってGPU環境で学習させたい場合には，参考にして頂ければと思います．\n参考  Available SageMaker Studio Instance Types Amazon SageMakerの料金 NVIDIA Docker って今どうなってるの？ (20.09 版) What is AWS Step Functions?  ","date":"2022-01-30","permalink":"https://masatakashiwagi.github.io/portfolio/post/aws-stepfunctions-gpu-setting-for-sagemaker-jobs/","tags":["AWS","Dev"],"title":"Step Functionsで自作Dockerfileを使ってSageMakerのGPUマシンを動かす方法"},{"content":"はじめに Step FunctionsでSageMakerのProceesingJobを使ってカスタムコンテナを実行した際に，その実行スクリプト内でExperimentAnalyticsのAPIを使用していたところ，「ValueError: Must setup local AWS configuration with a region supported by SageMaker.」というエラーが発生したので，その対処方法をメモしておきます．\n結論から言うと，エラー内容にある通り「region」の指定を行うことで解決できます．\n方法としては2つあります．\n 「boto_session，sagemaker_client」の「region_name」を指定する 環境変数に「AWS_DEFAULT_REGION」を設定する  2つ目の方法のStep Functionsの定義ファイルに環境変数: AWS_DEFAULT_REGIONを1行追記するのが簡単かと思います．\nConfiguration Error SageMaker Experimentsに保存されている実験結果はsagemaker.analytics.ExperimentAnalyticsのAPI使うことで取得することができます．今回，Step FunctionsでSageMakerのProceesingJobを使ってカスタムコンテナを実行した際に，以下のエラーが発生しました．\n ValueError: Must setup local AWS configuration with a region supported by SageMaker.\n Configurationは公式ドキュメントを見ると以下のことが書かれています．\n Configuration - Overview Boto3 looks at various configuration locations until it finds configuration values. Boto3 adheres to the following lookup order when searching through sources for configuration values:\n A Config object that\u0026rsquo;s created and passed as the config parameter when creating a client Environment variables The ~/.aws/config file   上から順番に優先度が高いものになっていて，下位で設定している内容よりも上位で設定した内容が反映されます．\n今回の場合は，1番目と2番目は特段設定していないので，3番目が採用されて問題ないかなと思っていましたが，上述のエラーが発生しました．AWSのクレデンシャル情報（config, credentials）はローカルの~/.aws/配下に置かれており，build時にローカルにあるファイルをvolumesマウントしてコンテナ内と同期しています．\nこの設定では上手くいかなかったので，1番目or2番目の設定を行ったところ正常に動作したので，こちらの方法を記載しておきます．もし，volumesマウントの方法で上手くいく方法があれば教えて下さい！\nConfig objectをboto3 clientに渡す方法 公式ドキュメントに記載されている優先度が1番高い方法になります．botocore.config.Configをインスタンス化して使うことで解決する方法になりますが，今回はわざわざConfig objectを使わずにregion_nameを指定する方法を説明します．\nimport boto3 import sagemaker # sessionとclientの設定を行う boto_session = boto3.session.Session(region_name=\u0026quot;ap-northeast-1\u0026quot;) sagemaker_client = boto_session.client(service_name='sagemaker', region_name=\u0026quot;ap-northeast-1\u0026quot;) sagemaker_session = sagemaker.session.Session(boto_session=boto_session, sagemaker_client=sagemaker_client) # ExperimentAnalyticsをインスタンス化する trial_component_analytics = sagemaker.analytics.ExperimentAnalytics( experiment_name='sample-experiments01', sagemaker_session=sagemaker_session ) # データフレーム化 analytics_tables = trial_component_analytics.dataframe()  コードの流れは以下になります．\n boto_sessionとsagemaker_clientを作成する sagemaker.session.Sessionの引数にそれぞれを渡す sagemaker_sessionをsagemaker.analytics.ExperimentAnalyticsの引数に渡す 取得した実験結果をデータフレーム化する  ここで，最初の「boto_sessionとsagemaker_clientを作成する」部分で，「boto3.session.Sessionのregion_name」とこのsessionを使った「clientのregion_name」に該当するregionを指定する必要があります．この2つをセットしておくことで，今回発生したエラーを回避することができます．\nこの場合はカスタムコンテナで実行するスクリプトの修正変更が必要になってきますが，次に説明する環境変数に渡す方法はこの辺りの修正は必要ないので，簡単かなと思います．\n環境変数を設定する方法 Step Functionsのワークフローを定義するjsonファイルのEnvironment変数に「AWS_DEFAULT_REGION」を設定する方法になります．\nStep Functionsの定義ファイルのパラメータ部分は下記のような感じです．（今回はSageMakerのProcessingJobを使って実行しています）\n\u0026quot;Parameters\u0026quot;: { \u0026quot;AppSpecification\u0026quot;: { \u0026quot;ImageUri\u0026quot;: \u0026quot;hogehoge.dkr.ecr.ap-northeast-1.amazonaws.com/mlops-experiments:latest\u0026quot;, \u0026quot;ContainerEntrypoint\u0026quot;: [ \u0026quot;python3\u0026quot;, \u0026quot;/opt/ml/code/get_experiments.py\u0026quot; ] }, \u0026quot;Environment\u0026quot;: { \u0026quot;AWS_DEFAULT_REGION\u0026quot;: \u0026quot;ap-northeast-1\u0026quot; } }  スクリプトの中身は以下になります．sessionの設定が必要なくなります．\nimport sagemaker # ExperimentAnalyticsをインスタンス化する trial_component_analytics = sagemaker.analytics.ExperimentAnalytics( experiment_name='sample-experiments01' ) # データフレーム化 analytics_tables = trial_component_analytics.dataframe()  おわりに 今回は，Step FunctionsでSageMakerのProceesingJobを使った際に発生したエラーの対処方法を備忘録として残したものになります．\nConfigurationの設定に優先順位があることを知ったので，この辺りは今回に限らず注意が必要だなと思いました．今回のエラーに対する対処方法は複数あるので，開発している状況に合わせて使い分けていければと思います．\nあと，個人的には実行しているStep Functionsのregionをセットして欲しい気持ちもありますが，まーこれは状況次第なので，なんとも言えない気もします\u0026hellip;\n参考  Boto3 Docs - Configuration How to fix aws region error \u0026ldquo;ValueError: Must setup local AWS configuration with a region supported by SageMaker\u0026rdquo;  ","date":"2021-12-26","permalink":"https://masatakashiwagi.github.io/portfolio/post/value-error-local-aws-configuration/","tags":["Dev","AWS"],"title":"ValueError: Must setup local AWS configurationの対処方法"},{"content":"はじめに 今回は，teamayaという個人プロジェクトで進めているデータ連携の話になります．コードは以下のリポジトリに置いてあるので，ご自由に使用下さい！\n 具体的には，手元にあるスプレッドシートのデータをBigQueryの特定のテーブルに連携するまでの話になります．\n普段家計簿のデータをスプレッドシートに手入力で管理してるんですが，そのデータをBigQueryに集めて色々と検証できると良いなーという思いから，データ連携を始めました．加えて，可視化も良くしたい思いもあり，Data Studioでダッシュボードを作ったりもしています．\nデータ連携をするだけであれば，Embulkを単体実行するのでこと足りますが，今回はサーバーモードで立ち上げたDigdag UIを使ってワークフローを実行しています．スケジュール実行，ワークフロー管理や履歴管理などがUIからだとしやすく，使い心地などを知るためにも使用しました．\nまた，Docker環境で実行できるように構成しています．Docker化することで，簡単に別環境に持っていくことができますし，スクラップ\u0026amp;ビルドがしやすいのもあります．\n今回は以下の2種類の方法でデータを連携する方法を紹介します．内容的には既に技術記事に書かれているものが多いと思いますが，今回はDockerコンテナで各タスクが実行できるようにしているので，その辺りを参考頂けたらと思います．\n Dockerコンテナ内からEmbulkを直接実行して，データを転送する方法 Digdag UIからワークフローを実行して，データを転送する方法  こちらも裏側では，Embulkが実行されます．    今回のデータ連携フローのアーキテクチャーは以下のような感じです．\nアンドリュー・カーネギーの以下の名言にもあるように，機械学習エコシステムを自分で作っていくために，一歩一歩進めています！\n 最も高い目標を達成するには、一歩一歩進むしかないという事実を、頭に入れておかなければならない。\n BigQueryとスプレッドシートの設定 GCPのアカウント登録方法は割愛しますが，gmailがあれば簡単に登録できます．登録が完了したら，適当なプロジェクトを作成して下さい．\nGoogle Sheets APIの有効化を行う 「APIとサービス」 → 「ライブラリ」と画面遷移し，検索窓に「Google Sheets API」と入力して検索すると，スプレッドシートのAPIを有効化できる画面に遷移するので，有効化を行います．ここで有効化しておかないと，この後スプレッドシートを使用したデータ連携が出来ないので注意下さい！\nサービスアカウントの作成 それが終わったら，サービスアカウントを作成します．「IAMと管理」 → 「サービスアカウント」へアクセスした後，必要な情報を入力し，キーの作成からJSONを選択してキーの作成を行います．そうすると，サービスアカウントのJSONファイルがダウンロードされるので，これを~/.gcp配下に置いておきます．\nサービスアカウントのメールアドレスをスプレッドシートに登録 データ連携したいスプレッドシートを開き，右上の共有ボタンから「ユーザーやグループを追加」の枠にダウンロードしたサービスアカウントのメールアドレスをコピー\u0026amp;ペーストして，送信をクリックします．そうすることで，このスプレッドシートのデータを登録したサービスアカウントで転送することができます．\nここで，メールアドレスの許可をしていない場合，Embulk実行時に以下のエラーが発生します．\n Error: (ClientError) forbidden: The caller does not have permission\n BigQueryにデータセットを作成 データを格納するために，事前にデータセットを作成しておく必要があるので，データセットIDを適当に決めて，データセットの作成を行っておきます．\n1. Embulkを直接実行してデータ転送を行う場合 この方法は，Dockerコンテナ内からEmbulkを直接実行して，スプレッドシートのデータをBigQueryのテーブルに転送する方法になります．\nEmbulkの細かい説明は割愛しますが，簡単に言えば，バルクデータローダーの役割としてBigQueryなどのデータレイク/データウェアハウスにデータ転送を行うことができます．\nデータ転送を行うために用意するものとしては，以下になります．\n Gemfile Liquidファイル Dockerfile \u0026amp; docker-compose.ymlファイル  Gemfileを用意する EmbulkのプラグインをGemfile/Gemfile.lockでバージョン管理するために用意します．Embulkには，データのInput/Outputのプラグインがあリ，これを使うことで様々なデータソースからターゲットにデータを連携することができます．\n Input: embulk-input-google_spreadsheets Output: embulk-output-bigquery  source 'https://rubygems.org/' # No versions are specified for 'embulk' to use the gem embedded in embulk.jar. # Note that prerelease versions (e.g. \u0026quot;0.9.0.beta\u0026quot;) do not match the statement. # Specify the exact prerelease version (like '= 0.9.0.beta') for prereleases. gem 'embulk' # input spreadsheets plugin gem 'embulk-input-google_spreadsheets' # ouput bigquery plugin gem 'embulk-output-bigquery' gem 'tzinfo-data'  Liquidファイルを作成する Embulkの設定ファイルとしてLiquidファイルを作成します．YAMLファイルに設定を記述することもできますが，以下のメリットでLiquidファイルを使用しています．\n 変数を設定することができる 同じ設定内容を共通ファイルとして使うことができる etc\u0026hellip;  BigQueryとスプレッドシートの情報は，.envファイルを作成して，環境変数として管理しています．これらの変数をLiquidファイルで使用しています．\nin: type: google_spreadsheets auth_method: service_account {% comment %} GCPのサービスアカウントのJSONファイルパス {% endcomment %} json_keyfile: {{ env.GCP_SERVICE_JSON }} {% comment %} スプレッドシートのURL {% endcomment %} spreadsheets_url: {{ env.SPREADSHEETS_TABLE }} default_timezone: 'Asia/Tokyo' {% comment %} スプレッドシートのワークシートタイトル {% endcomment %} worksheet_title: year_purchase_amount_2019 {% comment %} headerを指定している場合は2行目からとなる {% endcomment %} start_row: 2 {% comment %} カラム名と型を指定する {% endcomment %} columns: - {name: id, type: long} - {name: date, type: timestamp, format: '%Y/%m/%d', timezone: 'Asia/Tokyo'} - {name: category, type: string} - {name: purchaser, type: string} - {name: purchase_amount, type: long} - {name: memo, type: string} out: type: bigquery mode: replace auth_method: service_account json_keyfile: {{ env.GCP_SERVICE_JSON }} {% comment %} BigQueryのプロジェクト名 {% endcomment %} project: {{ env.BIGQUERY_PROJECT }} {% comment %} BigQueryのデータセット名 {% endcomment %} dataset: {{ env.BIGQUERY_PURCHASE_AMOUNT_DATASET }} {% comment %} BigQueryのテーブル名 {% endcomment %} table: daily_purchase_amount auto_create_table: true source_format: NEWLINE_DELIMITED_JSON default_timezone: 'Asia/Tokyo' default_timestamp_format: '%Y-%m-%d' formatter: {type: jsonl} encoders: - {type: gzip} retries: 3  envファイルの説明も補足でしておきます．適宜設定している環境に合わせて修正します．\nSPREADSHEETS_TABLE=\u0026lt;該当するスプレッドシートのURL: https://docs.google.com/spreadsheets/d/hogehoge\u0026gt; BIGQUERY_PROJECT=\u0026lt;BigQueryのプロジェクト名\u0026gt; BIGQUERY_PURCHASE_AMOUNT_DATASET=\u0026lt;BigQueryのデータセット名\u0026gt; GCP_SERVICE_JSON=/root/.gcp/hoge.json  Dockerfile \u0026amp; docker-compose.ymlファイルを作成する 今回はdocker環境から実行するので，Dockerfileとdocker-compose.ymlファイルを作成します．（後ほど使うDigdagの内容も記載されています）\nFROM openjdk:8-alpine LABEL MAINTAINER=masatakashiwagi ENV DIGDAG_VERSION=\u0026quot;0.9.42\u0026quot; ENV EMBULK_VERSION=\u0026quot;0.9.23\u0026quot; RUN apk --update add --virtual build-dependencies \\ curl \\ tzdata \\ coreutils \\ bash \\ \u0026amp;\u0026amp; curl --create-dirs -o /bin/digdag -L \u0026quot;https://dl.digdag.io/digdag-${DIGDAG_VERSION}\u0026quot; \\ \u0026amp;\u0026amp; curl --create-dirs -o /bin/embulk -L \u0026quot;https://dl.embulk.org/embulk-$EMBULK_VERSION.jar\u0026quot; \\ \u0026amp;\u0026amp; chmod +x /bin/digdag \\ \u0026amp;\u0026amp; chmod +x /bin/embulk \\ \u0026amp;\u0026amp; cp /usr/share/zoneinfo/Asia/Tokyo /etc/localtime \\ \u0026amp;\u0026amp; apk del build-dependencies --purge ENV PATH=\u0026quot;$PATH:/bin\u0026quot; # Install libc6-compat for Embulk Plugins to use JNI # cf: https://github.com/jruby/jruby/wiki/JRuby-on-Alpine-Linux # https://github.com/classmethod/docker-embulk RUN apk --update add libc6-compat # Copy Embulk configuration COPY ./embulk/task /opt/workflow/embulk/task # Make bundle WORKDIR /opt/workflow/embulk RUN embulk mkbundle bundle # Copy Gemfile file # This is the workaround, because jruby directory is not created COPY ./embulk/bundle/Gemfile /opt/workflow/embulk/bundle COPY ./embulk/bundle/Gemfile.lock /opt/workflow/embulk/bundle WORKDIR /opt/workflow/embulk/bundle # Install Embulk Plugins RUN embulk bundle # Set up Digdag Server COPY ./digdag /opt/workflow/digdag # ADD https://github.com/ufoscout/docker-compose-wait/releases/download/2.9.0/wait /bin/wait # RUN chmod +x /bin/wait WORKDIR /opt/workflow CMD [\u0026quot;tail\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/dev/null\u0026quot;]  bundle辺りでまわりくどいやり方をしていますが，RUN embulk bundleを実行した際に上手くインストールできなかったため，ワークアラウンドとして，mkbundleした後にGemfile/Gemfile.lockをdockerコンテナ内にCOPYした上で，RUN embulk bundleを行っています．\nEmbulkをdockerコンテナで実行する方法 では，実際にEmbulkの実行を行うために，まずはEmbulkのコンテナサービスを立ち上げます．\n# コンテナの立ち上げ docker compose up -d embulk  バックグラウンドでコンテナを起動しておきます．その後，dry-runを行うためにpreviewコマンドを実行します．\n# dry-run docker exec embulk sh /bin/embulk preview -b embulk/bundle embulk/task/spreadsheet/export_hab_purchase_amount.yml.liquid  dry-run実行後の結果を載せておきます．\ndry-runが大丈夫だった場合，本番実行を行います．本番はrunコマンドを実行します．\n# production-run docker exec embulk sh /bin/embulk run -b embulk/bundle embulk/task/spreadsheet/export_hab_purchase_amount.yml.liquid  本番実行が上手くいくとBigQueryにデータが入っていることを確認できます．\n2. Digdag UIからワークフローを実行してデータ転送を行う場合 次に説明するこちらの方法は，digdag serverを立ち上げて，UI上からワークフローを実行して，スプレッドシートのデータをBigQueryのテーブルに転送する方法になります．（裏でEmbulkが動いています）\nDigdagの細かい説明は割愛しますが，簡単に言えば，設定ファイルでバッチのワークフロー実行を定義・管理できるワークフローエンジンになります．スケジュール実行や失敗時の通知などを行うことができます．\nDigdagのワークフローからデータ転送を行うために用意するものとしては，以下になります．\n digファイル serverのpropertiesファイル docker-compose.ymlファイル  上記に加えて，1で説明したEmbulk実行に必要なファイルも用意します．\ndigファイルを用意する 個別のワークフローを単発実行する場合は，digdag run hoge.digで良いのですが，今回はUIから実行したいので，厳密にはdigファイルの中身が要ります．記述する内容としては，ワークフローで実行していくタスクをコードに落としていきます．\nDigdagでは，エラーの場合や成功した場合に，どういった処理をするのかを書くことができるので，例えば，それぞれの場合でslackに通知を行えたりもできます（今回はslack通知は実装できていませんが，今後実装していきたいです！）．リトライ回数の設定やスケジュール実行の設定もここでできます．\n+task: _retry: 1 sh\u0026gt;: /bin/embulk run -b /opt/workflow/embulk/bundle /opt/workflow/embulk/task/spreadsheet/export_hab_purchase_amount.yml.liquid _error: echo\u0026gt;: workflow error... +success: echo\u0026gt;: workflow success!  今回は以下のワークフローになっています．\n task:  リトライ回数: 1回 Embulkの実行 エラーの場合はworkflow error...をechoする   success:  workflow success!をechoする    successはtaskが正常に終了した場合に，実行されることになります．\nserverのpropertiesファイルを用意する サーバーモードで起動するために，引数にオプションを指定する必要があるのですが，それらの設定をserver.propertiesファイルに集約しています．設定内容は色々とあるので詳しくは公式ドキュメント: server-mode-commandsを参照下さい．\nこのファイルには，サーバーの情報やデータベースの情報を記載しています．\nserver.bind = 0.0.0.0 server.port = 65432 server.admin.bind = 0.0.0.0 server.admin.port = 65433 server.access-log.pattern = json server.access-log.path = /var/log/digdag/access_logs log-server.type = local # database情報 # database.type = memory database.type = postgresql database.user = digdag database.password = digdag database.host = postgres database.port = 5432 database.database = digdag database.maximumPoolSize = 32  サーバーモードで起動すると，ワークフローの情報を保存するために，データベースの設定が必要になってきます．今回はPostgreSQLを別コンテナで立てて，Digdagコンテナと接続することにしています．ちなみに，これらの情報をインメモリで保存することもできます（この場合は，database.type = memoryとして下さい）．\ndocker-compose.ymlファイルを作成する 色々と試して上手くいかなかったのですが，最終的には以下の内容で落ち着きました．service共通の定義はx-templateにまとめています．serviceは3つありますが，Digdagを使う場合はdigdagとpostgresのみを立ち上げて使います．\ndockerのdepends_onは依存関係（起動順序）を指定できますが，DB起動後のアプリ起動までを制御できるわけではないので，postgresの起動完了前にdigdagがアクセスしてしまい起動失敗する事があります．\n Control startup and shutdown order in Compose  このため，condition: service_startedと設定することで，postgresが起動後にdigdagが立ち上がるようにしています．\ndepends_on: postgres: condition: service_started  また，ttyをtrueに設定しているのは，コンテナが正常終了して止まらないようにするためになります．\nversion: '3.8' # Common definition x-template: \u0026amp;template volumes: - ~/.gcp:/root/.gcp:cached - /tmp:/tmp env_file: - .env services: digdag: container_name: digdag build: . tty: true ports: - 65432:65432 - 65433:65433 volumes: - /var/run/docker.sock:/var/run/docker.sock command: [\u0026quot;java\u0026quot;, \u0026quot;-jar\u0026quot;, \u0026quot;/bin/digdag\u0026quot;, \u0026quot;server\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;digdag/server.properties\u0026quot;, \u0026quot;--log\u0026quot;, \u0026quot;/var/log/digdag/digdag_server.log\u0026quot;, \u0026quot;--task-log\u0026quot;, \u0026quot;/var/log/digdag/task_logs\u0026quot;] depends_on: postgres: condition: service_started \u0026lt;\u0026lt;: *template postgres: image: postgres:13.1-alpine container_name: postgres ports: - 5432:5432 environment: POSTGRES_DB: digdag POSTGRES_USER: digdag POSTGRES_PASSWORD: digdag volumes: - /tmp/data:/var/lib/postgresql/data tty: true \u0026lt;\u0026lt;: *template embulk: container_name: embulk build: . \u0026lt;\u0026lt;: *template networks: default: external: name: teamaya  Digdagをdockerコンテナで実行する方法 では，Digdag UIを使うためにDigdagをサーバーモードで起動します．\n# コンテナの立ち上げ docker compose up -d digdag  バックグラウンドでコンテナを起動しておきます．docker psコマンドでコンテナが立ち上がっていることを確認したら，http://localhost:65432/でUIにアクセスします．以下の画面が表示されたらOKです．\nNew projectからNameを設定したら，Add fileをクリックして，digファイルのコードをコピー\u0026amp;ペーストします．貼り付けたら，Saveで内容を保存します．そしたら，Workflowsのタブを選択し，先ほど追加したワークフローが表示されているのでクリックします．右上のRunボタンを押して実行が完了すると，下図のような結果になります．\nStatusがSuccessになっていれば，正常終了でBigQueryにデータが入っていると思うので，確認してみて下さい．\nおわりに 今回は，手元にあるスプレッドシートのデータをDigdag/Embulkを用いてBigQueryに連携するを紹介しました．\nやっぱり，UIで直感的に状況や情報がわかるのはメリットだなと思います．複数のワークフローが動作したりする環境だとそれらも管理できるので良いと思います．スケジュール実行やエラーや成功時の通知設定なども出来るので活用したいと思います．\nまた今回は，データ連携にDigdagを用いましたが，他にもApache AirflowやそのマネージドサービスであるCloud Composerを使っても同等のことができると思います．この辺りも別途試していきたいと思います．\n個人的な次のステップとしては，Dataformやdbtを使った「データの品質管理」に興味があるので，データをTransformしたり，テストしたりして考えていきたいです．また，データレイク/データウェアハウス/データマートの設計などを併せて学習していきたいと思います！\nこれとは別軸で機械学習パイプラインの設計もしていこうかなと考えています！\n参考  Digdag 入門 EmbulkとDigdagとデータ分析基盤と digdagをDockerizeしてECS上で運用することにしました - 雑なメモ Docker Compose の depends_on の使い方まとめ DockerのTTYって何?  ","date":"2021-12-22","permalink":"https://masatakashiwagi.github.io/portfolio/post/integrate-spreadsheets-to-bigquery-with-digdag/","tags":["Dev","Data"],"title":"スプレッドシートからBigQueryへDigdagを使ったデータ連携"},{"content":"はじめに 最近，データエンジニアリングとMLOpsの領域への関心が個人的に高まっていて，何か個人プロジェクトで出来ないかと考えていたところ，普段スプレッドシートで記録している家計簿のデータを使って，データエンジニアリングとMLOpsの技術検証をしようと言う考えに至りました．\nそもそも，なんで上記領域への関心が高まってきたかというと，以前までは，ある問題設定に対して，どういったアプローチでその問題を解くかといったモデリング部分に興味があり，特徴量エンジニアリングや新しいアルゴリズムを試すといった部分が楽しいと感じていました．一方で最近は，そこで作ったモデルはそのままでは機能せず，何らかのシステムに載せて動かし続けることで真価を発揮すると思っていますし，動かし続けて使っていかないと意味がなく，せっかく作ったモデルがもったいないなーという気持ちがあります．\n機械学習モデルのシステム化のためには，どういったことを知っておくといいのかを考えた場合，データ連携を含めたデータを貯める部分とモデル作成後のMLOps部分を理解しておくことが大事だと思うので，ここをより深めて行きたいと思った次第です．\nデータ連携周りは自分自身でモデルを作る際にも，「簡単にデータを取り出せる，品質が保たれているデータ」というのは大事な部分であり，世の中的にもデータを活用する流れはこれからも進んでいき，データ基盤を作れる人の価値は上がっていくと思うので，じゃあそこを自分でも出来るようになっておこうという思いがあります．\nもう一つのMLOpsは構成要素が色々あり技術的にも興味深い（サービス名とかは知ってるけど，どうやって使うの？的な部分もあります）のと，まだまだこの分野の知見は日本でも多くないなという印象なので，しっかり理解しておくと今後より活きてくるかなと思っています．\nそういった流れの中で，理解も含めて個人プロジェクトでできることをしたいというお気持ちがです．\n技術スタックとしては，普段BigQuery以外は業務でもあまり触れる機会がないGoogle Cloud Platform (GCP) のサービスとOSSを組み合わせてできることをしようと考えています．\nプロジェクト名: teamaya プロジェクトを始めるにあたりプロジェクト名を決めるとワクワクするので，最初に決めることにしました．沖縄好きなので，沖縄の言葉を組み合わせて何か名前を付けたいと考えていて，沖縄県と東京都のハーフである妻にも相談しながら今回の「teamaya」と言う名前にしました．\ngithubのリポジトリは以下になります．（スター貰えると泣いて喜びます😂）\n teamayaは\u0026quot;team\u0026ldquo;と\u0026rdquo;maya\u0026ldquo;を組み合わせた造語です．teamは「〔動物に引かせて～を〕運ぶ、運搬する」という意味があり，maya（マヤー）は沖縄の方言で猫という意味があるので，猫にデータを運んで貰うという意味を込めてこの名前にしました．\nちなみに，リポジトリにあるアイコンのハイビスカス🌺を付けた猫😸は妻に書いて貰いました！\nどういったプロジェクトなのか 今のところ考えているのは，大きく2つになります．\n データ連携  ローカルのデータをクラウド上に連携 連携したデータをELTツールで変換 \u0026amp; データの品質管理 データの可視化   機械学習システムの構築  パイプライン構築 実験管理 モニタリング \u0026amp; 通知    まず，データ連携については．ローカルからクラウドへの連携，特に特定のデータソースからBigQueryに連携する部分を実施して，データウェアハウス（DWH）を作ろうと思っています．そこからELTツールでデータマート（DM）を作成し，BIツール（Data Studioなど）でデータの可視化を行うといった流れを検証しようと考えています．\n個人的には，DataformやdbtといったサービスでELTパイプラインを作って，データ変換やデータの品質管理を一通り試してみたいと思っています．\n機械学習システムの構築に関しては，機械学習モデル作成のためのパイプラインや実験管理などを行いつつ，コードのテストやデータ・予測結果のモニタリングなどをできるようにしたいのと，Feastなどのfeature storeを試したりもしたいなと思っています．\n現状では上記2つを構築していきたいと思ってますが，進めていく中で興味が湧いたものを適宜取り入れていきたいなと思っています．\nおわりに 今回はプロジェクトを新しく始めたので，その紹介になります．ローカルからクラウドにデータ連携する部分については，既に作成しているので別途内容を紹介したいと思います．\n試行錯誤しながら面白そうなものは色々と取り入れて進めて行きたいと思うので，もし面白いツールなどがあればTwitterで教えて頂ければ嬉しいです．\n参考  MLOps.toys  ","date":"2021-12-12","permalink":"https://masatakashiwagi.github.io/portfolio/post/personal-project-teamaya/","tags":["Dev","Poem"],"title":"teamayaという個人プロジェクトをはじめてみた"},{"content":"はじめに Buy Me a Coffeeというコーヒー1杯をクリエイター活動のサポートという形で寄付するサービスがあります．このサービスの存在は以前から他のエンジニアの個人ブログなどで知っていたのですが，@hurutoriyaさんが投稿されたこちらの記事を見て，僕も同意することが多かったので，自分の個人ブログでも導入してみたというお話です．\nクリエイターの活動をサポートするサービス Buy Me a Coffee以外にも最近は色々とクリエイターの活動をサポートする寄付サービスがあるので紹介しておきます．\n Buy Me a Coffee: コーヒー1杯（実際には$5）をクリエイター活動のためにサポートするサービス Ko-fi: 名前の通り，サポートしたい人に対してコーヒー代を送るというサービス OFUSE: 1文字2円でOFUSEレターやOFUSEコメントを書いてサポートするというサービス  Buy Me a Coffeeを導入しようと思った理由 @hurutoriyaさんが書かれた内容と概ね一緒ですが，お金が欲しいからというわけではなく（お金が欲しいなら広告を貼ると思います），感謝を伝える1つの手段として，コーヒーを1杯プレゼントするよーという表現が良いなと感じています．\nいいね！なども記事を書いた側からすると読んでくれた人からの嬉しいリアクションの1つだと思います．ただ，いいね！以上に読んだ人にとって価値がある技術記事などに対しては，このようなサポートで感謝を伝える方法があってもいいのかなと思います．サポートして貰った側からすると，誰かのためになってるという感覚をより感じることができますし，何よりもっと有益な内容を届けていこうというモチベーションにも繋がるなと思います．\nこれらの理由と僕もより良い記事を届けていきたいという想いから，今回Buy Me a Coffeeを導入してみようと思いました．\nおわりに 実装については，こちらのサイトからロゴやグラフィックがダウンロードすることができます．また，挿入する文字やそのフォント・色など自由にカスタマイズできるGenerateページがあり，そこからGenerateを実行すると，HTMLに使用できるimage codeが出来上がるので，それをコピーして簡単にサイトに導入することができます．\n今回は，Buy Me a Coffeeのサポートセクションをページ最下部に導入して，その導入したお気持ちを書いたポエムになります．\n参考  投げ銭サービスのBuy me a cofee をBlog に導入してみた コーヒー1杯で支援するサービス「ko-fi」と、開発者がコーヒーを奢られる仕組みの話☕ さまざまな収益化機能をひとつに。クリエイター応援プラットフォーム「OFUSE（オフセ）」誕生。  ","date":"2021-12-07","permalink":"https://masatakashiwagi.github.io/portfolio/post/implement-buy-me-a-coffee/","tags":["Poem"],"title":"Buy Me a Coffeeをブログに追加した話"},{"content":"はじめに 普段はPythonのSciPyというライブラリを用いて，ABテスト実施後の2群間における有意差を調べるために検定を行っていますが，Pythonを使っていないorこのようなライブラリに触れたことがない人でも簡単に検定が行えるようにスプレッドシートを使ってMann-WhitneyのU検定を実施したものになります．\n公開中のスプレッドシート → Mann-Whitney U-test in spreadsheet\nコピーしてご自由にお使い下さい（全自動でない部分があるので，ご留意下さい）．\nMann-WhitneyのU検定とは Mann-Whitney（マン・ホイットニー）のU検定（ウィルコクソンの順位和検定）とは，2つの母集団が特定の分布であることを仮定しないで，「2つの分布の重なり具合」を検定します．（ノンパラメトリック方法の一つ）\nこれは，2つの母集団の中央値の差に注目しています． → こちらの記事を見ると，中央値の検定というわけではないみたいです．（自分も誤って理解していました）\nU検定の特徴としては，外れ値の影響を受けにくいなどが挙げられます．一方でよく使われるt検定の場合，平均値を見ているので外れ値があるとその影響を受けます．そのため，外れ値除去などの対処が必要なケースが発生します．\nここで，U検定には以下の仮定があります．\n 2つの母集団は互いに独立 2つの母集団の分布が正規分布であると仮定できない 2つの母集団のサンプルサイズが同数でなくても良い  また，帰無仮説は以下になります．\n 帰無仮説: 2群間に差がない（2つの分布が等しい）  検定を行う手順 検定を行う手順を紹介します．まずA群とB群の2つの群を考えます．\n それぞれのサンプルサイズをn1\u0008, n2とした場合に，2つの群を混ぜたデータ(n1+n2)を用意します． 1のデータを昇順に並び替えます． 並び替えたものに対して，順位を割り当てます（ランク付け）．もし同順位を持つ要素が存在する場合は，順位の平均を計算し，その順位の平均を各要素に割り当てます． A群に属するサンプルの順位和を計算する（=R1） 同様にB群に属するサンプルの順位和を計算する（=R2）  ここまで計算すると，検定統計量（U値）は以下になります．\n$$ U_1 = n_1n_2 + \\frac{n_1(n_1 + 1)}{2} - R_1 $$ $$ U_2 = n_2n_1 + \\frac{n_2(n_2 + 1)}{2} - R_2 $$ $$ U = min(U_1, U_2) $$\nUが計算できたら，Mann-Whitney検定表を用いて有意差5%で棄却できるかどうかを確認します．\nα=0.05の表を眺めて，今回のサンプルサイズn1, n2に該当する値と計算したU値の大小関係を比較して，計算した値が小さい場合には，帰無仮説を棄却します，つまり有意差ありとなります．逆に計算した値の方が大きい場合には，帰無仮説を棄却できないので，有意差なしとなります．\nここで，サンプルサイズがn1\u0026gt;20またはn2\u0026gt;20の時は，検定統計量Uを標準化してz値を求めて，標準正規分布で近似する方法を用います．平均値，標準偏差，z値は以下の計算式で求めます．\n$$ \\mu_u = \\frac{n_1n_2}{2} $$ $$ \\sigma_u = \\sqrt{\\frac{n_1n_2(n_1+n_2+1)}{12}} $$ $$ z = \\frac{U - \\mu_u}{\\sigma_u} $$\nz値が計算できたら，標準正規分布表を用いて，該当するp値を見に行きます．\n(z値をスプレッドシートの組み込み関数であるNORMSDISTに代入して，1から引くことでp値を計算しています: 式=1-NORMSDIST(z))\n p≧αの時，帰無仮説を棄却できない p\u0026lt;αの時，帰無仮説を棄却する．つまり，有意差ありとなる  スプレッドシートでU検定を行う 公開しているスプレッドシートはこちらになります．（再掲）\nサンプルデータとしてグループABの身長のデータを載せています．こちらのデータを検定したい2群のデータに適宜変更して頂くと，#検定を行う手順 で紹介した方法に則ってp値の計算がされます．\nサンプルサイズがn\u0026lt;=20の場合は，U値での評価になるので，その場合はリンクにあるマン・ホイットニーのＵ検定表を用いて，該当する値から検定結果を見積って貰うと良いです．\n※ 補足:\nサンプルデータのデータ順序は意識しないで問題ありません，順序並び替えで自動的に昇順で並び変わります．ただし，順位については，スプレッドシートのマウスオーバーでセルの右下に表示される黒い部分をデータが存在している部分まで下にズラして貰う必要があります．（スプレッドシートを完璧に使いこなしているわけではないので，特に順位をつけてる部分が自動化できていないです．もしご存知の方は，方法を教えて貰えると大変助かります🙏）\nおわりに 今回は，スプレッドシートを使ってMann-WhitneyのU検定を試してみた内容になります．Pythonを使っていない非エンジニアの方でも検定を行えるようにスプレッドシートに実装しました．ABテストを実施して検定するまでを誰でも簡単にできるようになれば良いなと思ってます．実装していて，スプレッドシートって意外と組み込みの関数が用意されていることを改めて知ることができました😄\nまた，普段ライブラリを何気なく使っていますが，内部の計算方法やどうゆう手続きで出力されるのか，またその値をちゃんと理解して使っていかないとなーということを改めて感じました．例えば，scipy.stats.mannwhitneyuはp値を出すだけであれば良いが，U値を使いたい場合には少し使いづらいと感じました．\nP.S. t検定についてもスプレッドシートで実施できるようにしているので，また紹介したいと思います．\n参考  Mann–Whitney U test Mann-Whitney Table 標準正規分布表 スプレッドシート - NORMSDIST Divine, et al. (2018) Mann-Whitney検定は中央値の検定ではない  ","date":"2021-11-16","permalink":"https://masatakashiwagi.github.io/portfolio/post/mann-whitney-utest-in-spreadsheet/","tags":["Data Science","Statistics"],"title":"スプレッドシートで行うMann-WhitneyのU検定"},{"content":"はじめに AWSのSageMaker上でSageMaker Python SDKを使用して独自の機械学習モデルを作成することができますが，その際に学習や評価が行える Estimator というSageMakerのinterfaceがあります．\n一方で，SageMaker Experimentsで実験管理を行いたい場合には，このEstimatorに色々と渡してあげる必要があります．\nその中でも学習時に出力されるlossの値や評価メトリクスを記録するためには，Estimatorのmetric_definitionsに正規表現を記述してログから上手く取得する必要があります．\nこれをより簡単にするために，CustomCallback関数を作成した話になります．\nCallback関数のカスタマイズ Tensorflow，厳密にはKerasのCallback関数をカスタマイズします．tf.keras.callbacks.Callbackクラスを継承したCustomCallBack(tf.keras.callbacks.Callback)クラスを作成します．この作成したクラスをmodel.fit時に引数のcallbacksに渡してやることで使用することができます．\n今回はSageMaker Experimentsで使うことを想定したもので，Estimatorのmetric_definitionsに渡すRegexとして，以下のようなログが出力されて欲しいとします．（メトリクスはRMSEとした場合を想定） MetricDefinitionsはこちらが参考になります → Define Metrics\nsagemaker.estimator.Estimator( ..., metric_definitions={ {'Name': 'Train Loss', 'Regex': 'train_loss: (.*?);'}, {'Name': 'Validation Loss', 'Regex': 'val_loss: (.*?);'}, {'Name': 'Train Metrics', 'Regex': 'train_root_mean_squared_error: (.*?);'}, {'Name': 'Validation Metrics', 'Regex': 'val_root_mean_squared_error: (.*?);'}, } )  しかしながら，Kerasでモデルを作成する際のデフォルトでは，学習時のLossは「loss」，メトリクスは「root_mean_squared_error」でprefixが無い状態になります．これをCallback関数をカスタマイズすることでprefixに「train_」を付けて，Regexで簡単に取得したいという気持ちです．\nTensorflow公式ドキュメントのWriting your own callbacksやtf.keras.callbacks.Callbackを参考に作成しました．\nimport tensorflow as tf class CustomCallback(tf.keras.callbacks.Callback): def on_train_begin(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of training. \u0026quot;\u0026quot;\u0026quot; def on_train_end(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of training. \u0026quot;\u0026quot;\u0026quot; def on_epoch_begin(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the start of an epoch. \u0026quot;\u0026quot;\u0026quot; def on_epoch_end(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of an epoch. \u0026quot;\u0026quot;\u0026quot; def on_train_batch_begin(self, batch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of a training batch in fit methods. \u0026quot;\u0026quot;\u0026quot; def on_train_batch_end(self, batch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of a training batch in fit methods. \u0026quot;\u0026quot;\u0026quot;  上記コードの中で必要なものを修正すれば大丈夫です．今回は学習の開始終了とエポックの終了時に呼ぶように修正しました．\nimport tensorflow as tf import datetime class CustomCallBack(tf.keras.callbacks.Callback): def on_train_begin(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of training. \u0026quot;\u0026quot;\u0026quot; print(f\u0026quot;Start training - {str(datetime.datetime.now())}\u0026quot;) # get parameters self.epochs = self.params['epochs'] # the epoch when training is stopped self.stopped_epoch = 0 # initialize the best loss as infinity self.best_loss = np.Inf # list of best metrics values self.best_metrics_values_list = [] def on_train_end(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of training. \u0026quot;\u0026quot;\u0026quot; if self.stopped_epoch \u0026gt; 0: best_values = ' - '.join(self.best_metrics_values_list) print(f\u0026quot;Epoch {self.stopped_epoch + 1}: early stopping\u0026quot;) print(f'Final results: {best_values}') print(f'Finish training - {str(datetime.datetime.now())}') def on_epoch_end(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of an epoch. \u0026quot;\u0026quot;\u0026quot; keys = list(logs.keys()) metrics_values_list = [] for key in keys: if key.startswith('val_'): metrics_values_list.append(f\u0026quot;{key}: {logs.get(key):.4f};\u0026quot;) else: metrics_values_list.append(f\u0026quot;train_{key}: {logs.get(key):.4f};\u0026quot;) values = ' - '.join(metrics_values_list) print(f\u0026quot;Epoch {epoch+1}/{self.epochs} - {values}\u0026quot;) current_loss = logs.get('val_loss') if np.less(current_loss, self.best_loss): self.best_loss = current_loss self.best_metrics_values_list = metrics_values_list else: self.stopped_epoch = epoch # fit時にcallbacksに作成したカスタマイズクラスをインスタンス化したものを渡す model.fit( ..., callbacks=[CustomCallBack()], )  出力は以下のような感じになります．今回はprint文で出力していますが，loggerを用意してlogger.infoを使うのも良いかと思います．\nStart training - 2021-11-09 23:48:12.787257 Epoch 1/10 - train_loss: 4.6889; - train_root_mean_squared_error: 2.1654; - val_loss: 11.1416; - val_root_mean_squared_error: 3.3379; ... Finish training - 2021-11-09 23:48:15.095133  おわりに 今回はSageMaker Experimentsで実験管理を行う上でログ出力の形を修正したいという動機からCallBack関数をカスタマイズしました．Callback関数の中身を知るためにソースコードを読んだりして勉強になりました．Tensorflowのフレームワークは拡張性があり，カスタマイズの方法もドキュメントに整備されているので，比較的容易に修正できると思います．今回は時間経過や予測時間の表示は省いてしまったので，余裕があればログにこれらを出力するようにしていきたいです．\n追記  2022/02/24: 更新  earlystoppingに対応する形式にCallBack関数を修正しました．current_loss = logs.get('val_loss')でlogsからgetするvalueはcallbacks.EarlyStopping(monitor='val_loss')でmonitorに指定している値になります．\n参考  Sagemaker Training APIs - Estimator Writing your own callbacks tf.keras.callbacks.Callback  ","date":"2021-11-10","permalink":"https://masatakashiwagi.github.io/portfolio/post/customize-tf-callback/","tags":["Dev","Machine Learning"],"title":"TensorflowのCallback関数をカスタマイズ"},{"content":"はじめに 6月から友人の@navitacionさんと一緒にPodcastの配信を開始しました．テック系の話題など自分たちが興味のある内容をあれこれと話すものになります．\nAnchorという無料で配信できるプラットフォームを使用しており，チャンネル名は「Double-M2.fm」になります．\n 各Episodeの要約もありますので，興味を持って頂いた方はこちらから見ることができます．\n この記事では，naviさんと始めたキッカケと実際の配信方法をまとめています．配信方法はあまり検索しても記事が無かったりしたので，これから始めようとしている人の参考になればと，そしてPodcast配信者が一人でも多く増えればなーと思います．\n配信のキッカケ 配信のキッカケは初回配信時にもチラッと触れたのですが，大きく2つ（+1）あります！\n コロナ禍で，オフラインイベントが無くなったことで，イベント後に参加者同士で雑談したり，情報交換したりする機会が無くなってしまったので，それを定期的にしたいという想いから アウトプットを意識するようになって，それを継続的に行っていく場の一つとして，音声による方法もあるのではという想いから 話のタネとしていいかなと笑  一つずつ説明していくと， まず①について，これは思っている人が多いんじゃないかなと思いますが，コロナ禍でオンラインでのイベントは多く開催されており，オフライン時に比べたら移動する手間もなくハードルが下がって，参加人数の制限も実質無制限で，抽選漏れの懸念も無くなりと良いことも多くある一方で，イベント後に登壇者以外の人同士や登壇者との会話がほぼほぼ無くなって，雑談とか情報交換したりする機会がかなり減ったと個人的に感じています．\nそういった状況で，会社の人以外の人と雑談など会話する時間が圧倒的に減ったというのもあり，定期的に情報交換したり意見を言い合える場があると良いなーということで今回Podcastを始めました． （最近だと，Twitterが音声会話サービスのSpaceを始めて，そこで気軽に会話が生まれるのがスゴく良いなと感じてます！）\n②については，かなーーりサボり気味だったのですが，アウトプットちゃんとして行かないとなーという気持ちが高まり，それを強制的に行う一つの方法だったりもします！ Podcastで話すために，何かしらの話題を探したり，書籍・論文などを読んで調べたりとトピックの内容を整理する必要があると思います．また，内容を自分の言葉で相手に説明することで理解の助けになると思っています．（人に説明してみると，思ったよりわかってないなーと感じることあるあるだと思います）\nアウトプットの方法の一つとして，ブログにまとめるということ以外に音声でもやろうかなといったところです． あと，普段から「これ！Podcastで話せるかも」とった意識をするだけで理解の仕方が変わると思うので，この習慣を付けたい狙いも個人的にあります！\n最後に③について，これはTwitter上ではお互い認知していても，リアルだと会ったことない人との話のネタの一つにしようかなという魂胆です笑\n配信方法 配信するプラットフォームですが，これは僕が聞いている他のPodcast配信者のを参考にして，Anchorにしました．Anchorは携帯でも簡単に録音することができて，それを配信できちゃったりもします．\n以下が僕たちが配信している各種構成になります．\n※ 全て無料で行うために，これらの構成になっています，有料でもいい場合はここで紹介したような複数ツールを使わなくて済むと思います．\n メール: Gmail 配信プラットフォーム: Anchor 録音: Zencastr 画面共有: Google Meet スクリプト: Scrapbox + GitHub 編集: Audacity 音楽 (BGM): Evoke Music アイコン: Canva  1. メール  各種サイトにアカウント登録する際にはメールアドレスが必須なので，まず共通で利用するメールアドレスを取得しました． 簡単に作成できるので，Gmailを新規作成し利用しています．  2. 配信プラットフォーム  最初にも書きましたが，配信プラットフォームはAnchorを採用しています． アカウントを作成し，Podcast name, descriptionなどを設定するだけで大丈夫です．無料で使うことができるのでおすすめです！ New Episodeから録音したり，既にある音源をアップロードすることも可能になっています．また，BGMなども用意されています． ただ，遠隔地にいる人同士での複数人録音は出来なさそうだったので，僕たちは録音は後述の別ツールを使うことにしました． 音源をアップロードすると，スケジュールでの投稿予約ができます．そして，最初のEpisodeが登録されると，自分たちだけのPublic Siteが生成され，そこで新しいPodcastを聴くことができます．  WHERE TO LISTENの部分ですが，新しいPodcastが配信されたら，RSSのクローラーが検知して，色んなアプリで聴くことができます！ただし，SpotifyとApple Podcastsに関しては，RSSのURLを登録する必要があります．詳しくはこちらのnote記事が参考になるかと思います．ちなみに，RSSのURLは配信が開始したら表示されるSettings-\u0026gt;Distributionから確認することができます．\n3. 録音 録音はAnchor上では行わず，Zencastrというツールを使いました．またZencastrは複数人の録音も行うことができ，非常に便利です！\n プランは無料のHobbyistと有料のProfessionalがあります．僕たちは無料プランを使っていますが，制限としては以下になります．\n 無料プランの制限：  1．ゲストは2名まで 2．1ヶ月あたり8時間の録音時間    基本的に二人での配信かつ週1回30分程度の録音時間なので，無料プランで全く問題ありません．また，Zencastrでは画面の録画もできるみたいです．（※コロナの期間は，無料プランでもゲストと録音時間が無制限になっているみたいです）\nこちらもAnchor同様，アカウント作成後，Create New Episodeで新規作成を行うと画面と音声の録画・音声録画（画面表示あり）・音声録画のみの3パターンから選択します．\nタイトルを入れて作成すると，下記のような画面が出ます．\nここで，Inviteタブをクリックすると同時に会話する人のメールアドレスにリンクを送付する形で招待することができます．Inviteした人が入ってくると，そのまま通話状態になり録音を開始することで，そのまま両方の音声を録音することができます．\nあとは，録音終了後参加していた人の音声をMP3でダウンロードすることができます．（これは管理者のみ可能な操作）\n僕たちは使わなかったですが，Macでネット通話の音声を録音する方法（Soundflower, LadioCast, GarageBand）の記事を紹介しておきます．→ Macでネット通話の音声を録音する方法（Soundflower, LadioCast, GarageBand）\n4. 画面共有 スクリプトなり資料なりを見ながら会話がすることが多いので，画面共有が必要になってきます．そのために，Google Meetを使って画面共有しながら会話するようにしています．\n5. スクリプト（台本・要約など） 収録を行う前に，事前に30分程度会話して何を話すか決めています．その際に，Scrapboxを活用して台本を作成したり，ネタ帳なども雑多に書いています．あとは，共有しておきたいことをScrapboxに書いて基本的にはここを見ながらいつも収録しています．\n参考にした記事 → 初めてのポッドキャスト、試して気づいた「声」の醍醐味と9つのティップス\nScrapboxの他には，GitHubも使ってます．こちらは，Organizationを作成し，そこにPodcast用のrepositoryをさらに作成して，自己紹介ページや各配信の収録内容の要約を書き記しています．ページの更新時にはissueを作成し，何をしたか記録が残るようにしています．\n6. 編集 編集に関しては，特別何かしているわけではないです．なるべく時間をかけず無理なく進めていきたいと思っているので，行っている内容としては下記2点です．\n 音量調整・BGM挿入 収録中に予期しない割り込みが入って，収録を止めた際のトリミング・不要な会話の削除  一通り自分たちの会話を聞いて，気になる部分があればトリミングなどしているぐらいになります．\n編集ソフトはAudacityを使っていて，昔からある音楽編集ソフトになります．Windows・Macどちらも使用することができます． Macの場合だと初めから入っているGarageBandなども使えるみたいです！\n7. 音楽 (BGM) Podcastを配信する時には，自分たちの音声に加えてBGMを追加しています．ただ，自分たちのコンテンツにBGMを使用する場合には，著作権などが絡んでくるので，安易に好きな曲を使用することはできません．楽曲1つ1つ確認するのは大変なので，今回は著作権フリーでAIが曲を作成してくれるEvoke Musicというサイトの音源を使用しています．\nこのサイトでは，キーワードを指定することで，そのキーワードに合った曲をAIが自動で作成してくれます．1曲あたり数分程度の曲になります．\n※ β版の時は無料で使えてましたが，今は有料になってしまったみたいです．\n8. アイコン（カバーアートなど） Anchorにはカバーアートが設定できるので，そのアイコンなどを作成するために，Canvaというデザインサイトを利用しました．Canvaは豊富なテンプレートデザインがあるので，それをベースに自分でいい感じに編集するだけでかなりオシャレなロゴなどを作成することができます．\n無料プランだと保存時の画像サイズや解像度などを変更できないので，少し残念ですが，かなりオススメのサイトです．有料だともっと出来る幅が増えそうですが，現状だと無料プランでも十分かなと思っています．\nおわりに 今回は6月から開始したPodcastについて，やろうと思ったキッカケとその配信方法をまとめました．配信方法については意外と記事がなかったので，もしこれから配信しようと考えている人の参考になれば嬉しいです！\n今回紹介した方法以外にも，もっと良い配信方法があると思うので，ご存知の方は是非教えて欲しいです！！一人でも多くのテック系Podcastが増えて盛り上がると良いなと思います！！\nP.S. 細かい設定や登録など聞きたいことがある場合には，遠慮なくTwitterなどでご連絡頂ければと思います．\n","date":"2021-06-13","permalink":"https://masatakashiwagi.github.io/portfolio/post/podcast-broadcast-method/","tags":["Poem"],"title":"Podcastによる配信のキッカケとその方法"},{"content":"Kaggle-Shopeeコンペの振り返り 2021/03/09~2021/05/11まで開催していたShopeeコンペの振り返りになります．\n2週間程度しか手を動かせなかったですが，久しぶりに参加したので備忘録として記録を残しておきます．\n最終的な結果は179th/2464で銅メダルで，特に凝ったことは何もしていなかったので，妥当かなと思います．\nこのコンペは上位10チーム中7チームが日本人チームで，日本人のレベルの高さを改めて実感できるコンペでした！\n概要 コンペの内容は簡単に言うと，画像とテキスト情報を用いて、2つの画像の類似性を比較し，どのアイテムが同じ商品であるかを予測するコンペになります．\n 開催期間: 2021/03/09 ~ 2021/05/11 参加チーム数: 2464 予測対象: posting_id列にマッチする全てのposting_idを予測する．ただし，posting_idは必ずself-matchし，グループの上限は50個となっている． データ: 投稿ID，画像，商品のタイトル，画像の知覚ハッシュ(perceptual hash)，ラベルグループID 評価指標: F1-Score その他: コードコンペ  My Solution  画像特徴量・テキスト特徴量・画像のphash値をconcatして結果をユニーク化したものを最終的な予測値としました． 何も複雑なことはしていないモデルですが，結果的に銅メダルを取ることができました．  Image Model  eca_nfnet_l0  loss: ArcFace pooling: AdaptiveAvgPool2d scheduler: CosineAnnealingLR loss(criterion): CrossEntropyLoss size: 512*512   eca_nfnet_l1  loss: CurricularFace pooling: MAC scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512   efficientnet_b3  loss: ArcFace pooling: GeM scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512   swin_small_patch4_window7_224  loss: CurricularFace scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 224*224   common  augmentation by albumentations.  HorizontalFlip VerticalFlip Rotate RandomBrightness   optimizer  Adam      少し工夫したポイント  画像特徴量を抽出する部分で，いくつか工夫した点をあげます．  CNN Image Retrievalを参考にしました．     lossにArcFaceとCurricularFaceを用いた  ArcFaceを使っている人が多かったが，CurricularFaceも使いました．スコア的にはCurricularFaceの方がよかったです．   いくつかのモデルでpooling層にGeMとMACを用いた  Google Landmark Retrieval Challengeで上手くいっていたGeMやMACなどのプーリング手法を用いました．   loss(criterion)にFocalLossを用いた 最終的に得られた特徴量をconcatした後，ZCAWhiteningによる次元削減を行った  有効でなかったもの  一方で，上手くいかなかった内容としては以下になります．  resnext50_32x4d swin_small_patch4_window7_224 with ArcFace CosFace, AdaCos PCA Whitening (worse than ZCAWhitening)    Text Model  paraphrase-xlm-r-multilingual-v1  loss: ArcFace scheduler: linear schedule with warmup loss(criterion): CrossEntropyLoss optimizer: AdamW   TF-IDF  textの方のモデルは特に改良する時間が取れなかったので，ほとんど手を付けれてなかったです．\n transformerとTF-IDFで得られた特徴量それぞれに対して，Cosine Similarityを計算し，テキストのpredictionを作成しました．  最終的な予測値は画像特徴量とテキスト特徴量に加えて，画像のphash値を追加して，ユニークを取った値としています．\n反省  post-processingが全然できていなかった  他の解法を見るに，post-processingでスコアが伸びているので，この部分は結構大事だったんだなと感じています． 6位の解法にもありましたが，今回のコンペでは，label_groupの長さが2以上であることから，「予測した結果のposting_idが1つしかない場合，強制的に似たものを持ってきて，2つにする」というアイデアでLBがかなり上がるみたい   ImageとTextをMulti-modal的にモデルに組み込んで学習することができていなかった グラフ理論全然わかってないです笑 細かい部分  他のoptimizerを試す  SAMとか   Database-side feature augmentation (DBA) / Query Extension (QE)  全然知らなかったので，End-to-end Learning of Deep Visual Representations for Image Retrievalを読んで勉強したい   閾値の調整 テキストモデルの追加     ここからは上位の解法を書きたいと思います．数もそれなりにあるので，載せるのは上位5つにします．最後に上位5つ以外にもDiscussionsに投稿されている解法のリンクを載せておきます．\n1st Place Solution 解法はこちらになります: 1st Place Solution - From Embeddings to Matches\nModel  Image: 2つのモデル  eca_nfnet_l1 * 2   Text: 5つのモデル  xlm-roberta-large xlm-roberta-base cahya/bert-base-indonesian-1.5G indobenchmark/indobert-large-p1 bert-base-multilingual-uncased   loss: ArcFace poolingした後にbatch normalizationとfeature-wise normalizationを行った ArcFaceのチューニングを行った  学習段階で徐々にmarginを大きくした（埋め込み表現のqualityに影響する）  画像に対するmargin: 0.8~1.0 テキストに対するmargin: 0.6~0.8   marginを大きくすると，モデルの収束に問題が出たので，以下を行った  warmup stepsを大きくする cosineheadに対するlearning rateをより大きくする gradient clippingを行う   class-size-adaptive marginも試したが，不均衡性が小さかったため，改善は少しだけだった  image: class_size^-0.1 text: class_size^-0.2     global average poolingの後にFC層を追加するとモデルが悪くなるが，feature-wise normalizationの前にbatch normalizationを追加するとスコアが改善した  Features  Combining Image \u0026amp; Text Matches  マッチングさせる方法をいくつかトライした   画像のembeddingによるマッチングとテキストのembeddingによるマッチングを結合する 画像のembeddingとテキストのembeddingをconcatしてからマッチングする 1と2を組み合わせてマッチングする   これは，画像のembeddingが強く示唆するアイテム・テキストのembeddingが強く示唆するアイテム・画像とテキストのembeddingがどちらも適度に示唆するアイテムを取り入れることができる     Iterative Neighborhood Blending (INB)  QE/DBAとは少し異なる，embeddingからマッチする商品を検索するパイプラインを作った k-Nearest Neighbor Search:  近隣探索ラリブラリ: faiss (k=51: 50(自身以外)) + 1(自身))   Threshold:  コサイン類似度をコサイン距離(=1-コサイン類似度)に変換し，distance\u0026lt;thresholdを満たす(matches, distances)のペアを取得した 閾値処理を行う場合，1つのクエリに対して少なくとも2つ以上のマッチがあることがコンペで保証されているので，distanceがmin2-thresholdを超えた場合にのみ，二番目に近いマッチを除外する   Neighborhood Blending:  kNNによるサーチとmin2による閾値処理をした後，各アイテムの(matches, similarities)ペアを取得し，グラフを作成する  各ノードはアイテム，エッジの重みは2つのノード間の類似性を示す 近傍のみが繋がっており，閾値条件とmin2条件を満たさないノードは切断される   近傍のアイテムの情報を使いたいので，クエリアイテムのembeddingを改良し，よりクラスタを明確にする  重みとして類似性を持つ近傍のembeddingを加重合計し，それをクエリembeddingに追加する    上図を簡単に説明すると，最初Aは[B,C,D]と繋がっているが，加重合計した結果閾値=0.5の場合，Cとの接続が切れて，Aは[B,D]のみと接続していることがわかる このNBの処理を評価指標の改善が止まるまで繰り返し実行する 最終的なNBのパイプラインは以下になる    About Threhsold Tuning:  調整する閾値は全部で10種類ある  stage1のtext, image, combinationの閾値が3つ stage2の閾値が1つ stage3の閾値が1つ 直接最終結合部分に繋がるstage1のtext, imageの閾値が2つ stage1~3のmin2閾値が3つ   最終的にはstage2,3の閾値を2つ調整するだけでよかった     Visualizations of Embeddings before/after INB  INBの効果を可視化したノートブックが公開されている  (SHOPEE) Embedding Visualizations before/after INB 見るからに各点が凝集されたクラスタを形成していることがわかる      Others  画像に対するCutMix(p=0.1) 画像のaugmentation  horizontal flipのみがよかった   madgrad optimizer: https://github.com/facebookresearch/madgrad 学習データ全体を使った学習  2nd Place Solution 解法はこちらになります: 2nd place solution (matching prediction by GAT \u0026amp; LGB), 2nd Place Solution Code\nSummary  1st stage: 画像・テキスト・画像+テキストデータに対するコサイン類似度を得るためにMetric Learningによるモデルを学習する 2nd stage: 同じlabel groupに属するアイテムのペアかどうかを識別するために\u0026quot;メタ\u0026quot;モデルを学習する  Model  image: 2つのモデル  backbone  nfnet-F0 ViT   loss: CurricularFace optimizer: SAM embeddingの結合: F.normalize(torch.cat([F.normalize(emb1), F.normalize(emb2)], axis=1))   text: 3つのモデル + TF-IDF  indonesian-bert multilingual-bert paraphrase-xlm   image+text: nfnet-F0とindonesian-bertのFC層のembeddingを結合したもの Training Tips:  label groupのサイズに基づいたSample Weighting  1 / (label group size) ** 0.4      Features Graph features  それぞれのアイテムのtop-Kコサイン類似度の平均と分散  K=5, 10, 15, 30, etc   標準化（mean=0, std=1） Pagerank  Others  テキストの長さ #の記号 Levenshtein距離 画像ファイルのサイズ 画像の高さと幅 Query Extension  Ensemble Methodology  ローカルデータでそれぞれのモデルのベストな閾値を計算する 上記閾値でそれぞれのモデルの予測値を差し引く 差し引かれた予測値を合計する 合計値\u0026gt;0の場合，アイテムペアが同じグループに属するとする お互いにエッジを持たないペアを削除する  A-Bはあるが，B-Aがない場合は両方を削除する    Post-Processing  中間中心性が最も高いエッジを再帰的に削除する（Graph-Based）  Others  Performance and Memory Tunings  CuDF, cupy, cugraph: GPUを有効に使うためには大事 ForestInference: 40分かかるCPUでの推論が2分になる   Not worked  Graph-basedの特徴量 固有ベクトルの中心性とjaccard End-to-end model Local feature matching    3rd Place Solution 解法はこちらになります: 3rd Place Solution (Triplet loss, Boosting, Clustering)\nModel  image: 2つのモデル  backbone  efficientnet_b2 ViT (DINO)     text: 2つのモデル (異なるtokenizersとCLIP)  indonesian-bert multilingual-bert   common  loss: TripletLoss (margin=0.1) 各エポック，label_groupsから重複したペアを作成し，各バッチでlabel_groupsから1ペアが挿入される．バッチサイズは128を使っていたため，バッチ毎に64の重複を取得し，ランダムな5点でそれらの各々を比較する 5fold CVで，validationスコアを計算する時は，validation-foldの中から候補を選ぶのではなく，学習サンプル全体から候補を探した  この方法はCV/LBのgapを抑えて，相関を取ることが出来る oofを用意して，2nd-levelの予測に使用する CVから得られた5つのモデルで，テストデータに対して推論を行った より速く推論するためには，validationなしの1つのモデルでテストデータにfitさせること   ArcFaceは上手くいかなかった    Features  複数モデルを用いて，異なる表現方法を作成するのが良い 全てのembeddingsから候補となる近しいものを結合し，ペアが実際に重複しているかどうかにかかわらず、binary targetを使用してペアオブジェクトのサンプルを作成する  3M点のペアがあり，重複率は4%だった   これらに対して，GBM(=CatBoost)を作成し，embedding毎に計算する  pairwise-distances (コサイン類似度，ユークリッド距離など) 両方の点周辺の密度 points ranks   最終的には500個特徴量を作成し，CVやLBを使いながら，重複確率による閾値の候補を出す\u0008 -\u0026gt; LB:0.76+  Post-Processing  クラスタリングを行ったが，embeddingsを使うのではなく，pairwise-distanceを使う 重複推定のためにGBM(=CatBoost)の確率を使い，類似度を求める 凝集クラスタリングのアイデアを採用  各単一点をクラスタとして開始し，平均的なクラスタサイズが閾値に等しくなるまでそれらをマージしていく クラスタが単一の場合，最も近傍のクラスタにマージする -\u0026gt; LB:0.79    Others  最適化するために  half precision(torch AMP)を使って学習と推論を行った 画像モデルの場合，画像の読み込みとリサイズにNVIDIA DALIを使った GBMの推論には，Rapids ForestInference Libraryを使った   上記の方法を使わないと，2時間で全てのモデルを推論することは不可能だった  4th Place Solution 解法はこちらになります: 4th Place Solution\nModel  image: backboneにnfnetとefficientnet  pooling: GeM＆Avg pooling embeddingが大きいほど，LBのスコアがよくなった loss: ArcFace marginの調整が大事だった   text: BERTベース + TF-IDF  TF-IDF: かなり大きいembeddingを生成し，notebook上ではメモリの制約を受けるため，Random Sparse Projectionで次元削減を行った loss: ArcFace marginの調整が大事だった    Ensemble  画像のembedding・テキスト（BERTベース）のembedding・TF-IDFのembeddingを結合して，正規化した これらのベクトル表現のそれぞれに対して，pairwiseコサイン類似度を計算し，3つの行列を作成した  最初に二乗し，その後加重平均を取って行列を結合した    Post-Processing  閾値の調整 Rank2 matching  もし，AがRank2にBを持っており，BはRank2にAを持っている場合，お互いに追加する   Rank2とRank3の違いが大きい場合，Rank2のIDを追加する 少なくとも1つの他とのマッチング（Rank2のコサイン類似度が極端に小さくなければ） Query Expansion マッチしていない行との再マッチング  その他上位解法のリンク  Kaggle ShopeeコンペPrivate LB待機枠＆プチ反省会 5th Place Solution 6th place solution 7th place solution 8th Place Solution Overview Public 16th / Private 10th Solution 11th Place Solution 14th Place Gold - Image Text Decision Boundary 15th place solution Public 13th / Private 16th solution 18th place solution - You really don\u0026rsquo;t need big models 19 place. Voting and similarity chain 26th Place Solution : Effective Cluster Separation and Neighbour Search [31st Place] Object Detection approach Public 39th / Private 37th Solution 48th Place Silver - Simple Baseline Public 56th / Private 57th Solution 62th Place Solution (Stacking Logistic Regression) 72nd place solution  ","date":"2021-05-14","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-shopee-solution/","tags":["Kaggle"],"title":"Kaggle-Shopeeコンペの振り返りとソリューション"},{"content":"はじめに 今まで仕事では，開発環境としてIntelliJを使っていたのですが，最近はVSCodeの人気が高くExtensionsも便利なものが多くあるということで，個人的な作業をする時はVSCodeを使ってみようと思って使っています． そんな中で，タイトルにもあるようにVSCodeからgit pushしようとしたら，\u0026lt;アカウント名\u0026gt;@github.com: Permission denied (publickey).とエラーが出たので，それを解消してVSCodeでgit pushできるようにした備忘録になります．\nエラー原因（SSH接続エラー） 「Permission denied (publickey)」とあり，GitHubにSSH接続するために，公開鍵を登録しておかないといけないのですが，それをしていなかったので，エラーが発生したということになります．\n以下のコマンドを打つことで接続を確認することができます．\nssh -T git@github.com \u0026gt; git@github.com: Permission denied (publickey).  ではどうすればいいかというと，鍵を生成してGithubに登録すればいいということになります．\n公開鍵と秘密鍵の作成 詳細な作成方法はこちらの記事が参考になります．\n簡単に手順を載せておきます．\ncd ~/.ssh ssh-keygen -t rsa -b 4096 -C \u0026quot;\u0026lt;メールアドレス\u0026gt;\u0026quot; -f github_rsa # オプションをいくつか設定して，鍵を生成 # 以下実行結果（一部マスクしてます） Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in github_rsa. Your public key has been saved in github_rsa.pub. The key fingerprint is: SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \u0026lt;メールアドレス\u0026gt; The key's randomart image is: +---[RSA 4096]----+ |=== | |.B o . | |o.. . * . o | |. . . B +. oo .o*| | . o * OSo.oooo*+| | . = + = o ..*..| | E . . o . . ..| | . . | | | +----[SHA256]-----+  鍵の種類をRSAにし，鍵の長さを4096にしています．ファイル名はgithub_rsaと設定しました．\nGithubに生成した公開鍵を登録 cd ~/.ssh ssh-add -K github_rsa # 秘密鍵をssh-agentデーモンに登録 pbcopy \u0026lt; github_rsa.pub # pbcopyコマンドで公開鍵の中身をクリップボードにコピー  この後は，コピーした公開鍵の中身をGithubに登録します．\nGithubのアカウントからSettingsに進み，SSH and GPG keysを選択し，New SSH keyを押して，先程コピーした中身をペーストし，名前を決めて保存します．\nSSH接続確認 保存が完了したら，SSH接続できるか確認するために，以下のコマンドを打って確認します．\nssh -T git@github.com \u0026gt;Hi \u0026lt;ユーザー名\u0026gt;! You've successfully authenticated, but GitHub does not provide shell access.  Remote設定の上書き ここまで来たら後一息で，最後にremoteの設定を上書きします． 以下のような感じでリポジトリ名を書いて，実行すればOK．\ngit remote set-url origin git@github.com:\u0026lt;ユーザー名\u0026gt;/\u0026lt;リポジトリ名\u0026gt;.git  VSCodeからgit push 今までの設定が完了していれば，上記画像の手順でVSCodeの画面から簡単にgitにcommitやpushなどの操作を行うことができます．\nさいごに まだまだVSCode初心者なので，使いやすいExtensionsを取り入れて開発環境をカスタマイズしていきたいと思います！\n参考  初めてのgitは5ステップで完了 GitHubでssh接続する手順~公開鍵・秘密鍵の生成から~  ","date":"2021-03-26","permalink":"https://masatakashiwagi.github.io/portfolio/post/vscode_git_connect/","tags":["Dev"],"title":"VSCodeとgitを連携してpushできるようにするまで"},{"content":"はじめに Pytorchでモデルを作成していた際に，「RuntimeError: CUDA error: device-side assert triggered」が発生し，原因がよくわからなかったので，調べたことをメモしておきます．\nエラー発生の原因 調べてみると，原因としては以下のようなものがあります．\n ライブラリのVersionが違う ラベル/クラスの数とネットワークの入出力のshapeが異なる Loss関数の入力が正確でない  などなど\u0026hellip;\nよくあるのが，下2つかなと思います．\nラベル/クラスの数とネットワークの入出力のshapeが異なる 想定しているラベルもしくはクラス数とネットワークの出力のクラス数が異なる場合，この場合はFC層の最後にnn.Linear(input, num_class)を入れて調整する必要があります．\nLoss関数の入力が正確でない 僕が遭遇したのはこちらのパターンになります．\n例えば，BCELossを考えた場合，計算するためには値としては0~1を取る必要があります．そのため普通は最終出力にSigmoid関数 or Softmax関数を入れるかと思います．\nそれ以外にもLossの設計で以下のようにしておくと良いかと思います．\nclass BCELoss(nn.Module): def __init__(self): super().__init__() self.bce = nn.BCELoss() def forward(self, input, target): input = torch.where(torch.isnan(input), torch.zeros_like(input), input) input = torch.where(torch.isinf(input), torch.zeros_like(input), input) input = torch.where(input\u0026gt;1, torch.ones_like(input), input) # 1を超える場合には1にする target = target.float() return self.bce(input, target)  他の解決方法 他にも調べていると解決方法としてCUDAの設定を以下にすると良いなどもありましたが，解決するかどうかはよくわからないです．\nCUDA_LAUNCH_BLOCKING=1  おわりに 今回は，Pytorchでのモデル作成時に発生したエラーについて整理しました，モデル作成時にはモデルのIn/OutやLoss関数の定義をきちんと理解し把握しておく必要があると改めて感じました．同様のエラーが起きた場合には，この辺りをまずは調べてみるのが良さそうです．\n参考  CUDA error 59: Device-side assert triggered  ","date":"2021-02-01","permalink":"https://masatakashiwagi.github.io/portfolio/post/cuda_error_device-side_assert_triggered/","tags":["Dev"],"title":"RuntimeError: CUDA error: Device-side assert triggeredの解決方法"},{"content":"Kaggle-MoAコンペにTeam 90\u0026rsquo;sで初参加 このブログは2020/9/4~12/1まで開催していたMoAコンペでの取り組みを紹介する内容です． （コンペの詳細な内容については割愛します）\n今回のコンペでは，同世代のメンバーでチームを組んで取り組みました！\nチーム結成の経緯は，Twitterでお互いが90年生まれということを知って，同世代でKaggleチーム組んで戦いたいねーという感じだったと思います． それが少し前のことで当時取り組める良い感じのコンペがなかったのですが，今回テーブルデータのコンペで取り組めそうということで始まりました． チームでの取り組みはとにかく学びが多く，終盤までモチベーションを保つことができたのが大きかったです．\nまた，議論することで理解なども深まっていくので，コンペを通してより成長できたんじゃないかなと思います．\n今回の僕たちのチームでの取り組み方を紹介すると\n1. 情報はSlackで共有 2. 分析方針や実験結果はGithubのissueで管理 3. 毎週末に2時間程度のディスカッション\nといった感じです．\n3番目の週末のディスカッションは強制ではなく，参加可能な人が参加する形式で運用してました． （と言いつつもみんな真面目に毎回参加してました笑） 今回はチームでの取り組み方針の具体的な内容について少しだけ掘り下げます．\n1. 情報はSlackで共有 Slackをどうゆう感じで活用していたのかというと，\nコンペのDiscussionやNotebookの内容について疑問点などを話し合ったり，それ以外にも進め方の相談や雑談などを基本的に行っていました． あとはsubmitする時は一言声をかけるなどのsubmit管理もしていました．\nこうゆうのがあれば良かったなーというところでは，新着のDiscussionやNotebookをKaggleから連携して通知する仕組みを用意しておければ尚良かったのかなと思いました．\n2. 分析方針や実験結果はGithubのissueで管理 Githubをどうゆう感じで活用していたのかというと，\n分析での実験毎に1つのissueを立てて，そこでどうゆう実験をしたのかsubmitした結果のスコアがどうだったのかなどを記録として残してました． また，共通で使える特徴量生成のコードだったり，CVの切り方のコードなどの共有も行ってました．\nその他にはDiscussionの内容を整理したり，情報をまとめるために活用したりしていました．\n3. 毎週末に2時間程度のディスカッション 週末にGoogle meetでオンラインディスカッションでしていて，そこで何をしていたかというと，\n基本的には，今週何をしたのかを各々共有したり，わからない部分を話し合ってどうゆうふうに次進めて行くかなどをチームで考えていました． あとは，次の週でどうゆうことをするかの方向性を決めて終わる感じでした．\nもちろん雑談や仕事での苦労を労ったりもしていました笑\n最終順位 最終順位は4373チーム中34位の銀メダルで、金メダルまであともう少しのところまで行ったので，とても悔しい結果となりました．\n個人的にはInferenceの処理がエラーで通らない状況に最後の3日ぐらいでなって泣きそうになりました． チームメンバーにはweight0の状態で非常に申し訳なかったなと思ってます泣\n学習時に回していたノートブックでは，スコアがチーム内で作ったモデルの中でも上位5つ以内に入っていたので，アンサンブル時には効いてただろうなと思うと尚更です． 個人的な成長としては，テーブルデータに対してNeuralNetが有効に作用する場面について多少理解が深まったと感じています．\n今回のMoAでは，マルチラベルの予測だったので，一度に大量のクラスを予測する場合にはNNが有効でかつGBDT系と比較して計算速度も速いんだなと感じました．\nまた，特徴量的にも交互作用的な部分はNN内部の中間層の組み方などで実現できるので，GBDT系みたく大量に特徴量を用意しなくても対処できるのが大きいのかなと思っています．（今回のケースだとGBDTで大量のモデルを作るとなると速度的な部分で特徴量が膨大になるとかなり厳しい感じでした）\nあとは，NNの実装をPytorchで行ったこともあり，Pytorchの扱い方がわかるようになったのは大きいと思っています．（仕事ではTensorflowだったりするので\u0026hellip;）\nPytorchでの実装に関してはもっと進めて行きたいのとコードの整理も合わせてして行きたいので，次に参加予定のコンペではその辺りも意識して挑めたらなーと思います．\n","date":"2020-12-11","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-moa/","tags":["Kaggle"],"title":"Kaggle-MoAの振り返り"},{"content":"初期化したリストの更新処理でハマってしまった 今回は，初期化したリストを更新した際にハマってしまった失敗があるので，備忘録として残しておきます． 詳しい内容は参考サイトに載っています．（かなりわかりやすいです！）\npythonで決まった形のリストを予め作成しておきたい場合に，以下のようにすることがあると思います．\nshape you want: [[0, 0], [0, 0], [0, 0]] # 要素が2つあるリストが3つ \u0026gt;\u0026gt;\u0026gt; list1 = [[0] * 2] * 3 \u0026gt;\u0026gt;\u0026gt; list1 [[0, 0], [0, 0], [0, 0]]  そして，上記リストを何かしらの値で更新したい場合を考えます． 今回だと，[0][0]の要素を更新するとします．\n\u0026gt;\u0026gt;\u0026gt; list1[0][0] = 3.5 \u0026gt;\u0026gt;\u0026gt; list1 [[3.5, 0], [3.5, 0], [3.5, 0]]  結果は，各リストの0番目の要素が全て更新されています． この原因は，list1 = [[0] * 2] * 3と書くと，要素のリストが全て同じオブジェクトになってしまい，どこかの要素を変更すると全て変わってしまうからです．\n対処法 上記の結果を回避するためにはリスト内包表記を使うと解決することができます． 先程の例の場合，以下のように書くといいです．\n\u0026gt;\u0026gt;\u0026gt; list2 = [[0] * 2 for i in range(3)] \u0026gt;\u0026gt;\u0026gt; list2 [[0, 0], [0, 0], [0, 0]]  内包表記を使うと，リストはそれぞれ異なるオブジェクトとして扱われます． ですので，[0][0]の要素を更新すると，意図した部分だけが更新され問題ありません．\n\u0026gt;\u0026gt;\u0026gt; list2[0][0] = 3.5 \u0026gt;\u0026gt;\u0026gt; list2 [[3.5, 0], [0, 0], [0, 0]]  リストを初期化する際はこれらに注意しておかないと，本来の意図とは違う動きになってしまいます．\nこうゆうミスを気にしたくない場合には，numpyのarrayで作りたいshapeを作成し，その後にリストに変換すれば良いかもしれないです．\n\u0026gt;\u0026gt;\u0026gt; np.zeros((2, 3)).tolist() # 0で初期化 [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]] # 0以外の場合 np.ones((2, 3)).tolist() # 1で初期化 np.full((2, 3), 5).tolist() # 任意の値で初期化  参考  Pythonのリスト（配列）を任意の値・要素数で初期化  ","date":"2020-09-12","permalink":"https://masatakashiwagi.github.io/portfolio/post/list-objects/","tags":["Dev"],"title":"ネストしたリストの更新処理"},{"content":"ポートフォリオ作成 こちらのQiitaの記事でHugoを使って簡単にポートフォリオを作成できるというのを見かけたので，以前まで使っていたpersonal pageを移植しました． 移植した際に少し詰まった部分があるので，Tipsとしてこの記事で紹介します．\nこの記事は以前に使用していたHugo Themeの内容になります\n最初はHugo Theme Cactus Plusというテーマで作成していたのですが，再度作り直してます． （再作成した理由は，少しだけ凝ったテーマを使って見たくなったためです笑） 作り直したテーマはHugo Future Imperfect Slimになります．\nHugoはシンプルなデザインが多いので非常にオススメです．\n基本的な構築方法は上記Qiitaの記事に沿って行っています． 別途追加した要素としては，最初に作成したHugo Theme Cactus Plusと作り直したHugo Future Imperfect Slimそれぞれありますので，この際どちらも紹介します．\n  Hugo Theme Cactus Plus\n メニューの追加設定 Custom-CSSの設定（custom-cssの設定は簡単に設定できますので，今回は割愛します）    Hugo Future Imperfect Slim\n faviconの設定 github.ioでサイトをhostした場合のpath設定    個人的には，1回目に作成したテーマより2回目の方が簡単でした．\nHugo Theme Cactus Plus: メニューの追加設定 Hugo Theme Cactus Plusのテーマでは，デフォルトでAbout/Archive/Tagsの3つがメニューとして存在しています． 今回はそこにProjectsを新しく追加しましたので，その方法を記載します． 実施することとしては，以下の4ステップになります．\n content配下にprojectsディレクトリを作成し，_index.mdファイルを配置する． 記事などのページ情報はcontentで管理します．  ├── content │ ├── about │ ├── posts │ └── projects │ └── _index.md   themes/layouts/partials配下にあるnav.htmlにProjectsのリンクを追記する． これはTagsなどのリンクをコピーして，nameの部分はprojectsに修正すれば大丈夫です． メニューバーにProjectsを表示させるために，この部分を修正する必要があります．\n  themes/layouts/section配下にabout.htmlをコピーして，projects.htmlにrenameする． ここに追加することで，セクションのトップページとして扱われることになります．\n  最後に，コマンドラインでhugoを実行する． hugoコマンドを実行することで，必要なものが自動生成・反映されます．\n  以上でメニューを追加することができます． （他のテーマでは，もう少し簡単にメニュー追加が可能なものもあります）\nHugo Future Imperfect Slim: faviconの設定 faviconを設定する方法は，下記の3ステップになります．\n まず，下記のデフォルトのconfig.tomlの内容のうち，faviconとfaviconVersionを変更します．  [params.meta] description = \u0026quot;A theme by HTML5 UP, ported by Julio Pescador. Slimmed and enhanced by Patrick Collins. Multilingual by StatnMap. Powered by Hugo.\u0026quot; author = \u0026quot;HTML5UP and Hugo\u0026quot; favicon = false \u0026lt;-- trueに変更する svg = true faviconVersion = \u0026quot;1\u0026quot; \u0026lt;-- 1を削除する msColor = \u0026quot;#ffffff\u0026quot; iOSColor = \u0026quot;#ffffff\u0026quot;   config.tomlを修正したら，static配下にfaviconディレクトリを作成する．\n  static/favicon配下にfavicon.icoとfavicon-32x32.pngを配置する． なぜfavicon-32x32かというと？\n layouts/partials/meta.htmlのrel=iconに以下が記載されている  favicon-32x32 favicon-16x16 site.webmanifest    なので，これに合わせて名前を変更するかwebmanifestを新しく作成し，その中に諸々の内容を記載する必要がある．\n  github.ioでサイトをhostした場合のpath設定 今回作成したサイトをgithub.ioでhostした場合に発生した内容です． 各メニューのURLとして，https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/home/などとなって欲しいのですが， https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/portfolio/home/とportfolioが重なってしまうエラーが発生しました．\n上記エラーを回避する方法の紹介になります． config.tomlファイルに各メニューの設定をする箇所があります．\n[menu] [[menu.main]] name = \u0026quot;Home\u0026quot; identifier = \u0026quot;home\u0026quot; url = \u0026quot;/\u0026quot; \u0026lt;-- /を削除する pre = \u0026quot;\u0026lt;i class='fa fa-home'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 1 [[menu.main]] name = \u0026quot;About\u0026quot; identifier = \u0026quot;about\u0026quot; url = \u0026quot;/about/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-id-card'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 2 [[menu.main]] name = \u0026quot;Blog\u0026quot; identifier = \u0026quot;blog\u0026quot; url = \u0026quot;/blog/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-newspaper'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 3  urlの部分で先頭の/を削除することで，上記問題を回避することできます．\n 以上で今回紹介する内容は終了になります．\n参考となる記事などがあまりなかったので，試行錯誤しながら行いました．\nそのため，もっと簡単にする方法が他にもあるかもしれないですので，もし他にあれば，Twitterなどでコメント頂けると大変助かります．\n","date":"2020-07-13","permalink":"https://masatakashiwagi.github.io/portfolio/post/hugo-portfolio/","tags":["Dev"],"title":"Hugoを使ったポートフォリオ作成"},{"content":"はじめに Kaggleを始めて半年ほど経ち，個人的にこの半年で得たものを整理するという意味で「kaggle その2 Advent Calendar 2019」の20日目を担当します．\n簡単な自己紹介として，普段は都内のベンチャー企業で主に製造業のお客さんを相手にデータ分析の仕事と自社製品の開発を8:2ぐらいの割合で担当しています．\n実はJTCから転職して今の会社は2社目で，働いて2年弱になります．\nちなみに前職のJTCでは，マーケティングオートメーションツール導入などのSE的なことをしてました．なので，バリバリデータサイエンスをしていたわけではないです笑\nKaggleは転職した時ぐらいから知って，すぐやり始めたいと思ってたのですが，色々と仕事プライベート共に余裕が無くて満を辞して半年ほど前から本格的に参加し始めました！\nKaggleを始めて得たもの 今回はそんな半年ほど前からKaggleを始めて得たものとして，大きく3つあり，それについて書きます．\n データサイエンス関連の知識 実務へのフィードバック 人との繋がり  それぞれ簡単ですが，書いていきたいと思います．\n1. データサイエンス関連の知識 よく言われていることですが，Kaggleは宝の山であり世界中のデータサイエンティスト・機械学習エンジニアの知恵や情報が特にコードレベルで共有されているのが魅力の一つです．掘れば掘るほど色々と出て来るので，これを活用しない手はないなという印象です．\nその中でも，特に個人的に得て良かった知見としては以下の4つかなと思います．\n1つ目は，\n 特徴量エンジニアリング  ドメイン知識に基づく特徴量エンジニアリングが有効であることはもちろん知っていましたが，それを作る発想であったり組み立て方が非常に勉強になってます． また関連して，どの単位で集約した特徴量を作るか，カテゴリカルデータやカウントデータの扱い，エンコーディングの仕方であったりと特徴量の作り方は非常に参考になってます．    2つ目は，\n パイプライン設計  パイプライン設計はKaggleを始めてから特に意識させられた部分になります．参考になる情報をいくつか上げておきます[1, 2, 3]．    これを意識して良かったこととしては\u0026hellip;\n 特徴量の管理が楽になる 試行錯誤した結果をログという形で後から確認できる 結果の再現性も容易になる 計算回した後は寝てられる笑  他にも色々と良いことはあるので，是非オススメしたいです！後々の再利用のためにも整理しておくと，一から全てを作り出さなくても良いので，有用かと思います．\n3つ目は，\n バリデーションの重要性  モデルの汎化性能を考える上では大事な要素で，Kaggleでは特にpubulic LBで上位に入っていても，バリデーションをきちんとしていないとprivate LBで大きくshake downしてしまう結果になることがよく？あるのかなという印象です．  学習データの結果が良くてもテストデータで全然良くないとなると使い物にならないので、この辺りは実務でも活きてくる部分になります．運用段階で全然使えないモデルが出来上がるのを回避できる方法の1つ．      4つ目は，\n NNをテーブルデータで使う方法  Neural Networkは画像認識の領域で使われてますが，それをテーブルデータに使う方法がKaggleでは見かけます．  テーブルコンペでは，GBDT系のアルゴリズムの方がまだまだ精度的には良いですが，モデルの多様性や特徴量抽出の自動化的な部分でNNモデルも十分に活用できると思ってます．   この辺りはもっとkernelなどで理解して自分の武器にしていきたいなという感じです．ただ，前までは選択肢にもなかった気がするので．様々な手法を見た結果得られた知見かなと．    2. 実務へのフィードバック Kaggleで実施する内容と実務での内容が必ずしも直結するわけではないですが，分析スキルの向上は実務でも大きく活きてます．\n例えば，実務でデータ受領後，EDAを進める中でデータの勘所を掴むのが以前より早くなったのと，何をどうすれば良いかを掴むのが以前よりスピードが上がったと感じてます．それによって，案件を進めていくスピードが上がったので，色々と試行錯誤できる時間を確保できるようになったと思います．\nまた，Kaggleで有効な手法を製品へフィードバックすることも進めているので，自分自身だけでなく会社へも還元できつつあるのかなという感じです．\n一方でKaggleが楽しすぎて，仕事中でもコンペのことが気になって手を動かしたくなったり，休日だいたい費やしてるので出不精になったりしてます笑\nTwitterでも書きましたが，良くも悪くも世界中の人たちと競い合って評価されるので，負けたくない精神は会社でも発揮されてます！\n人との繋がり Kaggleを始めてから，もくもく会や勉強会に参加する頻度が増えたかなと感じてます．大体最後には懇親会があるので，コンペの話や仕事の話で盛り上がって色々と情報交換が出来ていて楽しい限りです．\nあとは，コンペ終了後の反省会に参加することでコンペでの苦しみなどを共有できるのも良いコミュニティーだなと思います．そういった場で社内以外のデータサイエンティスト・機械学習エンジニアの方々とお話しできるのは非常に良い刺激になります．自分が知らないことを知ってる人がめっちゃいるので，勉強になりまくりです．\n前々から社外での繋がりを増やしたいと思ってたところでKaggleという共通の話題があるので，比較的話がしやすい環境ができてKaggle様様です．もっとKaggleでの繋がりを増やして，お互い切磋琢磨できる環境に持っていきたいですね．\nおわりに まだまだ半年しか経っていないですが，濃い経験や知見をKaggleを通して得られているので，これからも継続していきたいです！\n人との繋がり的には，もう少し仕事面でも色々と相談できる関係性を作って，どんなことをやっててどうゆう課題や問題意識があるかとか聞いてみたいです．\nあとは幸いにも，勉強会で知り合った方とコンペでチームを組んで頂けるようになったので，次はチームで参加する楽しみも味わいます！\n最後にKaggleでは、まさに以下の話を体現できるのではと思ってます！\n 頭で理屈をわかったところで，体感を伴わない知識は活用できない（ハリガネサービス）\n P.S. DSBコンペでメダルを獲得します！\n参考  [1] Kaggleで使えるFeather形式を利用した特徴量管理法 [2] データ分析コンペで使っているワイの学習・推論パイプラインを晒します [3] hakubishin3/kaggle_ieee  ","date":"2019-12-20","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-get-something/","tags":["Poem","Kaggle"],"title":"Kaggleを始めて半年経って個人的に得たもの"},{"content":"はじめに 今回は，先日初めて見たファイル形式のExcel Binary Workbook (xlsb)に関して，pythonでcsvにパースする話です．\n.xlsxはよくあるExcelファイルの形式ですが，それのバイナリー形式である.xlsbに関しての話です．（今まで見たことなかった拡張子です笑）\nExcelの闇やExcelとの格闘は色々ありますが，今回はそこをグッと堪えて進めたいと思います笑\n.xlsbとは Weblio辞書によると以下のように記載されています．\n .xlsbとは，Excel 2007で作成したブックを「XML形式でないバイナリブック」として保存する際に用いられる拡張子である．.xlsbでブックを保存した場合はファイル全体がバイナリ形式で保存され，XMLベースである.xlsxなどのファイル形式で保存した場合と比べて，ファイルサイズを数分の1程度に抑えることができる．\n 受け取ったファイルは.xlsb形式でも100MBぐらいあったため，.xlsx形式だとかなり容量が大きく，ファイルを開くと処理が重たくなることが想像できるので，圧縮したのだと考えられます．\nExcelを扱えるpythonライブラリ openpyxl 定番のopenpyxlです．\n上記ページにも記載されてますが，Excelファイルの拡張子である.xlsxを扱うことができます．\n openpyxl is a Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files.\n いつものようにこのライブラリで処理しようとしたところ，下記のようなエラーが発生しました．\nopenpyxl.utils.exceptions.InvalidFileException: openpyxl does not support binary format .xlsb, please convert this file to .xlsx format if you want to open it with openpyxl  もう一度openpyxlの説明を見ると，確かに扱える拡張子はxlsx/xlsm/xltx/xltmとなっているので、.xlsbは扱えないのが分かります．\nそこで，.xlsbの拡張子が扱えるライブラリを調べたところ，pyxlsbというのがあるみたいなので，それを使うことにしました．\npyxlsb pyxlsbは公式の説明にあるように，xlsb形式を扱えるpythonライブラリになります．\n pyxlsb is an Excel 2007-2010 Binary Workbook (xlsb) parser for Python.\n インストールはpipですることができます．\npip install pyxlsb  公式のサンプルコードを記載しておきます．\nimport csv from pyxlsb import open_workbook with open_workbook('Book1.xlsb') as wb: for name in wb.sheets: with wb.get_sheet(name) as sheet, open(name + '.csv', 'w') as f: writer = csv.writer(f) for row in sheet.rows(): writer.writerow([c.v for c in row])  もしpandasのデータフレームに変換したい場合は，参考ページのコードで可能となります．\nただし，時刻変換に関して少し注意が必要なので，記載しておくと公式にもある通り，日付はfloatに変換されてしまうため，convert_date関数を使う必要があります．\n Note that dates will appear as floats. You must use the convert_date(date) method from the corresponding Workbook instance to turn them into datetime.\n なので，元のファイルに時刻が入っている場合には上記変換をコードの中に入れて処理する必要がありますのでご注意下さい．\nprint(wb.convert_date(41235.45578)) \u0026gt;\u0026gt;\u0026gt; datetime.datetime(2012, 11, 22, 10, 56, 19)  今回は個人的に嵌ってしまった.xlsb形式のファイルを扱う方法を紹介しましたが，出来ればデータ分析をするようなデータをExcelファイルで扱いたくないのが本音です．\nもちろん簡単なデータの可視化とか表計算とかExcelが活躍する場面は多々あると思うので，使い分けていきたいとは思います．\n参考  Stack Overflow - Read XLSB File in Pandas Python  追記  2020/01/05: 更新  pandasの「version=1.0.0」で.xlsbファイルをロードできるようになったみたいです．方法はpd.read_excelの引数でengine=\u0026quot;pyxlsb\u0026quot;と指定するだけです．\n# Returns a DataFrame pd.read_excel(\u0026quot;path_to_file.xlsb\u0026quot;, engine=\u0026quot;pyxlsb\u0026quot;)  参考: https://pandas.pydata.org/docs/user_guide/io.html#io-xlsb\n","date":"2019-10-05","permalink":"https://masatakashiwagi.github.io/portfolio/post/excel_processing_using_python/","tags":["Dev"],"title":"Excel Binary WorkbookをPythonで処理"}]