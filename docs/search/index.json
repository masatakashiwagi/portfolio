[{"content":"はじめに もう今年もあと数えるほどで終わってしまいますね😅（去年も同じこと書いている笑）\n遅くなりましたが，この記事は MLOps Advent Calendar 2024 の24日目の記事になります！\n去年は「機械学習パイプラインの作り方を改めて考えてみる」というポストをしましたが，今年は「マルチテナント環境における機械学習適用」について考えていることや悩みを紹介したいと思います．\nこの内容を書こうと思ったモチベーションについて最初に説明すると，昨今 SaaS でのプロダクトやサービス提供が増えていて，ソフトウェアなどを複数のクライアント企業で共有するモデルであるマルチテナント方式が取られていますが，プロダクトの機能として機械学習（ML）サービスをこのようなマルチテナント環境で提供する際の考え方や悩みどころに関する記事はあまり世の中で見かけない気がします．（以前からこのような方式で機械学習を提供している会社はたくさんありそうに思うのですが\u0026hellip;）そこで，参考になる記事が1つでもあればと思い，このブログを書くことにしました．\n本ブログの構成としては，最初にマルチテナント環境での機械学習の話題が触れられている AWS の記事「Implementing a Multi-Tenant MLaaS Build Environment with Amazon SageMaker Pipelines」と Azure の記事「Architectural approaches for AI and ML in multitenant solutions」を参考にマルチテナント環境での ML の考慮事項について紹介します．後半では，僕が経験してきた SaaS 提供での ML サービスの話題なども例として紹介したいと思います．\nマルチテナント環境での ML 適用のための考慮事項 ML 適用を考える際には，モデルの学習と推論という2つの大きなフェーズがあります．これらのフェーズには，モデルとデータ，そしてそれらを支えるインフラシステムが密接に関連しています．\nマルチテナント（複数の顧客が同一のシステムを共有する）環境ではテナントの分離が重要な関心事になります．AWS のアーキテクチャガイドラインでは，これを Silo（完全分離）・Pool（共有）・Bridge（ハイブリッド）の3つのモデルとして紹介（Silo, Pool, and Bridge Models）しています．\nML の文脈では，データとモデルのセキュリティが特に重要です．具体的には，テナントが他のテナントのデータやモデルへ未承認または不正にアクセスできないようにする必要があります．例えば，投稿監視システムでは，テナント固有の監視ルールがある一方で，暴力的な投稿の検出など，すべてのテナントに共通する要件も存在します．このような共通要件に対しては，すべてのテナントのデータを統合して学習することで，より大規模なデータセットを活用でき，効果的なモデルを構築できる可能性があります．ただ，このようなデータ統合については，セキュリティ上問題ないか，契約的に大丈夫かといった制約を確認する必要があります．\nテナントの分離パターン ここで話をテナントの分離に戻すと，マルチテナント環境における ML モデル・パイプラインの実装方式として，データとモデルの観点から主に以下の2つのアプローチがあります．\nテナント専用のモデル\n各テナントに専用リソースが提供されるモデルで，テナント固有のデータのみで学習を行い，テナントごとに独立したモデルが作成される ML パイプラインの定義は複数のテナントで共有されることも多い 参考：AWS の MLaaS（Machine Learning as a Service）実装ガイドラインではテナント専用 ML モデルの具体的な構築例が載せられている [pic: テナント専用のモデルのパイプラインイメージは Azure の記事にある図が分かりやすいと思うので，そちらか拝借しています]\nテナント共有のモデル\n複数のテナントでリソースを共有しているモデルで，すべてのテナントのデータを用いて学習が行われ，単一の共有モデルが作成される テナント共有のモデルの例としては，事前学習済みの共通モデルを全テナントで利用するケースがある [pic: テナント共有のモデルのパイプラインイメージは Azure の記事にある図が分かりやすいと思うので，そちらか拝借しています]\nさらに，これらのハイブリッドアプローチとして，共有の事前学習済みモデルをベースにテナント固有のデータでファインチューニングを行う方式があります．この方式は，一般的な特徴を共有モデルで捉えつつ，テナント固有の要件に対応できる利点があります．\nスケーラビリティは要注意 スケーラビリティは特に大事な要素だと感じます．サービスが拡大していくとテナントの数が増えていき，それに伴いテナントが保有するデータ量（ユーザー数，ログデータ）も増えていきます．\nテナント専用のモデル\n必要なモデル数はテナント数に比例します．使用するデータについてはテナント毎でデータ量の差はあるものの，基本的には増加の一途を辿ることになるので，モデル学習時にどれだけのデータを使用するかの調整と，コンピューティングリソースの適切な割り当て（CPU, メモリ）が必要になります．これはコストにも関わってきます．また，テナント数に比例したモデル数になるので，マネージドサービスを使う場合は Quotas and limits が存在するため，システム設計をきちんと行う必要があります．\nテナント共有のモデル\nすべてのテナントデータを用いて学習を行うため，学習用のリソースがテナント数の増加と同じ速度でスケーリングされる可能性が低くなりますが，その分データ量が専用モデルと比較して多くなり，モデルの学習時間も長くなる傾向にあります．これは学習頻度にも影響してきます．\n次に推論についても考えてみると，テナント専用のモデルを用意する場合が特に厄介です．方針は大きく2つあると思います．\nテナント毎に独立したサービングコンテナを用意する（単一モデルエンドポイント） Pros: テナント毎のカスタマイズが容易で，リソースもテナント毎に適した配分をすることが可能になる Cons: テナント毎にサービングコンテナを用意する必要があるため，インフラコストが増加しがちで，デプロイメント作業やシステムの管理が複雑になる 複数のテナントモデルをホストできる共有サービングコンテナを用意する（マルチモデルエンドポイント） Pros: インフラコストは効率的になる．アクセス頻度の高いモデルと低いモデルが混在している場合は，トラフィックが効率的に処理可能になる Cons: テナント間の分離レベルが低下するため，障害が発生した場合に影響範囲が全体に波及する．また Noisy Neighbors（うるさい隣人）に注意が必要で，リソースをテナント間で共有している場合は，1つの大きなテナントによりリソースを過度に占有され，システム全体のパフォーマンスが低下する可能性がある マルチエンドポイントと単一モデルエンドポイント（Dedicated Endpoint）の違いがわかりやすいと思います．\n[pic: AWS の Multi-model endpoints というドキュメントから図を拝借しています]\nどちらを選択しても一長一短あるので，自分たちのユースケースに応じた選択になると思います．マネージドサービスを利用している場合は，オートスケールする設定にすることが多いと思いますが，気づいたら複数インスタンスが立ち上がってコストが爆発するケースもあるので，推論システムを用意する場合は慎重に進める必要があると感じています．\n推論システムの話は，「事例でわかる MLOps - 6.3.5 推論システム -デプロイと推論システム-」の章で解説されているので，参考になると思います！\nその他の要素 他には，パフォーマンスや実装の複雑さ，コストの話など考えることが多くあります．またマルチテナントでは公平性についても議論されていて，Fairness in multi-tenant systems という AWS のアーキテクチャガイドラインが記された記事もあります．\nSaaS プロダクトでの ML サービスを考えてみる 題材として推薦システムを取り上げ、以下の2つのユースケースについて検討してみます：\nコンテンツ間の関連性に基づくレコメンド ユーザーに応じたコンテンツのレコメンド（パーソナライズレコメンド） 1. 共有モデルを用いた関連コンテンツの推薦システム 関連コンテンツをどのような方法で実現するかは様々なアプローチがありますが，今回は一例としてコンテンツのタイトルや本文のテキスト情報の類似性を使った推薦を考えます．テキストの類似性を見るのであれば，学習済みの埋め込みモデルを用いて埋め込みベクトルを計算し，コサイン類似度で関連性の定量化を行うことができます．この場合は，テナント共有のモデルの章で紹介した共通の学習済みモデルから推論を行うパターンが考えられそうです．\n提供する形態は，バッチ推薦として1日1回コンテンツに紐づいた推薦リストを用意してテナント毎にレコメンドを行います．\n前処理や埋め込みベクトルの計算，レコメンド結果の保存といった一連の処理を毎日行う必要があるため，学習パイプラインを用意して処理をコンポーネント単位に分割して実行できると良さそうです．これをテナント毎にパイプラインを用意するか，それとも1つのパイプラインですべてのテナントのデータを処理するか悩ましいです．\n悩ましい点としては，エラーハンドリングやリトライ処理の設計があります．エラーが発生した場合にどの時点から処理を再実行すべきなのか，すべての処理が完了しないとすべてのテナントに展開できないのかなど考えておくべきことがあります．\nまた，データ量にも注意が必要になります．テナントによってコンテンツ量が違うため，あるテナントでは数千件，別のテナントでは数万件など考えられます．1つのパイプラインで全てを処理しようとしたら，推論時間も気にしないといけないです．\n[pic: すべてのテナントのデータを一括で処理する vs. テナント毎にデータを処理する]\n1つのパイプラインですべてのテナントのデータを処理 特定のテナントの処理でエラーが発生した際に，後続のテナントの処理に影響が出る リトライ処理をどの位置から実行するか，それを容易に可能とする仕組みが必要になる データ量がテナント毎で異なるため，マシンスペックなど適切なリソース設定が難しくテナント毎のコスト計算がしづらい テナント毎のパイプラインでデータを処理 スケールする仕組みを用意しないと容易に破綻する．例えば，パイプラインを起動するスケジューラーをテナント毎に用意して起動させると考えた場合，テナント数だけ管理が必要で現実的ではない．またマネージドサービスを利用する場合は Quotas and limits が存在する このため，モデルの定義やパイプラインの構成などは同一でもパイプライン自体をテナント毎に用意する選択肢が考えられます．これはテナントのデータ量に応じて適切なリソースを割り当てることができたり，エラーによる影響を最小限に留めるといったメリットがあります．\n[pic: テナント毎のパイプラインでデータ処理を行うスケールするアーキテクチャ図が，AWS の記事で紹介されているので拝借しています]\nこのアーキテクチャでは，同時実行数を制御するために，Amazon SQS を活用して，Lambda によって実行状況を確認しながら，Amazon SageMaker Pipelines を起動する役割を担っています．\nAWS ではなく，Google Cloud の Vertex AI Pipelines を活用する場合は，同時実行数の上限は300ですが，上限を超えた場合は自動的にキューに入れてくれる設計になっています（参考：Vertex AI quotas and limits - Vertex AI Pipelines）．そのため，Cloud Scheduler と Cloud Functions を用いて起動するタイミングを制御すれば良さそうです．\n共有モデルを用いた場合は，モデルに関しては1つに定まるためそこに関しては考える要素が減ってきます．では，テナント専用のモデルを用いる場合はどうでしょうか？\n2. 専用モデルを用いたパーソナライズ推薦システム 専用モデルといった場合，データは1つのテナントのみを使用し，モデルアーキテクチャはすべてのテナント共通であるケースと，モデルアーキテクチャもテナント毎で異なるケースが考えられます．\n機械学習を用いた推薦アルゴリズム特にディープラーニングを活用した場合は，精度にデータ量の問題が関わってきます．データ量が少ないと期待した精度が出ないため，テナントの属性データをクラスタリングしクラスター（大小の規模）に応じてより適切なアルゴリズムを選択したくなります．\nテナント毎のデータ量 多い場合：ディープラーニングによる複雑ではあるが効果的な手法 少ない場合：協調フィルタリングなどのオーソドックスな手法 このようなケースでは，設定ファイルやパラメータを渡せるようにしたり，テナント毎にメタデータを付与して動的にアルゴリズムを切り替えて管理する仕組みが必要とされるかもしれません．専用モデルを用意するということは共通モデルと比べてモデルに関する要素が加わるため，より一層複雑性が増します．\n専用モデルをオンライン推論したい場合は，「スケーラビリティは要注意」の章でも触れましたが，とにかく難易度が上がるので，可能ならなるべく避けたいところです．\nおわりに マルチテナント環境での ML 適用を考えた場合の懸案事項について，主にデータとモデルの観点から考えてみました．テナント専用モデルと共有モデルそれぞれのアプローチには一長一短があり，サービスの要件や運用条件に応じて適切な選択が必要だと思います．\nスケーラビリティだけなく，今回はあまり触れていないですが，セキュリティ・コストも重要な考慮すべき点として挙げられます．また推薦システムの例のように，実際のシステム設計では，要件に合わせたパイプラインの構成やエラーハンドリング・リソース管理など，様々な技術的な課題に直面しますが，クラウドサービスを組み合わせた適切なアーキテクチャ設計を行うことで対応できると思っています．なので，もっと色々なユースケースが世の中に紹介されると嬉しいと個人的に感じます（toB 系のサービスの話は中々世に出せないケースもあると思いますが）．\nマルチテナント環境は，1つの toC サービスで開発する場合とは考えることが変わってくるので，また違う難しさがあります．スケールアウト（マルチテナント）とスケールアップ（シングルサービス）のような感覚が個人的にあります😅\nまだまだ綺麗に整理しきれていないと思えますが，また定期的にユースケースを整理したいと思います👋\n参考 Implementing a Multi-Tenant MLaaS Build Environment with Amazon SageMaker Pipelines Implementing SaaS Tenant Isolation Using Amazon SageMaker Endpoints and IAM Architectural approaches for AI and ML in multitenant solutions 事例でわかる MLOps - 6章 顧客ごとに複数機械学習モデルを出し分ける学習と推論のアーキテクチャ ","date":"2024-12-24","permalink":"https://masatakashiwagi.github.io/portfolio/post/multi-tenant-ml-pipeline/","tags":["ML","MLOps"],"title":"マルチテナント環境における機械学習適用について考える"},{"content":"はじめに 2024年1月24日に発売される「Azure OpenAI Service ではじめる ChatGPT/LLM システム構築入門」を著者の一人である立脇さんから献本頂いたので，書評を書いていきます！\n改めて，出版おめでとうございます🎉\nAzure OpenAI Serviceではじめる ChatGPT/LLMシステム構築入門（AOAIドーナツ本）を著者の立脇さんから献本頂きました🎉\nAzure上でのChatGPTを用いたシステム構築の話はもちろんのこと、ガバナンスやResponsible AIにも言及していて興味深く読ませて頂いてます！後ほど書評ブログも書かせて頂きます。 pic.twitter.com/oRqcYAV0gr\n\u0026mdash; asteriam (@asteriam_fp) January 20, 2024 書籍の目次 目次は以下のようになっていて，全体は4部構成（+付録）になっています．\n第1部: Microsoft Azure での ChatGPT 活用 第1章: 生成 AI と ChatGPT 第2章: プロンプトエンジニアリング 第3章: Azure OpenAI Service 第2部: RAG による社内文章検索の実装 第4章: RAG の概要と設計 第5章: RAG の実装と評価 第3部: Copilot stack による LLM アプリケーションの実装 第6章: AI オーケストレーション 第7章: 基盤モデルと AI インフラストラクチャ 第8章: Copilot フロントエンド 第4部: ガバナンスと責任ある AI 第9章: ガバナンス 第10章: 責任ある AI 付録 付録 A: サンプルコード実行環境の準備 付録 B: ChatGPT の仕組み（詳細編） 内容の紹介と感想 各章の概要について簡単に触れつつ，個人的に興味深かった点や印象に残った点を紹介したいと思います．\n本書は初めに読者の目的とレベル感に合わせてどの章から読めば良いかが示されていて，全体の見通しがクリアになるように設計されています．そのため，目的に合わせて必要な章を効率良く読めるでしょう．\nでは，各章の概要についてかいつまんで紹介したいと思います！\n第1部: Microsoft Azure での ChatGPT 活用 第1部は，最初に生成 AI と ChatGPT 基礎的な概念と仕組みを紹介しています．ChatGPT がどういったタスクをこなすことができるかについてもまとめられていて，ユースケースの例もあるので，活用のイメージを膨らませることができます．\nまた，生成 AI の出力に関係してくるプロンプトの基本的な書き方の紹介，気をつけるべき観点が整理されていて，プロンプトエンジニアリングを学ぶことができます．\n第1部の最後は，Azure OpenAI Service の使い方が書かれています．Open AI が開発した AI モデルをマネージドサービスとして使用することができ，Azure OpenAI の始め方が図を使って丁寧に解説されています．\n第2部: RAG による社内文章検索の実装 第2部は，ChatGPT を使った社内文章検索システムを題材にして，RAG の説明から実際に Azure 上でどういったサービスを組み合わせてシステムを構築するかについても丁寧に説明されています．後半には，RAG の評価の仕方も記載されています．\nまた，Azure Machine Learning プロンプトフローを使って効率良くプロンプトの検証・開発をする方法も紹介されています．\n第3部: Copilot stack による LLM アプリケーションの実装 第3部は，ChatGPT などの LLM を組み込んだアプリである Copilot を開発するための話が，Copilot フロントエンド・AI オーケストレーション・基盤モデルの3段階で紹介されています．\nCopilot フロントエンドの説明では，UX 向上のためにストリーミング処理の実装や参考資料などが紹介されていて，ユーザビリティの重要性を教えてくれます．\n第4部: ガバナンスと責任ある AI 第4部は，生成 AI のサービスや機能を開発・運用していく上でのガバナンスの話や責任ある AI に対する Microsoft の取り組みなどが紹介されています．ガバナンスの話では，認証周りや課金，アクセス制限など組織で活用していく上で考慮すべき問題を取り上げています．\n興味深かった点/印象に残った点 ここからは個人的な感想を書いていこうと思います．\nまず全体的に注釈で細く補足がなされていたり，参考文献が挙げられていてとても親切です．タイトルにも含まれるように Azure を使った内容にはなっていますが，プロンプトエンジニアリングや RAG の設計・評価，基盤モデルの話など ChatGPT/LLM をサービスに活用する上で知っておくべき一般的な内容についてもしっかりと説明がされていて，まだまだキャッチアップ不足の僕にはとても参考になりました！\n後半ではガバナンスや責任ある AI (Responsible AI) を取り上げていますが，これらについて言及している書籍はまだまだ少ない状況なので，とても有益です．サービスとして機械学習を提供するのであれば，こういった内容についても理解を深め取り組んでいくべき（自戒を込めて）で，それを助ける参考文献が色々と記載されています．\n特に個人的な推しポイントは3つあります．\n1. RAG を使った検索システムの設計と評価 Retrieval-Augumented Generation (RAG) を使うメリットやアーキテクチャを最初に説明することで，これからどういったものを考えていくのかがわかりやすかったです．コンテキストが示されて体系的に説明がされるので，テックブログのように断片的なものよりは理解が進みやすいです．\n特に検索システムの説明が厚くてとても良かったです！Azure AI Search を題材にしていますが，インデックス作成・ドキュメント検索の流れを図を用いて丁寧に解説されています．インデックス作成の流れは別の検索システムを使うにしても参考となります．特に RAG を使った検索では，ドキュメントの文字数と文の意味に基づいた検索が重要であるという内容がしっかり書かれていて理解が深まりました．\nまた，Azure AI Search では，フルテキスト検索とベクトル検索のハイブリッド検索を利用することができ，ハイブリッド検索とセマンティックランカーを行うことでより高精度な検索が実現可能とのことです（下図）．こういった柔軟な検索手法の組み合わせが，比較的簡単にマネージドサービスとして実装できるのは楽だなと感じます．\n参考: Azure AI Search: Outperforming vector search with hybrid retrieval and ranking capabilities\nRAG の評価についても書かれていて，RAG の評価では，検索精度の問題と生成精度の問題の２つが挙げられます．特に生成精度の評価はまだまだスタンダードな方法がなく難しいですが，その一例として，OSS で RAG 用の評価フレームワークである Ragas があるよと挙げられてます．単一のメトリクスで判断するのではなく，複数の組み合わせがおすすめされてて，関連性・一貫性・類似性の評価について解説もされています．評価方法どうすればいいかあまり理解していなかった自分にとっては良い示唆を貰えました！\n2. オーケストレータ作成のための Azure Machine Learning プロンプトフロー Azure Machine Learning プロンプトフローはローコードでオーケストレータを作成できるサービスで GUI/CLI で利用できるとのことです．\n実行フローの可視化 チームでの共有やデバッグ 複数のプロンプトを試せるプロンプトバリアント機能 etc\u0026hellip; 特に興味を持ったのは，プロンプトバリアント機能でプロンプトは色々と試行錯誤して良さげなものを探すと思いますが，これらを一度に複数パターンを定義できて実行できるとのことで，実験速度がめっちゃくちゃ上がりそうだなと感じました！改善しやすい仕組みが自分で整備しなくても整えられているので，やりたいことに集中でき，開発者体験としてとても良さそうに感じられました！\n3. 責任ある AI (Responsible AI) の取り組み Microsoft では，責任ある AI (Responsible AI) として，以下の6つの原則を掲げています．\n公平性 AI システムはすべての人を公平に扱う必要があります 信頼性と安全性 AI システムは信頼でき安全に実行する必要があります プライバシーとセキュリティ AI システムは安全であり，プライバシーを尊重する必要があります 包括性 AI システムはあらゆる人に力を与え，人々を結びつける必要があります 透明性 AI システムは理解しやすい必要があります アカウンタビリティ AI システムにはアカウンタビリティが必要です 参考: 責任ある信頼された AI - Cloud Adoption Framework\n詳細は上記参考ドキュメントを読むとわかりやすいです．これらを定義し，それをどのように実践するかといったことについてもきちんと公式ドキュメントとしてまとめられていて素晴らしいです．信頼性と安全性は MLOps とも関連性が高い項目ですね．\n本書では，取り組みの紹介として，コンテンツフィルタリングの例があります．生成 AI が不正利用されるとサービスに大きな影響が及びます．これに対して，Azure OpenAI では入出力にフィルタリング機能が実装されていて，適切でない内容対しては処理がストップされるようです．こういったことをマネージドサービスとして提供してくれるのかと感心したと同時に，サービスを使う側にとって安心で心強いと感じました．仮にこういったことを自前で実装するとなると，かなりしんどくて（どういった項目を扱うか，実装や運用はどうするかなど），素早くサービスを展開しづらくなります．\n気になった点 気になった点は特にないですが，強いて言うなら，LLM 開発する上で有用な Microsoft 製の OSS がいくつか紹介されていたので，それについて付録などでもう少し使い方や組み合わせ方があっても面白いかなと思いました．\nおわりに 今回献本頂いて書籍を読むまで Azure, Azure OpenAI Service のことはほぼ知りませんでした🙇‍♂️\nただ本書を読んでいくことで，Azure OpenAI Service の使い勝手の良さや導入のしやすさなどはとても感じることができました！興味深かった点/印象に残った点でも書きましたが，Azure Machine Learning プロンプトフローを使った開発は非常に開発者体験が良さそうに感じるため，まだ時間が取れず，実際には触れていないのですが，これを機に是非触ってみたいと感じさせられるものでした💪\n他にも Microsoft の OSS が所々で紹介されていて知らないものも多かったので，参考になりました！\nDeepSpeed Deep Learning モデルの学習・推論を高速化するフレームワーク Semantic Kernel LLM を自分のアプリに簡単に統合できる SDK Azure を既に使ってる人はもちろんのこと，使ったことが無い人も本書を通して Azure OpenAI Service を使った ChatGPT/LLM のシステム開発に挑戦してみると楽しいと思います！\nもしこれらに興味がある人は是非手に取って見て下さい！！\n","date":"2024-01-21","permalink":"https://masatakashiwagi.github.io/portfolio/post/review-azure-openai-service-book/","tags":["BOOK"],"title":"「Azure OpenAI Service ではじめる ChatGPT/LLM システム構築入門」を読んでの書評"},{"content":"はじめに もう今年もあと数週間ということで，1年があっという間に終わってしまいますね😅\nこの記事は MLOps Advent Calendar 2023 の15日目の記事になります！アドベントカレンダーの日付を選ぶ際についつい自分の好きな数字を選びがちですが，皆さんはどうですか？笑\n最近，機械学習（ML）パイプラインの良い構成やパイプラインとコンポーネントの良い組み方に興味があり，それをどう管理するかのディレクトリ構成などを考えたりしているのですが，きっかけとしては参考にも載せてある「From MLOps to ML Systems with Feature/Training/Inference Pipelines」という Hopsworks の CEO である Jim Dowling が書いた記事を以前読んでとても良いなと思ったのがきっかけです．\nまた，AB テストなどのオンライン検証を見据えた時に，どういったディレクトリ構成だとスムーズに開発ができたり，実施までの工程を減らして素早くデプロイし，検証できるかも考えることが多々あり，良い方法を模索しているのもあります．\nこの記事では，その辺りの自分が今考えている内容について紹介しようと思います！技術検証系の記事ではなく，考え方の一例を紹介する形になります．\n僕の中の現時点の結論は，\nコンポーネント単位ではなく，パイプライン単位で管理し，パイプラインは程よくモジュール化する です．\nパイプライン: 処理単位で分けられたコンポーネントで構成 ML パイプラインを組み立てる時，大抵の場合は処理単位で分けられたコンポーネントを用意し，それを有向グラフで繋げてパイプラインを構成することが多いかと思います．下図は学習パイプラインや推論パイプラインの例を書いています．パイプラインの中には，ストレージやデータベースからデータを抽出する処理だったり，推論や予測結果をデータベースに保存する処理が加わるかもしれません．\n処理をコンポーネントもしくはモジュールというかもしれませんが，それ毎に分けて I/O を定義し，管理することはとても良いと思います．疎結合になっていることで，処理の入れ替えなどが容易でメンテナンスもしやすくなります．\n一方で，こんな疑問や悩みを持ったりしています．\nどこまで細かくコンポーネントに分けて使うのが良いか？ どの程度，共通化や汎用化を考えるのが良いか 過度なコンポーネント化は逆に管理しづらくなる？ チーム開発を考えた場合にコンポーネント単位での分割は効率的なのか？1 バッチ/リアルタイムの両方を満たすことを考えるとどうだろうか？2 （共通化や汎用化を行い再利用することを意識して設計開発することはとても大事だと思います．一方で，最初からそれを意識しすぎると中途半端なものができたりして，逆に使いづらいことが過去の経験としてあるので，何度か使われる内に共通化を進めて行く方がユースケースに合ったものが出来て良いかなと最近は思っています．）\nコンポーネント単位での接続では，インターフェイスがコンポーネント間でのやり取りになり，先行するコンポーネントの出力結果に応じて後続のコンポーネントに微調整が入ることもあります．例えば，AB テストをするのに，ちょこっとだけ先行のコンポーネントを変えたことで，後続のコンポーネントにも修正が入り，それが元で既存のパイプラインに影響が出たりして厄介だなと感じたことがあります．（これは僕の実装力不足な点もあると思いますが\u0026hellip;😅）\nFeature/Training/Inference Pipelines とは？ 冒頭紹介したブログでは，Feature/Training/Inference (FTI) Pipelines というものを紹介しています．これは独立した3つのパイプラインから構成されていて，それぞれが独立して開発や運用ができる点が良く，役割もパイプライン毎に明確になっています．\n記事では，バッチ/リアルタイムの ML システムの例を紹介した後に，その両方に対応した Unified Architecture for ML Systems として紹介されています．\nそれぞれのパイプライン間のインターフェースがこの例だと，Feature Store/Model Registry になっています．これは彼らの製品である Feature Store を上手く活用する仕組みになっていてなかなか考えられているなと感じました！😄\nそれぞれのパイプラインの役割はどうなっているか 役割は以下のように説明されています．\na feature pipeline that takes as input raw data that it transforms into features (and labels) a training pipeline that takes as input features (and labels) and outputs a trained model an inference pipeline that takes new feature data and a trained model and makes predictions. feature pipeline: 生データを入力として受け取り，特徴量（とラベル）に変換する training pipeline: （feature pipeline で生成された）特徴量（とラベル）を入力として受け取り，学習済みモデルを出力する inference pipeline: （feature pipeline で生成された）新しい特徴量と学習済みモデルを受け取り，予測を返す 例えば，レコメンドシステムのバッチ推論で良くあるパターンだと，下図3のように feature pipeline, training pipeline, inference pipeline を3つ繋げて（上側の点線）最終的に結果をストレージやデータベースに保存しておくという流れになります．\nまた，リアルタイムレコメンドのパターンだと，feature pipeline, inference pipeline の2つが繋がり（下側の点線）結果をアプリケーションに返す流れになります．\nパイプラインは適度にコンポーネント化されていて，それぞれのステージで明確なインターフェースがあります．個々のパイプライン内でさらにコンポーネント化されていても良いですが，あくまでパイプライン間での関係を見るので，内部は柔軟になっていて良いと思います．\n特に ML パイプラインはスクラップ \u0026amp; ビルドされることも多々あるので，そこまで内部をコンポーネントに分けてもそこまでの恩恵が無いと感じています．むしろ，チームで1つのパイプラインを開発するケースでは，これぐらいの粒度の方が開発効率が良いのではないかなと思います．\nありがちなモノリシックなバッチパイプライン（下図4）よりは遥かに使いやすく，リアルタイムへの移行もスムーズになり，開発スピードは上がるはずです．\nパイプライン間のインターフェース Feature/Training/Inference (FTI) Pipelines の考え方の場合，各パイプライン間のインターフェースはデータに関しては，Feature Store や Object Storage (S3, GCS \u0026hellip;etc) などになるかと思います．\n僕はマネージドサービスでパイプラインを組むことが多いのですが，AWS と GCP だと以下のような感じでできるのではないでしょうか．\nAWS: パイプライン: SageMaker + Step Functions (or SageMaker Pipelines) 構造: Step Functions では，ステートマシン同士の接続が可能なので，パイプライン同士の接続は容易にできる パイプライン間のインターフェース: S3，Amazon SageMaker Feature Store GCP: パイプライン: Vertex AI Pipelines 構造: コンポーネント同士はオブジェクトで繋がる仕組み．Step Functions のようにパイプライン同士の接続ができるかは把握できていない パイプライン間のインターフェース: GCS，Vertex AI Feature Store FTI Pipelines で疑問や悩みは解消するのか 最初に挙げていたいくつかの疑問や悩みについて，\nどこまで細かくコンポーネントに分けて使うのが良いか？ → より少ない単位（パイプライン単位）で管理可能 チーム開発を考えた場合にコンポーネント単位での分割は効率的なのか？ → パイプライン単位で分割することである程度は効率的に開発できそう バッチ/リアルタイムの両方を満たすことを考えるとどうだろうか？ → バッチからリアルタイムへの移行などもパイプライン単位ならしやすい バッチ/リアルタイムのアーキテクチャどうするか問題は，Lambda/Kappa Architecture の話などもあるので，またどこかでブログにまとめたいです また，AB テストを実施する場合は，パイプラインを AB テスト用にもう1セット準備することで比較的容易に対応できるのではないでしょうか．\n最後にディレクトリ構成をどんな感じにするのが良いか紹介しようと思いましたが，まだ整理できていないのですいません🙏\nLayerX さんのこちらの記事5で紹介されているモノレポの構成は，僕のところもモノレポで管理しているというのもありとても参考になります．\nあとは，パイプラインでまとめている今回の構成において，エラーが発生した場合のリカバリー方法やエラー箇所の特定をどうするかについてはまた別途考えていきたいです．\nおわりに 今回紹介した内容がどのような場合でも上手く機能するかは正直わからないですが，こういった選択肢もあるという形で見て頂ければと思います．今の僕の所属するチーム規模や体制などではワークするかもしれないですが，もっと大きな規模になると別かもしれません．\n僕も1年後には別の方法が良いかもなと感じているかもしれないので，またその時に良い方法を模索して考えたいと思います😄\nもし他にもこういった考え方や構成が良いよなどありましたら，是非お話を聞きたいので，よろしくお願いします🙏\n参考 From MLOps to ML Systems with Feature/Training/Inference Pipelines チームで分担した場合には分担の仕方にもよるが，例えば，前後のコンポーネントの用意が遅れたりして，なかなかパイプラインを動かせないなどもありうる\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nバッチの場合，一度に大量のデータを処理することを想定するが，リアルタイムの場合，ストリーミングデータを想定するので，入口の扱いが異なるが処理は一貫したものにすべき\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n(From MLOps to ML Systems with Feature/Training/Inference Pipelines) Figure 11: The Feature/Training/Inference (FTI) pipelines Mental Map for building ML Systems\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n(From MLOps to ML Systems with Feature/Training/Inference Pipelines) Figure 4: A monolithic batch ML system that can run in either (1) training mode or (2) inference mode.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://tech.layerx.co.jp/entry/2023/11/16/185944#%E5%AE%9F%E9%9A%9B%E3%81%AE%E9%81%8B%E7%94%A8%E6%96%B9%E9%87%9D\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-12-15","permalink":"https://masatakashiwagi.github.io/portfolio/post/how-to-recreate-ml-pipeline/","tags":["ML","MLOps"],"title":"機械学習パイプラインの作り方を改めて考えてみる"},{"content":"はじめに Netflix は RecSys のトップランナーの1社であり，そこで行われているオペレーションは非常に興味深く気になっていたので，2022年のテックブログで紹介された RecSysOps に関する取り組みからノウハウを学ぼうと思い，記事の内容を取り上げながら，所感を書いていこうと思います．\nRecSysOps: Best Practices for Operating a Large-Scale Recommender System Large-Scale と書かれていますが，Netflix のような規模でなくても試せることは多いと思います．\nRecSysOps とは？それに必要な要素とは？ RecSysOps (= Recommender Systems Operations) という言葉はこのブログが初かなと思います．もちろんこういった言葉で表現されていないけど，各社推薦システムの運用を通して何かしらのプラクティスを持っているとは思います．\nRecSysOps が何を指すかは，動画で以下のように述べられています．\nRecSysOps: Lessons we learned while operating a large RecSys\n多くのリクエストに応えたり，安定的に稼働させ続ける（可用性）推薦システムを運用する上で得られたプラクティスを RecSysOps として紹介してくれています．\nこの取り組みを行うことで，以下の点で役立ったと書かれています．\nReduce our firefighting time Focus on innovations Build trust with our stakeholders RecSysOps には4つの主要な取り組みがあり，\nIssue Detection Issue Prediction Issue Diagnosis Issue Resolution これらを通して，Netflix のような大規模な推薦システムを健全に運用できているとのことです．\nこういった取り組みは自分達が本来集中したいこと，また検証したいことだったりクリエイティブな活動に時間を割けるようにする上でも大事な取り組みだなと感じる．機械学習システムは本番環境に導入されてからが 本番 なので，最初は上手くいってても，時間が経過するにつれて綻びというか予期せぬこと・開発時には想像していなかった事象は平気で起きるので，これらを検知したり予測して，適切に対処していく活動は尊いなと感じます．\nモニタリングして，検知できる状態にするまではよく行われるが，じゃあ検知した後にそれらが分析できる状態になっているか，Runbook のような形でオペレーション手順にまで落とし込めているか，これらができている会社はまだまだ多くない印象です．\nでは，それぞれ4つのトピックを見て行きます．\nIssue Detection RecSysOps における最初のフェーズで，「Issue Detection = 問題の検出」が最も重要な要素だと語られています．理由としては，これが後続ステップのトリガーとなるため，これが行われないと後続ステップは意味がないからです．\nこの取り組みとしては，3ステップ紹介されています．\nステップ1: 関連する分野から既知の全てのベストプラクティスを取り入れる 最初のステップとしては，以下のように紹介されています．\nThe very first step is to incorporate all the known best practices from related disciplines.\n「関連する分野から既知の全てのベストプラクティスを取り入れる」ということで，推薦システムの場合だと，ソフトウェアエンジニアリングと機械学習が含まれるため，DevOps と MLOps の両方のプラクティスを取り入れることが示されています．ここでも ML Test Score をチェックリストとして活用できるよと紹介されています．\n本番環境で発生する問題は無数にある \u0026amp; 未知なる問題もあるため全てに対応することは難しいと感じますが，問題に気づくための取り組み・検出範囲を拡げるための教訓は先人の知恵から学ぶことができると思うので，こういったものをリサーチするのも必要だと感じます．\nステップ2: システムを自分たちの視点からエンドツーエンドで監視する 2番目のステップとしては，以下のように紹介されています．\nThe second step is to monitor the system end-to-end from your perspective.\n「システムを自分たちの視点からエンドツーエンドで監視する」ということで，大規模な推薦システムの場合，多くのチームが開発や運用に関わってきます．ML チームの視点から見ると，\nアップストリームチーム データを提供するチーム ダウンストリームチーム モデルを使用するチーム があります．データチームや自分たちのモデルを使いたい別チームが社内に居るかもしれません．それぞれのチームが独自にモニタリングなどの監視をしているかもしれないですが，そのチームの取り組みだけに頼らずに自分たちの視点からも必要な情報（入力と出力など）をモニタリングするのが良いと紹介されています．\nこの教訓は，ML Test Score の「Monitor 1: Dependency changes result in notification」でも似たようなことが述べられています．例えば，データチームや開発チームと連携して，スキーマ変更や保存されるデータが変わる場合は連携するようにオペレーションを整えるべきだったりが挙げられます．\nデータチームはデータ全体を見ているので，ML だけが使う一部のデータをどこまで感知しているかはわからないので，ML で使用するデータに関しては ML 側できちんと把握して必要なことは自分たちで率先して動くことが大事だと感じます．\n他チームに任せるのではなく，オーナーシップを持って自分たちの範疇で取り組めることに関しては積極的に行っていく姿勢は素晴らしいですね．\nステップ3: ステークホルダーの懸念を理解する 3番目のステップとしては，以下のように紹介されています．\nThe third step for getting a comprehensive coverage is to understand your stakeholders’ concerns.\n「ステークホルダーの懸念を理解する」ということで，推薦システムの文脈では，主要な視点として，メンバー（ユーザー）とアイテムがあります．この取り組みは問題の検出要素のカバー範囲を拡げるのに良いと書かれています．\nメンバー（ユーザー）視点 提供している推薦モデルによって高く評価されていないアイテムを選択している場合，何かしらの潜在的な問題があるかもしれません．また，これは将来の優れたインスピレーションの源になるとも書かれています．\nアイテム視点 Netflix の場合，アイテムに責任を持つチームはアイテムのコールドスタートと潜在的な本番バイアスに関する懸念を持っています．彼らの懸念に関してサポートするために，メトリクスの定義・モニタリングツールの提供だけでなく，問題がアイテムごとに発生しているかどうかについてのインサイトの提供などもしています．これらを通して，アイテムに関連する主要な問題に積極的に対処し，ステークホルダーとの信頼関係を構築していると書かれています．\nNetflix のようなアイテムに責任を持つチームやカスタマーサクセス・サポートチームのようにユーザーのことを常に考えているステークホルダーの懸念や悩みをヒアリングしたり，サポートすることは自分たちが気づいていない新しい視点を提供してくれるので，とても参考になると日々の業務でも感じます．こういったチームからの要望などを元にモニタリングすべき項目を追加したり参考にしたりしているので，ホットラインを築いて協力していくことで問題に気づくことはありそうです．\nIssue Prediction 2つ目のトピックは，「Issue Prediction = 問題の予測」です．問題が本番環境で起こる前に予測して修正することです．\nNetflix では，アイテムのコールドスタート問題が重要なテーマです．この問題を過去のデータを使用して，ローンチ当日の本番モデルの行動統計を予測できるモデルを構築して予測しているとのことで，これによりアイテムのコールドスタートに関連する潜在的な問題を一週間以上前に検出し，問題を修正する時間を確保できているとのことです．\nこの取り組みは正直かなり難易度が高いと感じました．アイテムに関する事前の情報やメタデータを駆使しながら，類似アイテムとの関連性を見つけて予測するのが良いのですかね？正直これに関するアイデアは現時点だとあまりないです😅\nIssue Diagnosis 3つ目のトピックは，「Issue Diagnosis = 問題の原因特定」です．3つステップに分けて取り組みを紹介しています．\n問題を分離して再現する まず最初のステップとしては，以下のように紹介されています．\nthe next phase is to find its root cause.\n「問題を分離して再現すること」ということで，問題を再現するには事前に適切なログ設定を行う必要があると書かれています．これは何かしら問題がある場合に，単純にコードを再実行しても問題が再現できない状況があるということです．そのため，問題を理解するための再現に必要なログを出力しておくのが大事です．ただし，コストを削減するためにトラフィックの一部に対してログの記録を行っているとのことです．この場合には，適切なスライスでカバレッジを満たせるサンプリング方法を設計する必要があるとのことです．\nNetflix レベルになると，全部をロギングしておくとコストが尋常じゃないぐらい増えるので，こういった観点も必要になるんだなと感じました．ロギングは完全に同意で，問題の再現だけでなくモデル改善・分析観点でも必須だと思います．\n問題がMLモデルの入力またはモデル自体と関連しているかどうかを理解する 2つ目のステップとしては，以下のように紹介されています．\nThe next step after reproducing the issue is to figure out if the issue is related to inputs of the ML model or the model itself.\n「問題がMLモデルの入力またはモデル自体と関連しているかどうかを理解すること」ということで，入力データに関連しているかどうかは，入力が正確で有効であることを検証する必要があるので，非常に難しい問題ですとのこと．\nこれ難しいですね．データ全体でスケーリングしたりする処理を入れているとそのモデルも持っておかないとですし，ある特徴量を生成するのに別の特徴量を使っている場合は，その追跡やリネージュ関係を理解する必要があるので，より複雑な問題になりそうです．そういったこともあり，データのバリデーションは大事な取り組みの1つになってきますね．\nMLモデルとその学習データの内部を詳しく調べて問題の根本原因を見つける 3つ目のステップとしては，以下のように紹介されています．\nthe next step is to dig deep inside the ML model and its training data to find the root cause of the issue.\n「MLモデルとその学習データの内部を詳しく調べて問題の根本原因を見つける」ということで，ここでは例えば，モデルを解釈するために SHAP / LIME などの可視化ツールを使ったり，決定気のノードやニューラルネットワークのレイヤーを可視化する方法などを述べています．\nIssue Resolution 最後のトピックは，「Issue Resolution = 問題の解決」です．解決には，短期的で緊急度が高いものから，長期的に取り組む必要があるものが考えられます．ML モデルは高度に最適化されているので，気軽に手動で変更することはできないし，しても適切な推薦を提供できない可能性が生じるということです．\nここでは，事前に緊急に解決すべきかどうかの選択肢やトレードオフを用意しておくというのが紹介されています．\n検出または予測コンポーネントのチェックが定期的に自動実行されているか 人間の判断が必要な場合に，その人が必要な情報をすぐに手に入れるようになっているか Hotfix が必要な時に，デプロイを数クリックで簡単に適用することができるか 後半2つは非常に刺さります．例えば，データを集めるだけでなくすぐに使える状態に整理されているか，対応時のレポートラインやステークホルダーとの調整が事前にされているか，優先度の応じて対応できるようにオペレーションが確立されているかなど色々とありそうで，こういったことを事前に準備しておけると，いざその問題に直面した時に自分たちが楽になるし，変に焦る必要もなくなるので，動きやすいし信頼感もあるだろうな感じます．\nおわりに RecSysOps という推薦システムに焦点を当てていますが，これは別に推薦システム特有のものではなく，他の機械学習システムにおける MLOps として，適用できる話だと感じました．泥臭いオペレーションや関連するチームとの信頼関係を通して，日々の運用やサービスが安定稼働しているのを感じます．Issue Detection の3つのステップに全てが詰まってる気がしますね．\n機械学習の場合は，データ・コード・モデルの三大要素があるので，問題を検知していく上でこれらの「Versioning・Reproducibility・Monitoring」への取り組みはより必要だと改めて感じました．\n参考 RecSysOps: Best Practices for Operating a Large-Scale Recommender System SlideShare YouTube ","date":"2023-10-02","permalink":"https://masatakashiwagi.github.io/portfolio/post/learning-recsysops-from-netflix-practices/","tags":["MLOPS","RECSYS"],"title":"Netflix の事例から RecSysOps を学ぶ"},{"content":"はじめに Vertex AI Pipelines のサービスアカウント周りが分かりづらかったのと，GCP の他のサービスへの権限付与（IAM ロールの付与）の方法について備忘録として残しておきます．\nVertex AI Pipelines に関連する3つのサービスアカウント Vertex AI Pipelines に関連するサービスアカウントは以下の3つあります．\nパイプライン実行時に指定できるサービスアカウント パイプラインの各コンポーネントが各種リソースにアクセスするために GCP 側が用意したサービスアカウント（サービスエージェント） Vertex AI Service Agent Vertex AI Custom Code Service Agent パイプライン実行時に指定できるサービスアカウント Vertex AI Pipelines の job.submit に指定できるサービスアカウントでパイプライン実行をするために使用されます．これを指定しない場合は，Compute Engine のデフォルトのサービスアカウントを使用してパイプラインが実行されます．\n# sample code SERVICE_ACCOUNT = os.environ[\u0026quot;SERVICE_ACCOUNT\u0026quot;] PROJECT_ID = os.environ[\u0026quot;PROJECT_ID\u0026quot;] job = aiplatform.PipelineJob( display_name=\u0026quot;sample-pipelines\u0026quot;, template_path=\u0026quot;sample-pipelines.json\u0026quot;, project=PROJECT_ID, location=\u0026quot;asia-northeast1\u0026quot;, enable_caching=False ) job.submit(service_account=SERVICE_ACCOUNT) 僕はこのサービスアカウントを開発時に勘違いしていて，ここのサービスアカウントに指定したもので，パイプラインの全てのコンポーネントが動くと勘違いしていました．なので，ML 用にカスタマイズした（必要な IAM ロールを付与）サービスアカウントを指定して実行したものの，権限エラーで動かずという感じで困っていました．\nConfigure a service account with granular permissions 次に説明する2つのサービスアカウントが実際の各コンポーネントを動かすサービスアカウントになります．\nGCP 側が用意したサービスアカウント（サービスエージェント） Vertex AI に関する GCP 側が用意したサービスアカウント（サービスエージェント）は2つあります．\nVertex AI Custom Code Service Agent Vertex AI Service Agent IAM ロールの画面上では，以下のような登録になっています．\nPrincipal Name Role service-PROJECT_NUMBER@gcp-sa-aiplatform-cc.iam.gserviceaccount.com AI Platform Custom Code Service Agent Vertex AI Custom Code Service Agent service-PROJECT_NUMBER@gcp-sa-aiplatform.iam.gserviceaccount.com AI Platform Service Agent Vertex AI Service Agent Vertex AI Custom Code Service Agent このロールはカスタムトレーニングコードを実行するために使用され，付与されているロールは Vertex AI Custom Code Service Agent (roles/aiplatform.customCodeServiceAgent) ドキュメントから確認できます．\nVertex AI Service Agent このロールはVertex AI 全般の機能を動作させるために使用され，付与されているロールは Vertex AI Service Agent (roles/aiplatform.serviceAgent) ドキュメントから確認できます．\n実際のパイプラインの各コンポーネントの権限は，上記2つのサービスアカウントのどちらかが使われることになるので，このどちらかのサービスアカウントに予め付与されていないサービスを使う場合は権限エラーになってしまいます．そのため，使用したいサービスの権限がリンク先のアクセスコントロールのページで付与されていない場合は，IAM ロールの画面から追加する必要が出てきます．\nGCP 側が用意したサービスアカウントに IAM ロールを付与する方法 2つのサービスアカウントのうち使用されるアカウントに対して，適宜必要なロールを付与することで権限がなかったサービスにもアクセスすることができます．\n例えば，Firestore へのアクセスは上記2つのアカウントには付与されていないため，デフォルトの状態だと権限不足でアクセスできません．そのため read/write できる Cloud Datastore User (roles/datastore.user) のロールを追加で付与することで Firestore への読み書きができるようになります．\n画面上からだと，上図の「Include Google-provided role grants」にチェックを入れることで，GCP 側が用意したサービスアカウントが表示されます．上記2つのサービスアカウントを探して必要な権限を編集して追加することで権限エラーを回避することができます．\nおわりに job.summit 時に指定したサービスアカウントでパイプラインのコンポーネント動かせると思い込んでいて，実際はパイプラインを実行するだけだったので，若干混乱しました．\nパイプラインを動かす実際のサービスアカウントは別で2つ存在しており，このドキュメントを見つけることがすぐにできず，時間を消費してしまったのもあり，もしこの辺りで悩んでいる人が居れば参考になるかなと思い，備忘録として残すことにしました．\n","date":"2023-08-14","permalink":"https://masatakashiwagi.github.io/portfolio/post/service-account-for-vertex-ai-pipelines/","tags":["GCP","DEV"],"title":"Vertex AI Pipelines のサービスアカウントで少しつまずいたので整理した"},{"content":"はじめに Annoy という Spotify が開発している Python 製の ANN (Approximate Nearest Neighbors) のライブラリがあり，それを使ってレコメンドアイテムの類似度を計算する機会があったのですが，コンテナ化したものを Vertex AI Pipelines 上で動かしていたところ，Fatal Python error: Illegal instruction というエラーが発生して困っていたので，今回はこのエラーの対処方法について書いていこうと思います．\n※ Annoy については，こちらの ZOZO さんの記事が詳しく解説してくれています．\ndocker build 時の環境による問題？ 発生した事象 Annoy のライブラリを含んだ Docker イメージを作り，それを元に作成したコンポーネントを機械学習パイプラインである Vertex AI Pipelines で動かしたところ，Fatal Python error: Illegal instruction というエラーが発生しました．これは具体的には，AnnoyIndex を構築する際に発生しているように思われます．\nAnnoy を含む Python ライブラリは poetry で管理し，Dockerfile 内で poetry install しています．\nエラー原因 今回の問題は，Docker コンテナのイメージをビルドする時に指定するオプションが原因でした．\nローカル環境は Apple M1 Max (ARM アーキテクチャ) で開発していたため，イメージ作成時に以下のオプションを指定してビルドしていました．\ndocker build --platform linux/amd64 -t sample-recsys:latest -f ./Dockerfile . AMD 環境でも動くように --platform フラグにターゲットプラットフォームである linux/amd64 を指定してビルドしていました．このビルドしたイメージを GCP の Artifact Registry にプッシュし，そのイメージを使って Vertex AI Pipelines で各コンポーネントの検証を行っていました．\n一方で，--platform フラグを付けずに M1 Mac からビルドしたイメージを使った場合には，exec format error というエラーが発生します．\n検証時には，ローカル環境から直接イメージをビルド \u0026amp; プッシュし，そのイメージを使ったコンポーネントを Vertex AI Pipelines 上で動かして検証を行っていました．その際に使用していたスクリプトをそのまま GitHub Actions での CI/CD 構築時に使用したことで，今回の Fatal Python error: Illegal instruction という事象が発生することになります．\nIllegal instruction というエラーが発生するという事象はいくつか Issue が上がっていました．\nError: illegal hardware instruction (core dumped) #492 \u0026ldquo;Illegal instruction: 4\u0026rdquo; when trying to build index (Python 3.7, Annoy version 1.17.0, macOS 10.14.6) #513 --platform フラグを付けてイメージをビルドしても問題ないライブラリも多数あるのですが，Annoy では linux/amd64 を指定したのを GitHub Actions の Runner (ubuntu-latest) で動かしたのがどうも上手く行かなかったみたいです．\nInitializing an AnnoyIndex crashes on AMD processors #472 解決方法 僕の例では，Initializing an AnnoyIndex crashes on AMD processors #472 の Issue に記載されている方法で解決することができました．\nAnnoy のライブラリをインストールする際に，コンパイルパラメータである ANNOY_COMPILER_ARGS を Dockerfile 内に環境変数として指定することで解決することができました．annoy/setup.py を見ると良いかもしれません．\nENV ANNOY_COMPILER_ARGS -D_CRT_SECURE_NO_WARNINGS,-DANNOYLIB_MULTITHREADED_BUILD,-mtune=native この環境変数を Dockerfile にセットすると，--platform に linux/amd64 を付け Docker イメージを GitHub Actions 経由でビルドしたものを Vertex AI Pipelines で使用しても，エラー無く処理が完了しました．\nあとは，そもそもこれはローカルの M1 Mac と GitHub Actions の両方で同一のスクリプトを使用したいが為に行っている対応策なので，スクリプトを別々にすれば，GitHub Actions でビルドする際には，上記の ANNOY_COMPILER_ARGS を設定することなく，シンプルに以下のビルドコマンドだけでいけます．\ndocker build -t sample-recsys:latest -f ./Dockerfile . また，，--platform のオプションを付けてビルドすると時間が余計にかかるみたいなので，無駄なものは付けない方が良さそうですね．\nおわりに 今回は思いもよらないエラーで悩んでいたので，こうして無事？エラーの原因と解決策が分かって良かったです．全然このことに気づかなかったので，Annoy をやめて Faiss を使用することも考えていました（Faiss でも同じことになっていたかもです笑）．\nコンテナイメージをビルドする際のローカルとプロダクション適用時のマシンスペックが違うことも考えた上で，再現性を意識したコードを書かなければと改めて感じたので，良い教訓となりそうです．\n参考 近傍探索ライブラリ「Annoy」のコード詳解 ","date":"2023-08-09","permalink":"https://masatakashiwagi.github.io/portfolio/post/annoy-lib-illegal-instruction-error/","tags":["DEV","ML"],"title":"ANN ライブラリの Annoy で build index する時に Illegal Instruction Error が発生した"},{"content":"はじめに Python のデータフレームを使ってレコメンドリストを生成した場合に遭遇した内容で，備忘録がてら筆を取ってます．内容としては，リストを ndarray に変換して，それらの値が入ったデータフレームを csv ファイルとして保存し，再度ロードする場合に発生する事象への対応方法になります．\n結論は，データフレームにリスト形式のデータを保存したい場合は，str 型に変換してから保存するようにしましょう！ということです．これは csv のカンマ区切りでの保存による影響を受けてしまうからです．\n発生した事象 例えば，各ユーザーに対してアイテムをレコメンドしたい場合を考えてみます．各ユーザーに対して用意されたレコメンドリストがあるとします．10人ユーザーが存在するとしてサンプルデータを作って考えてみると，\nimport random import pandas as pd import numpy as np user_id = [random.randint(1, 99999) for _ in range(10)] recommend_items = [np.array([random.randint(1, 99999) for _ in range(3)]) for _ in range(10)] # recommend_itemsのカラムには，リストをndarrayに変換したデータが入っている df_rec = pd.DataFrame({\u0026quot;user_id\u0026quot;: user_id, \u0026quot;recommend_items\u0026quot;: recommend_items}) print(df_rec['recommend_items'][0]) \u0026gt; array([4775, 44953, 85874]) # 数値は適当に入れてます print(type(df_rec['recommend_items'][0])) \u0026gt; numpy.ndarray データフレームの中身を見ると，以下のようなデータが入っています．\nuser_id recommend_items 0 8482 [4775, 44953, 85874] 1 83899 [96090, 53639, 82456] 2 93606 [8355, 76477, 58666] recommend_items のカラムのデータの実態は ndarray になっているのですが，見た目はリスト型のデータが入っていると勘違いが起きたりします．特に色々な処理を行っていくと，途中で型が変わっていることに気づいていない場合もあるかと思います．\nここで，各ユーザーのレコメンドリストをデモ的に ndarray に変換していますが，\nBigQuery に配列で格納されているデータをロードした場合 処理過程で，Numpy 配列に変換した方が高速に処理でき，結果をそのまま格納した場合 etc\u0026hellip; などの場合で生じることがあるかなと思います．\nこのデータフレームを csv ファイルに保存して，ロードすると，\n# csvファイルで結果を保存 df_rec.to_csv(\u0026quot;./tmp_recommend_results1.csv\u0026quot;, index=False) # 保存したcsvファイルをデータフレームとしてロードする tmp_df_rec = pd.read_csv(\u0026quot;./tmp_recommend_results1.csv\u0026quot;) print(tmp_df_rec['recommend_items'][0]) \u0026gt; [ 4775 44953 85874] 要素を1つ取り出してみると，カンマが取れて右詰めのような形式で表示されます．\nndarray を csv ファイルに保存する前にリストに変換する ndarray のまま csv ファイルに保存してしまうと，ロード時に意図しない形式で処理ができなくなってしまうので，簡単な方法としては，事前にリストに変換してからデータフレームに入れることが望ましいです．\nrecommend_items_list = [list(arr) for arr in df_rec['recommend_items']] df_rec['recommend_items'] = recommend_items_list print(type(df_rec['recommend_items'][0])) \u0026gt; list df_rec.to_csv(\u0026quot;./tmp_recommend_results2.csv\u0026quot;, index=False) tmp_df_rec = pd.read_csv(\u0026quot;./tmp_recommend_results2.csv\u0026quot;) print(type(tmp_df_rec['recommend_items'][0])) \u0026gt; \u0026lt;class 'str'\u0026gt; リストに変換してから csv ファイルで保存すると，値は文字列になります．リストとして再度使いたい場合は，以下のように組み込み関数の eval() か，ast.literal_eval() を使って文字列をリストに変換するのが良いです．\nimport ast # 以下のどちらかを使うと良い tmp_df_rec['recommend_items'] = tmp_df_rec['recommend_items'].apply(eval) tmp_df_rec['recommend_items'] = tmp_df_rec['recommend_items'].apply(ast.literal_eval) print(type(type(tmp_df_rec['recommend_items'][0]))) \u0026gt; list eval() と ast.literal_eval() の違いは，ast.literal_eval() はリテラルのみを含む式を評価するのに対して，eval() はリテラルに加え変数およびそれらの演算を含んだ式を評価することができます．\nこれでリストとして元の値を使うことができます．\nおわりに 今回の事象に全然気づかずにそのままデータを保存していて，いざ分析しようとした時に，あれ？なんかおかしいぞとなり発覚しました．正規表現などを使えば無理やりできるのかもしれないですが，あまり好ましいやり方ではないと思うので，今回のように忘れずにリストに変換するのが良いと思います．\nあとはそもそも csv ファイルに保存せずに別のファイル形式で保存するやり方もあると思います．が，なんやかんや分析する場合は csv ファイルが楽だったりもします笑\nでも，最近 csv ファイル絡みでの予期せぬことに遭遇することが多くて嫌になりつつあります\u0026hellip;😅\n参考 Pythonのast.literal_eval()で文字列をリストや辞書に変換 ","date":"2023-07-31","permalink":"https://masatakashiwagi.github.io/portfolio/post/note-on-saving-array-list-in-dataframe/","tags":["DEV","DATA"],"title":"データフレームでリストを ndarray にした値を csv ファイルで保存する場合の注意"},{"content":"はじめに 最近，スタバで期間限定で販売しているの冬の新作「バターキャラメルミルフィーユ ラテ」のミルクをアーモンドミルクに変更して飲んだら激ウマでハマってしまいました笑．皆さんも是非飲んでみて下さい！（ちなみに寒いので，ホットで注文しました😋）\n今回のポストは 情報検索・検索技術 Advent Calendar 2022 の20日目の記事になります．\nどんな内容の記事を書こうか悩みましたが，今回は技術的なものではなく，僕が今年の5月半ばぐらいから関わり出した「検索」に関しての取り組みやその感想を書こうと思います．具体的には，検索基盤の整備〜検索エンジンのチューニング〜輪読会などを通して感じたことを書きたいと思います．\nモチベーション的には，後から見返した時に当時の検索に対する初々しい気持ちはどんなものだったのかを残しておきたい気持ちです笑\n検索する上で欠かせない検索基盤 何と言っても検索エンジンにデータを連携し供給しないと検索は何もできないので，まずは検索基盤を整備するのが必要です．検索という行為は普段から Google 先生に聞いたり，各種Webサイト内で検索をしたりと何気なしに使っていますが，新しいデータを追加して検索できるようにするためには，検索エンジンのインデックスにデータを追加する必要があるというのを正直この時初めて知りました笑（最初はインデックスという単語をよく理解していなかったです笑）．\n※ 検索エンジンを使わずに，データベースに検索クエリを投げているケースもあると思いますが，ここでは，検索エンジンを使っているとします．\n検索エンジンにデータを定常的に連携するために検索基盤を用意するわけですが，検索基盤の具体的な構成とかの話は，僕が以前登壇したこちらの資料「JAWS DAYS 2022 - AWS のマネージドサービスで実現するニアリアルタイムな検索基盤」を見て頂けるとありがたいです．\n構築段階よりはどう日々の運用と上手く付き合っていくか，「日々運用をしていく上で，何に気をつけるべきなのか？どういったことを意識すると良いのか？」を考えると，そういった話は調べてもなかなか出てこないし，環境によって違うしという感じで悩ましいなという気持ちです．が，僕は以下の2つを意識しながら，設計して作るようにしました．\nユーザーへの影響を考える ユーザー視点で見ると，検索機能が使えなくなることは，そのサービスへの期待値を下げてしまうことになりますし，欲しい・知りたい情報を見つけられなくなるため，影響が大きいです．そのため，アラートの粒度やエラー時のリカバリーをどうするか，今回の検索基盤だと2つのインデックスを運用していたので，切り替え時にエラーが出てもそのまま動き続けれるように，削除タイミングやどのタイミングで切り替えるのが良いのか，などは考慮しながら作るようにしていました． システムを運用していくチームメンバーのことも考える 基盤を作った僕以外の人が触る時に，簡単に処理を修正できるか？エラーが起きた場合のリカバリーが簡単にできるか？といった部分は，構築後の運用において属人化を避けるためにも意識したいところでした． ドキュメントをきちんと用意することもそうですし，処理をモジュール単位で作る（Step Functions だとこの辺りは整理しやすいです）とかは意識していました． より大きなシステムを運用するとなると，また違った要素も必要なのかも知れないです．\nシステムは作った後の日々の運用をどうしやすくするかが大事だと思うので，この点は今後も意識したいと思っています．（なんか，検索システムに限った話ではない感じになってしまいましたね😅）\n検索基盤の構築で個人的にこれは良かったなと思った取り組みは，インデックスの切り替えで，Blue/Green deployment をパイプライン（AWS Step Functions）の中で行うようにすることで，安全に切り替わる方式を作れたのは，ユーザーへの影響を考えると良い取り組みだったかなと思います．他の会社さんはどういった感じで運用されているのかとても気になっているので，是非教えて下さい笑\nなので，検索基盤が安定\u0026amp;信頼できるものでないと，ユーザー影響もそうですし，検索エンジンの精度改善だったりもおぼつかなくなるので，検索基盤大事ですね！\nみんなで検索の書籍を読んでワイワイ話す チーム内の検索システムに対する共通認識や目線を合わせるために，「検索システム - 実務者のための開発改善ガイドブック（通称ペンギン本）」の輪読会を僕が主催したりもしました．（チーム内でも個々人で検索システムに対しての理解のバラツキがあったので，理解を深めて議論が活発になると良いなと思い始めました．あとは，単純にこの書籍がとても面白そうだったので，みんなで読みたいと思い始めた感じです笑）\nこの書籍は名の通りで，実務向けに書かれていてとても参考になることが多かったです．特に後半の第II部にその要素が詰め込まれていて，\n検索システムの運用の話 上で話した Blue/Green deployment の話やオフライン評価，障害検知など実務的に考えていく必要があることが載っていて，どういったことを運用時には考えて用意しておく必要があるのかなど参考になります 分析の観点の話 検索クエリだけでなく，その結果がどうだったかなどを考えるべきといったことが書かれています 改善箇所の発見プロセス Why-What-Who を定めないと意味のない改善になってしまういった心に刺さることが書かれています 機械学習を使ったランキングの話 あまり体系的に整理されたものはないと思うので，非常にありがたいことが書かれています etc\u0026hellip; これ以外にも色々とありすぎて全部はここに書けないですが，検索をする人は一度は目を通しておくべき内容だなと思いました！（定期的に読み直したいです）\n輪読会を通して，検索システムに対する理解やそれが及ぶ範囲などを知ることができたのと，それをチームメンバーでうちのサービスだとどうか？など自分達の状況に置き換えながら議論して，内容を共有しながら進めることができたので，とても学びがある会になったと思います．\nただ，準備に時間がかかったり，読み込みの時間が十分でなく議論がそこまで活発化しなかった会もあったので，そういった場合は会の最初に黙々と読む時間を用意しても良かったのかもと思ったりもしました．\n検索システムは奥が深くて知れば知るほど関わる人も多く一筋縄ではいかないなと思う一方で，取り組み甲斐のある面白い領域だなと感じてます😊\n検索エンジンのチューニングって難しいな メインは検索基盤周りを扱っていて，今の所チューニング部分は少しだけ関わっていますが，\nAnalyzer の定義 Query DSL の定義 辞書整備 etc\u0026hellip; 上に挙げたあたりで難しいなと日々感じてます\u0026hellip;\n最初の方はゼロ件ヒットの結果や検索ログに基づいたユーザー辞書・同義語辞書の整備は色々とやっていて，特に同義語辞書は何を同義語と設定するか，双方向なのか一方向の辞書展開なのかは答えがないので，言葉の意味や定義を調べたり，ユーザーの一人である妻に聞いたりしながら設定していました．（同義語の収集自動化したい\u0026hellip;）\nまた，検索ログを眺めていると，ユーザーが何を調べているか，ワードもそうですし全体でのそのワード回数などを感覚的に掴めるので，一度やったのは正解でした！\n他には，こんな感じで検索ワード入れると検索にヒットするのになーと思ったりして，ユーザー全員が検索の仕方が上手いわけではないので，サジェストで「これのことですか？」とか，意味は同じで「こちらの方がヒット数が多いですよ」とか，もう少しガイドがあったりすると改善できるのかな？と感じたり．\nあとは内部的に表記揺れを吸収して直したりとかはありそうだなと感じたりはしています．すぐに技術的に解決が難しくても，UI でもそういったガイドはできると思っていて，この辺りはエンジニアリングとデザインが上手く混ざり合うと良い体験をユーザーに提供できるのかなと感じています．\n機械学習の導入に関しては，検索結果のパーソナライズ化できる利点もありますが，それを動かす環境だったりレイテンシーを抑えて計算して結果を表示する工夫が必要だと思うので，そんなに簡単には導入でき無さそうかなと思ったりしてます🤔\nおわりに 半年程度検索という領域に携わって，手探りながら進めて感じたのは，\n検索ログを泥臭く見て改善ポイントを見つけたり，ユーザーの行動に思いを馳せる ゼロ件ヒットワードや検索利用率などの各種メトリクスを取って可視化できるようにする ロジック変更したら，定性チェックして検索結果に違和感がないか見る エラー検知はもちろんだけど，エラー時の Runbook を用意する リカバリーする手順はなるべく自動化して手動によるオペミスを減らす（インデックスの再作成） などが大事だなと思ったり（他にもいっぱいあると思います笑），あとは検索で良い体験を提供するためには，単純にアルゴリズムの話だけでなく，\nUI/UX 信頼できる検索基盤 検索エンジンのチューニング 辞書の整備 検索のパフォーマンス etc\u0026hellip; 本当に総合格闘技で取り組まないと成功しないなというの実感しているのと，そもそもユーザーにどういった検索体験を提供したいかを考えないと目指す方向性がブレてしまうので，言語化が必要だなというお気持ちです😅 これからもいっぱい悩んでいこうと思います！\n","date":"2022-12-20","permalink":"https://masatakashiwagi.github.io/portfolio/post/my-thoughts-on-the-search-experienced-in-the-first-six-months/","tags":["POEM"],"title":"検索に対する感想 - 検索基盤から検索エンジンの改善を始めて半年経て"},{"content":"はじめに Glue を使ってデータ連携する際に，例えばデータ連携したい期間を変えたり，環境情報を渡したり，などのパラメータを与えて実行したい場合の備忘録です．特に，Step Functions (SFn) 経由で Glue を実行する場合に，インプットパラメータに必要な情報を渡してそれを Glue にどう紐付けるかに関する内容になってます．\n今回の内容は，実務で発生した検索エンジンのインデックスに簡単にデータを流せるように，SFn のパイプラインのインプットにパラメータを渡すだけで実行できるようにしたかったので，その時に実施したお話です．\nGlue のジョブパラメータ設定 ジョブパラメータは Glue 実行時に渡すことができるパラメータで，デフォルトでもいくつか用意されています（参考：Job parameters used by AWS Glue）．\nこれに自前で用意したパラメータを受け取りたい場合，getResolvedOptions の第二引数にリストに渡されてくるパラメータを定義します．これは後ほどの SFn のインプットに与えて Glue で受け取るものです．\n# JobName: sample_glue_job import sys from awsglue.utils import getResolvedOptions # 動的に切り替えたい or 環境により変化する部分をパラメータとして受け取る args = getResolvedOptions( sys.argv, [\u0026quot;period_date\u0026quot;, \u0026quot;index_name\u0026quot;] ) # データ連携したい期間 period_date = int(args[\u0026quot;period_date\u0026quot;]) # データを投入する検索エンジンのインデックス名 index_name = args[\u0026quot;index_name\u0026quot;] ...(実際の処理が続く) パラメータは文字列のみしか受け付けないので，期間を数値で使いたい場合は，上記コードのように数値型に変換する必要があります．\nStep Functions の input にデータを渡して Glue で使う方法 Glue ジョブが作成できたら，それを Step Functions で動かしていきます．\n以下に Step Functions の State Machine のサンプルコードを載せていますが，ポイントは glue:startJobRun.sync アクションの Parameters にある Arguments の設定です．input パラメータを渡す場合は，$を key の末尾と value の先頭に付与する必要があります（参考：Pass Parameters to a Service API）．\nまた，key の先頭に--を付与しないとパラメータと認識されずにエラーになるので，注意が必要です．\n{ \u0026quot;Comment\u0026quot;: \u0026quot;Glueでデータ連携を行うステートマシン\u0026quot;, \u0026quot;StartAt\u0026quot;: \u0026quot;Glue-Job\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;Glue-Job\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::glue:startJobRun.sync\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;JobName\u0026quot;: \u0026quot;sample_glue_job\u0026quot;, \u0026quot;Arguments\u0026quot;: { \u0026quot;--period_date.$\u0026quot;: \u0026quot;$.period_date\u0026quot;, \u0026quot;--index_name.$\u0026quot;: \u0026quot;$.index_name\u0026quot; } }, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;FailState\u0026quot; } ], \u0026quot;Comment\u0026quot;: \u0026quot;データ連携用のGlueジョブ\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Success\u0026quot; }, \u0026quot;Success\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Succeed\u0026quot; }, \u0026quot;FailState\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Fail\u0026quot;, \u0026quot;Cause\u0026quot;: \u0026quot;Error\u0026quot;, \u0026quot;Error\u0026quot;: \u0026quot;Error\u0026quot; } } } SFn のコンソールから実行する場合，下図のように実行前の画面で JSON 形式で必要なパラメータを渡すことで，SFn で定義した Glue のジョブパラメータに値がセットされます．\n今回の場合だと，period_date と index_name をパラメータとして SFn の input から渡して，Glue でそれらを受け取り ETL 処理を実行していきます．\nおわりに 今回は Step Functions の input パラメータを変更することで，簡単に Glue のジョブパラメータに値を渡してパイプラインを実行する方法を紹介しました．\nGlue Studio からコードを直接変更することでもできますが，毎回コードを変更するのはバグが混入する可能性もあるので，パイプライン実行時の input で制御できた方が汎用性があり，シンプルかなと思い試してみました．\n参考 Job parameters used by AWS Glue Pass Parameters to a Service API ","date":"2022-11-16","permalink":"https://masatakashiwagi.github.io/portfolio/post/glue-job-params-and-stepfunctions-execution/","tags":["AWS","DEV"],"title":"Step Functions から Glue のジョブパラメータを指定して実行する方法"},{"content":"はじめに OpenSearch の Package 更新，つまりユーザー辞書やシノニム辞書の更新を Step Functions で行う場合に，直列で行うのが良いか並列で行うのが良いかメモ程度の備忘録として残しておきます．\n結論としては，OpenSearch の Package 更新は同時に複数更新すると，関連付けでエラーが発生する可能性があるので，直列でパイプラインを組みのが良いと思いました．特にインスタンススペックが低いとドメインへの負荷でエラーになる可能性が高いと思います．\n以下のようなエラーが発生する場合があります．\nStep Functions による辞書更新パイプライン まず初めに OpenSearch の Package を API で更新する手順を説明すると\nOpenSearch: UpdatePackage\nupdate-package\nAPI パラメータとして以下の JSON を渡す感じです\n{ \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;OpenSearchのドメインに関連付けられたパッケージの内部ID\u0026gt;\u0026quot;, \u0026quot;PackageSource\u0026quot;: { \u0026quot;S3BucketName\u0026quot;: \u0026quot;\u0026lt;パッケージが置かれているバケット名\u0026gt;\u0026quot;, \u0026quot;S3Key\u0026quot;: \u0026quot;\u0026lt;パッケージのファイル名\u0026gt;\u0026quot; } } OpenSearch: AssociatePackage\nassociate-package\nAPIパラメータとして以下の JSON を渡す感じです\n{ \u0026quot;DomainName\u0026quot;: \u0026quot;\u0026lt;関連付けを行う OpenSearch のドメイン名\u0026gt;\u0026quot;, \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;OpenSearch のドメインに関連付けられたパッケージの内部 ID \u0026gt;\u0026quot; } この2つを実行するだけになります．\n一方で，それぞれのAPIを実行すると処理が走るが，更新や関連付けには一定の時間が必要になります．そのため，OpenSearch: ListDomainsForPackage でドメインとパッケージの状態を確認し，ACTIVE 状態になったら次の処理を実行する必要があります．\nこれ踏まえて，ユーザー辞書とシノニム辞書の2つを更新する処理を直列と並列それぞれ実装してみます．\n直列でパイプラインを組んだ場合 ユーザー辞書を更新した後にシノニム辞書の更新を行うパイプラインになります．\n直列パイプライン - DSL { \u0026quot;Comment\u0026quot;: \u0026quot;全量データの同期と辞書更新のジョブ\u0026quot;, \u0026quot;StartAt\u0026quot;: \u0026quot;Update-Package-User-Dict\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;Update-Package-User-Dict\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot;, \u0026quot;PackageSource\u0026quot;: { \u0026quot;S3BucketName\u0026quot;: \u0026quot;\u0026lt;バケット名\u0026gt;\u0026quot;, \u0026quot;S3Key\u0026quot;: \u0026quot;\u0026lt;ファイル名\u0026gt;\u0026quot; } }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:updatePackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのアップデート\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure\u0026quot; } ], \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User1\u0026quot; }, \u0026quot;List-Domains-For-Package-User1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-User1\u0026quot; }, \u0026quot;Choice-Package-Active-Check-User1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Associate-Package-User\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-User1-10s\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータスに応じた処理の分岐\u0026quot; }, \u0026quot;Associate-Package-User\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;DomainName\u0026quot;: \u0026quot;\u0026lt;ドメイン名\u0026gt;\u0026quot;, \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:associatePackage\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure\u0026quot; } ], \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User2\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージの関連付けを行う\u0026quot; }, \u0026quot;List-Domains-For-Package-User2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-User2\u0026quot; }, \u0026quot;Choice-Package-Active-Check-User2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Pass\u0026quot; }, { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ASSOCIATION_FAILED\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-User2-60s\u0026quot; }, \u0026quot;Pass\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Pass\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Update-Package-Synonym-Dict\u0026quot; }, \u0026quot;Update-Package-Synonym-Dict\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot;, \u0026quot;PackageSource\u0026quot;: { \u0026quot;S3BucketName\u0026quot;: \u0026quot;\u0026lt;バケット名\u0026gt;\u0026quot;, \u0026quot;S3Key\u0026quot;: \u0026quot;\u0026lt;ファイル名\u0026gt;\u0026quot; } }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:updatePackage\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure\u0026quot; } ], \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym1\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのアップデート\u0026quot; }, \u0026quot;List-Domains-For-Package-Synonym1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-Synonym1\u0026quot; }, \u0026quot;Choice-Package-Active-Check-Synonym1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Associate-Package-Synonym\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-Synonym1-10s\u0026quot; }, \u0026quot;Associate-Package-Synonym\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;DomainName\u0026quot;: \u0026quot;\u0026lt;ドメイン名\u0026gt;\u0026quot;, \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:associatePackage\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure\u0026quot; } ], \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym2\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージの関連付けを行う\u0026quot; }, \u0026quot;List-Domains-For-Package-Synonym2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-Synonym2\u0026quot; }, \u0026quot;Choice-Package-Active-Check-Synonym2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Success-Associate-Package-Synonym\u0026quot; }, { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ASSOCIATION_FAILED\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-Synonym2-60s\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータスに応じた処理の分岐\u0026quot; }, \u0026quot;Success-Associate-Package-Synonym\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Succeed\u0026quot; }, \u0026quot;Wait-Synonym2-60s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 60, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym2\u0026quot; }, \u0026quot;Wait-Synonym1-10s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 10, \u0026quot;Comment\u0026quot;: \u0026quot;待機\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym1\u0026quot; }, \u0026quot;Wait-User2-60s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 60, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User2\u0026quot; }, \u0026quot;Wait-User1-10s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 10, \u0026quot;Comment\u0026quot;: \u0026quot;待機\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User1\u0026quot; }, \u0026quot;NotifySlackFailure\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::lambda:invoke\u0026quot;, \u0026quot;OutputPath\u0026quot;: \u0026quot;$.Payload\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;Payload.$\u0026quot;: \u0026quot;$\u0026quot;, \u0026quot;FunctionName\u0026quot;: \u0026quot;\u0026lt;LambdaのARN\u0026gt;\u0026quot; }, \u0026quot;Retry\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;Lambda.ServiceException\u0026quot;, \u0026quot;Lambda.AWSLambdaException\u0026quot;, \u0026quot;Lambda.SdkClientException\u0026quot; ], \u0026quot;IntervalSeconds\u0026quot;: 2, \u0026quot;MaxAttempts\u0026quot;: 6, \u0026quot;BackoffRate\u0026quot;: 2 } ], \u0026quot;Comment\u0026quot;: \u0026quot;処理失敗のslack通知\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;FailState\u0026quot; }, \u0026quot;FailState\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Fail\u0026quot;, \u0026quot;Error\u0026quot;: \u0026quot;Error\u0026quot;, \u0026quot;Cause\u0026quot;: \u0026quot;Error\u0026quot; } } } 並列でパイプラインを組んだ場合 ユーザー辞書とシノニム辞書の更新を同時に実行するパイプラインになります．\n並列パイプライン - DSL { \u0026quot;Comment\u0026quot;: \u0026quot;全量データの同期と辞書更新のジョブ\u0026quot;, \u0026quot;StartAt\u0026quot;: \u0026quot;Parallel\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;Parallel\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Parallel\u0026quot;, \u0026quot;Branches\u0026quot;: [ { \u0026quot;StartAt\u0026quot;: \u0026quot;Update-Package-User-Dict\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;Update-Package-User-Dict\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot;, \u0026quot;PackageSource\u0026quot;: { \u0026quot;S3BucketName\u0026quot;: \u0026quot;\u0026lt;バケット名\u0026gt;\u0026quot;, \u0026quot;S3Key\u0026quot;: \u0026quot;\u0026lt;ファイル名\u0026gt;\u0026quot; } }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:updatePackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのアップデート\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User1\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure1\u0026quot; } ] }, \u0026quot;NotifySlackFailure1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::lambda:invoke\u0026quot;, \u0026quot;OutputPath\u0026quot;: \u0026quot;$.Payload\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;Payload.$\u0026quot;: \u0026quot;$\u0026quot;, \u0026quot;FunctionName\u0026quot;: \u0026quot;\u0026lt;LambdaのARN\u0026gt;\u0026quot; }, \u0026quot;Retry\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;Lambda.ServiceException\u0026quot;, \u0026quot;Lambda.AWSLambdaException\u0026quot;, \u0026quot;Lambda.SdkClientException\u0026quot; ], \u0026quot;IntervalSeconds\u0026quot;: 2, \u0026quot;MaxAttempts\u0026quot;: 6, \u0026quot;BackoffRate\u0026quot;: 2 } ], \u0026quot;Comment\u0026quot;: \u0026quot;処理失敗のslack通知\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;FailState1\u0026quot; }, \u0026quot;FailState1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Fail\u0026quot;, \u0026quot;Error\u0026quot;: \u0026quot;Error\u0026quot;, \u0026quot;Cause\u0026quot;: \u0026quot;Error\u0026quot; }, \u0026quot;List-Domains-For-Package-User1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-User1\u0026quot; }, \u0026quot;Choice-Package-Active-Check-User1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Associate-Package-User\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-User1-10s\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータスに応じた処理の分岐\u0026quot; }, \u0026quot;Associate-Package-User\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;DomainName\u0026quot;: \u0026quot;\u0026lt;ドメイン名\u0026gt;\u0026quot;, \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:associatePackage\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User2\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure1\u0026quot; } ] }, \u0026quot;List-Domains-For-Package-User2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-User2\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot; }, \u0026quot;Choice-Package-Active-Check-User2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Success-Associate-Package-User\u0026quot; }, { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ASSOCIATION_FAILED\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure1\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-User2-60s\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータスに応じた処理の分岐\u0026quot; }, \u0026quot;Success-Associate-Package-User\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Succeed\u0026quot; }, \u0026quot;Wait-User2-60s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 60, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User2\u0026quot; }, \u0026quot;Wait-User1-10s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 10, \u0026quot;Comment\u0026quot;: \u0026quot;待機\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-User1\u0026quot; } } }, { \u0026quot;StartAt\u0026quot;: \u0026quot;Update-Package-Synonym-Dict\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;Update-Package-Synonym-Dict\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot;, \u0026quot;PackageSource\u0026quot;: { \u0026quot;S3BucketName\u0026quot;: \u0026quot;\u0026lt;バケット名\u0026gt;\u0026quot;, \u0026quot;S3Key\u0026quot;: \u0026quot;\u0026lt;ファイル名\u0026gt;\u0026quot; } }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:updatePackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのアップデート\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym1\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure2\u0026quot; } ] }, \u0026quot;NotifySlackFailure2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::lambda:invoke\u0026quot;, \u0026quot;OutputPath\u0026quot;: \u0026quot;$.Payload\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;Payload.$\u0026quot;: \u0026quot;$\u0026quot;, \u0026quot;FunctionName\u0026quot;: \u0026quot;\u0026lt;LambdaのARN\u0026gt;\u0026quot; }, \u0026quot;Retry\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;Lambda.ServiceException\u0026quot;, \u0026quot;Lambda.AWSLambdaException\u0026quot;, \u0026quot;Lambda.SdkClientException\u0026quot; ], \u0026quot;IntervalSeconds\u0026quot;: 2, \u0026quot;MaxAttempts\u0026quot;: 6, \u0026quot;BackoffRate\u0026quot;: 2 } ], \u0026quot;Next\u0026quot;: \u0026quot;FailState2\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;処理失敗のslack通知\u0026quot; }, \u0026quot;FailState2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Fail\u0026quot;, \u0026quot;Error\u0026quot;: \u0026quot;Error\u0026quot;, \u0026quot;Cause\u0026quot;: \u0026quot;Error\u0026quot; }, \u0026quot;List-Domains-For-Package-Synonym1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-Synonym1\u0026quot; }, \u0026quot;Choice-Package-Active-Check-Synonym1\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Associate-Package-Synonym\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-Synonym1-10s\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータスに応じた処理の分岐\u0026quot; }, \u0026quot;Associate-Package-Synonym\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;DomainName\u0026quot;: \u0026quot;\u0026lt;ドメイン名\u0026gt;\u0026quot;, \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:associatePackage\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym2\u0026quot;, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure2\u0026quot; } ] }, \u0026quot;List-Domains-For-Package-Synonym2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;PackageID\u0026quot;: \u0026quot;\u0026lt;パッケージID\u0026gt;\u0026quot; }, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::aws-sdk:opensearch:listDomainsForPackage\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Choice-Package-Active-Check-Synonym2\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータス確認\u0026quot; }, \u0026quot;Choice-Package-Active-Check-Synonym2\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Choice\u0026quot;, \u0026quot;Choices\u0026quot;: [ { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;Success-Associate-Package-Synonym\u0026quot; }, { \u0026quot;Variable\u0026quot;: \u0026quot;$.DomainPackageDetailsList[0].DomainPackageStatus\u0026quot;, \u0026quot;StringEquals\u0026quot;: \u0026quot;ASSOCIATION_FAILED\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;NotifySlackFailure2\u0026quot; } ], \u0026quot;Default\u0026quot;: \u0026quot;Wait-Synonym2-60s\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;パッケージのステータスに応じた処理の分岐\u0026quot; }, \u0026quot;Success-Associate-Package-Synonym\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Succeed\u0026quot; }, \u0026quot;Wait-Synonym2-60s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 60, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym2\u0026quot; }, \u0026quot;Wait-Synonym1-10s\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Wait\u0026quot;, \u0026quot;Seconds\u0026quot;: 10, \u0026quot;Comment\u0026quot;: \u0026quot;待機\u0026quot;, \u0026quot;Next\u0026quot;: \u0026quot;List-Domains-For-Package-Synonym1\u0026quot; } } } ], \u0026quot;End\u0026quot;: true } } } パイプライン内では OpenSearch: ListDomainsForPackage でパッケージの状況を確認し，ACTIVE 状態でない場合は数十秒の待機処理を入れてから再度確認する方法で更新を行っています．\n辞書更新にかかる処理時間は5分以内ぐらいなので，より安全に実施できる直列で更新する方法に落ち着きました．\nおわりに 今回は OpenSearch の複数の辞書更新を Step Functions で行う場合に，直列で更新処理を組むのが良いか，並列で更新処理を組むのが良いかをというところで，より安全に実行するという観点で直列を選択しました．\nパイプラインが長くなってしまいますが，辞書更新用の Step Functions を用意することで，このSFをデータ同期のパイプラインのステップの1つとして組み込めば，処理単位でモジュール化することができるので，修正やデバッグもやりやすくなると思います．\n","date":"2022-10-29","permalink":"https://masatakashiwagi.github.io/portfolio/post/series-or-parallel-pipeline-for-updating-opensearch-package/","tags":["AWS","DEV"],"title":"Step Functions での Opensearch Package の更新は直列or並列？"},{"content":"はじめに 以前から気になっていた OSS の Airbyte という EL に特化した Data Integration ツールを使ってみたかったので，今回はこれを使って以前 Embulk で実装していたスプレッドシートから BigQuery へのデータ同期処理と同じことができるか試してみた話になります．\nAirbyte は良い感じのUIがあるので，UI をポチポチしながら設定していきます．\nAirbyteとは？ Airbyte は OSS の ETL ツールですが，特に Extract と Load に注力しているツールになります．豊富なデータソース（Source）とターゲットソース（Destination）に対応していて，これらを設定することでデータを簡単に連携することができます．Transform 部分は内部的には dbt を使ってハンドリングしているみたいです．（Transformations with SQL (Part 1/3)）\n提供形態としては，OSS とマネージドサービス（クラウド版: 有料）があり，ローカルをはじめ，AWS/GCP/Azure と各種クラウドサービスでデプロイすることができます（参考: Deploying Airbyte Open Source）．\nConnector は既に用意されているもの（airbyte/airbyte-integrations/connectors/）もあれば，独自で作成することもできます．\nスプレッドシートから BigQuery に連携 基本的には，Tutorial に沿って進めていきます．まずは git clone して UI を立ち上げます．\ngit clone https://github.com/airbytehq/airbyte.git cd airbyte docker compose up docker compose を実行したら，http://localhost:8000 で UI にアクセスすることができます．\nここからは UI の世界で全て完結することができます！\nSource 案内に従って進めると，まずプルダウンから今回のデータソースである Google Sheets を Source type として選択します．右側に Setup Guide があるので，見ながら設定できて親切設計だなと感じました．\n以下の設定を埋めていきます．\nSource name 「Google Sheets」としています Authentication GCP のサービスアカウントを設定します．json ファイルの中身をコピーして貼り付けます cat ~/.gcp/hoge_service_account.json | pbcopyでクリップボードにコピーするとやりやすいです Row Batch Size デフォルトの200にしています Spreadsheet Link 対象となるスプレッドシートの URL を設定します 事前にサービスアカウントでのアクセスを許可しておく必要があります Destination 次に，プルダウンからターゲットソースである BigQuery を Destination type として選択します．\n以下の設定を埋めていきます．\nDestination name 「BigQuery」としています Project ID GCP にアクセスして現在使っているプロジェクト ID を設定します Dataset Location 「US」で良さそう？ Default Dataset ID BigQuery のデータセットとして作成される ID になります Loading Method Standard Inserts と GCS Staging の2種類あります Standard Inserts SQL INSERT で直接アップロードする方法で，非効率的なため GCS Staging を推奨しています（今回はこちらを選択） GCS Staging ファイルにレコードを書き込み，そのファイルを GCS にアップロードし，その後 COPY INTO テーブルを使用してファイルをアップロードする方法（GCS のバケットなどの情報が必要になります） Service Account Key JSON (Required for cloud, optional for open-source) GCP のサービスアカウントを設定します．json ファイルの中身をコピーして貼り付けます Transformation Query Run Type (Optional) interactive と batch の2種類あります Google BigQuery Client Chunk Size (Optional) デフォルトの15にしています Connection 最後に，Connection のセットアップを行います．設定した Source と Destination をセットし，Replication frequency（同期頻度）, Destination Namespace や Prefix などを決めていきます．\nTransfer Replication frequency 手動実行やスケジュール実行を選択できる Streams Destination Namespace Mirror source structure, Destination default, Custom format の3種類あります Destination Stream Prefix (Optional) 必要に応じて付与します Normalization \u0026amp; Transformation Raw data (JSON) Normalized tabular data 上記どちらかを選択しますが，Raw data だと json のままデータが格納されるので，Normalized tabular data で良いと思います 個人的に同期するスプレッドシートの各シートがそれぞれ表示されて，どれを同期するか選択して決められるというのが感動しました 🎉\n今回は1シートだけ連携することにし，Custom Transform の処理はせずに単純にデータをそのまま連携していきます．\n実行結果のログは以下のような感じ Sync Succeeded となっています．\nAirbyte 側は大丈夫そうなので，BigQuery の方も確認してみると，ちゃんと入ってるので問題なさそうです！\nおわりに 今回は，OSS の Airbyte を使ったスプレッドシートから BigQuery へのデータ同期を行ってみました．Airbyte は UI が用意されていて，直感的に操作できる+設定も簡単でデータ同期の体験としてとても良かったです！データソースが豊富なのもメリットとして大きいと思います．\nExtract \u0026amp; Load のみしか使えていないので，次は dbt を理解して Transform も追加して処理を実行してみたいと思います．あとは他のデータパイプラインツールとの比較とかも出来たら楽しそうかなと思いました．\n","date":"2022-09-29","permalink":"https://masatakashiwagi.github.io/portfolio/post/data-integration-airbyte/","tags":["DEV","DATA"],"title":"Airbyte でスプレッドシートのデータを BigQuery に連携"},{"content":"はじめに 2ヶ月以上ご無沙汰になってしまいましたが，久しぶりのテックブログになります．今回はタイトルにもあるように，Github Actions と CodePipeline を使ってマージトリガーで Step Functions のパイプラインを動かす CI/CD を構築したお話になります．\n今回のモチベーションは，3つほどあります．\nML 系のモデル学習パイプラインの構築と DryRun 的なものを毎回手動で実行するのがいいかげんめんどくさくなってきた 人数が少ない ML チームだと担当者が対応できない場合に，属人化したものを代わりにオペレーションするのが大変でオペミスが発生する可能性がある この辺りはドキュメント整備やチーム内での共有といった部分を整理しておく必要があるのは理解しつつ\u0026hellip; CI/CD 周りの設定含めてもう少し知識を付けて MLOps のレベルを上げたかった ML 系プロジェクトにおいて，CI/CD 整備の優先度が低かったり，そもそもソフトウェアエンジニアに比べてこの辺りの経験や知識が豊富でないということで後回しにされがちですが，MLOps を考える上で CI/CD は大事なファクターの1つなのでしっかり取り組むべきだと思います（CI/CD の自動化は Google が定義している MLOps level 2: CI/CD pipeline automation に相当する部分）．また，少人数チームの場合は尚のこと，人手をかけられない+属人化を排除する意味でも取り入れていくのが良いかなと思います．\n以下のリポジトリにソースコードなどを置いてあります．\nCI/CD パイプラインの構成 ※ はじめに，CodePipeline の設定や IAM ロールの必要な権限は詳細に説明しないのでご了承くださいませ．公式ドキュメントや巷にある詳細な説明がされているブログをご覧下さい．\n今回のゴールは Github のブランチマージから最終的に Step Functions のパイプラインを動かすところまでになります．本来は Step Functions 内で ML のモデル学習を行うパイプラインを構築しますが，今回はサンプルとして SageMaker ProcessingJob を単発で動かすだけになります．\n今回構築した CI/CD パイプラインは以下のような感じになります．\nディレクトリ構成は以下になります．\n. ├── .github │ └── workflows │ └── sam-codepipeline.yaml ├── .gitignore ├── README.md ├── async-processing ├── cicd-pipeline │ ├── README.md │ ├── config │ │ ├── buildspec.yml │ │ └── dev-codepipeline-ver1.json │ ├── container │ │ ├── Dockerfile │ │ ├── app │ │ │ └── src │ │ │ ├── hello.py │ │ │ └── logger.py │ │ ├── docker-compose.yml │ │ ├── requirements.lock │ │ └── requirements.txt │ └── sam │ ├── env │ │ ├── dev │ │ │ ├── samconfig.toml │ │ │ └── template.yaml │ │ └── prod │ │ ├── samconfig.toml │ │ └── template.yaml │ └── statemachine │ └── sample-ml-pipelines-ver1.asl.json └── teamaya 流れを説明していくと，\nGithub Actions パート\nsam-codepipeline.yaml name: sam-stepfunctions-codepipeline on: pull_request: branches: - dev - main types: [opened] # paths: # - 'cicd-pipeline/config/dev-codepipeline-ver1.json' # - './github/workflows/sam-codepipeline.yaml' workflow_dispatch: jobs: Build-Deploy-SAM: name: Build \u0026amp; Deploy SAM for Pipeline runs-on: ubuntu-latest timeout-minutes: 5 steps: - name: Checkout uses: actions/checkout@v2 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ap-northeast-1 # samによるdev/prod環境のAWSリソース更新 - name: Build SAM \u0026amp; Deploy SAM run: | if ${{ github.base_ref == 'dev' }}; then cd sam/env/dev sam build sam deploy --fail-on-empty-changeset --no-confirm-changeset elif ${{ github.base_ref == 'main' }}; then cd sam/env/prod sam build sam deploy --fail-on-empty-changeset --no-confirm-changeset else echo \u0026quot;Invalid branch name.\u0026quot; exit 1 fi Update-CodePipeline: name: Update Codepipeline for Step Functions runs-on: ubuntu-latest timeout-minutes: 5 needs: Build-Deploy-SAM steps: - name: Checkout uses: actions/checkout@v2 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ap-northeast-1 - name: Update Codepipeline run: aws codepipeline update-pipeline --pipeline file://config/codepipeline-ver1.json Pull Reqest が Open したタイミングで Github Actions が走る AWS Serverless Application Model (SAM) の Build と Deploy を行う マージ先のブランチに応じて，切り替わるようにしています．dev マージでは dev 用のリソースを作成し，main マージでは prod 用のリソースを作成します． AWS CLI を使って CodePipeline の action configuration を更新する Step Functions の StateMachineArn が ML のモデル学習のバージョンによって変更されることがあるので，後続の CodePipeline で動かす対象の Step Functions を更新します． CodePipeline パート\nbuildspec.yml version: 0.2 env: variables: ENV: \u0026quot;dev\u0026quot; REPOSITORY_NAME: \u0026lt;ECRのリポジトリ名\u0026gt; IMAGE_TAG: \u0026quot;latest\u0026quot; REGION: \u0026quot;ap-northeast-1\u0026quot; parameter-store: AWS_ACCOUNT_ID: \u0026quot;/CodeBuild/common/AWS_ACCOUNT_ID\u0026quot; AWS_ACCESS_KEY_ID: \u0026quot;/CodeBuild/common/AWS_ACCESS_KEY_ID\u0026quot; AWS_SECRET_ACCESS_KEY: \u0026quot;/CodeBuild/common/AWS_SECRET_ACCESS_KEY\u0026quot; phases: pre_build: commands: # - echo Login to Docker # - docker login --username $AWS_ACCESS_KEY_ID --password $AWS_SECRET_ACCESS_KEY - echo Set ECR repository URI - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com - aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $REPOSITORY_URI build: commands: - echo Build started - echo Building the Docker Image - docker build -t $REPOSITORY_URI/$REPOSITORY_NAME:$IMAGE_TAG container post_build: commands: - echo Login to Amazon ECR - aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $REPOSITORY_URI - echo Pushing the Docker Image to ECR started - docker push $REPOSITORY_URI/$REPOSITORY_NAME:$IMAGE_TAG PR が dev/main にマージされたタイミングで AWS で事前に設定している CodePipeline が走る 事前に CodePipeline 上で組んでいるフローが動く CodeBuild が起動し，Docker Image の Build が行われ，Image を ECR に push します Step Functions が起動し，パイプラインが走る ECR に登録している Image を使って，SageMaker ProcessingJob が動く 細かい部分で言うと，AWS_ACCOUNT_ID, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY などの機密情報は，パラメータストア（AWS Systems Manager Parameter Store）へ登録しておき，それを参照する形で使うようにしています．\nAWS Serverless Application Model AWS Serverless Application Model (SAM) とは AWS でサーバーレスアプリケーションを簡単に構築することがフレームワークになります．CloudFormation の拡張で CloudFormation で利用できるリソースは SAM でも使用することができます．YAML もしくは JSON 形式のテンプレートを使って簡単に環境構築ができますし，CLI も提供されてます．詳しくは公式の「AWS Serverless Application Model (AWS SAM) とは」を見て頂ければと思います．\n設定するファイルは2つあります．\nsamconfig.toml template.yaml dev 環境の設定ファイルを見ていくと，\nsamconfig.toml\nsamconfig.toml version = 0.1 [default] [default.deploy.parameters] stack_name = \u0026quot;dev-sample-codepipeline\u0026quot; s3_bucket = \u0026quot;aws-sam-cli-managed-samclisourcebucket-dev-sample-codepipeline\u0026quot; s3_prefix = \u0026quot;dev-sample-codepipeline\u0026quot; region = \u0026quot;ap-northeast-1\u0026quot; confirm_changeset = true capabilities = \u0026quot;CAPABILITY_IAM\u0026quot; disable_rollback = true こちらのファイルは1つのファイルに dev と prod の両方の設定を実装することもできますが，今回は環境毎にファイルを分けています．（後述するテンプレートと一緒に管理する必要があるが，微妙に環境毎で変数が違う部分もあるのでこのファイルも分けて管理した方が良いかなと思い分けています） [default.deploy.parameters] は sam deploy コマンドが実行された時に渡される引数になります 注意としては，s3_bucket は事前に作成しておかないとデプロイした際に S3 Bucket does not exist. といったエラーが発生します． S3 には，ビルドしたテンプレートファイルとリソースファイルが保存されます template.yaml\ntemplate.yaml AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; Transform: AWS::Serverless-2016-10-31 Description: \u0026gt; Create Resource - StepFunctions - EventBridge Parameters: EnvironmentVariable: Description: 環境変数 Type: String Default: dev VersionVariable: Description: バージョン番号 Type: String Default: ver1 StepFunctionsExecutionRole: Description: Step Functionsの実行ロール Type: String Default: arn:aws:iam::\u0026lt;AWSアカウントID\u0026gt;:role/StepFunctionsExecutionRole SageMakerProcessingImage: Description: SageMakerのProcessingJobを動かすImage Type: String Default: \u0026lt;AWSアカウントID\u0026gt;.dkr.ecr.ap-northeast-1.amazonaws.com/\u0026lt;ECRのリポジトリ名\u0026gt;:latest Resources: # =======Step Functions for ProcessingJob======== # DevMLPipelinesStateMachine: Type: AWS::Serverless::StateMachine Properties: Name: !Sub ${EnvironmentVariable}-sample-ml-pipelines-${VersionVariable} DefinitionUri: ../../statemachine/sample-ml-pipelines-ver1.asl.json DefinitionSubstitutions: ProcessingJobRole: !Ref StepFunctionsExecutionRole ProcessingImage: !Ref SageMakerProcessingImage ProcessingEnvironment: !Ref EnvironmentVariable Role: !Ref StepFunctionsExecutionRole Events: Schedule: Type: Schedule Properties: Description: パイプライン用のスケジューラー Enabled: False Name: !Sub ${EnvironmentVariable}-sample-ml-pipelines-${VersionVariable} Schedule: \u0026quot;cron(0 16 * * ? *)\u0026quot; JSON ではなく，YAML 形式で書けるので良きですね！ Parameters ブロックでは，値を変数化できるので，共通設定や dev/prod で動的に変わる部分だったりを書いておくと使い回しやすいかなと思います． Resources ブロックでは，Parameters ブロックで定義した変数を !Ref や !Sub で使うことができます． !Sub は値の一部に変数を使用したい時に使うことができます． Role の設定もできますが，今回は事前に設定しておいた IAM ロールを使用しています． 今回は Step Functions だけを定義したので，定義ファイル sample-ml-pipelines-ver1.asl.json を次に見ていきます． sample-ml-pipelines-ver1.asl.json\nsample-ml-pipelines-ver1.asl.json { \u0026quot;Comment\u0026quot;: \u0026quot;Sample ML pipelines\u0026quot;, \u0026quot;StartAt\u0026quot;: \u0026quot;SageMaker-Hello-World\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;SageMaker-Hello-World\u0026quot;: { \u0026quot;Comment\u0026quot;: \u0026quot;Hello Worldを出力する\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::sagemaker:createProcessingJob.sync\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;RoleArn\u0026quot;: \u0026quot;${ProcessingJobRole}\u0026quot;, \u0026quot;ProcessingJobName.$\u0026quot;: \u0026quot;States.Format('{}', $$.Execution.Name)\u0026quot;, \u0026quot;AppSpecification\u0026quot;: { \u0026quot;ImageUri\u0026quot;: \u0026quot;${ProcessingImage}\u0026quot;, \u0026quot;ContainerEntrypoint\u0026quot;: [ \u0026quot;python3\u0026quot;, \u0026quot;/opt/program/src/hello.py\u0026quot; ] }, \u0026quot;ProcessingResources\u0026quot;: { \u0026quot;ClusterConfig\u0026quot;: { \u0026quot;InstanceCount\u0026quot;: 1, \u0026quot;InstanceType\u0026quot;: \u0026quot;ml.t3.medium\u0026quot;, \u0026quot;VolumeSizeInGB\u0026quot;: 10 } }, \u0026quot;Environment\u0026quot;: { \u0026quot;PYTHON_ENV\u0026quot;: \u0026quot;${ProcessingEnvironment}\u0026quot; }, \u0026quot;StoppingCondition\u0026quot;: { \u0026quot;MaxRuntimeInSeconds\u0026quot;: 86400 } }, \u0026quot;Catch\u0026quot;: [ { \u0026quot;ErrorEquals\u0026quot;: [ \u0026quot;States.ALL\u0026quot; ], \u0026quot;Next\u0026quot;: \u0026quot;FailState\u0026quot; } ], \u0026quot;End\u0026quot;: true }, \u0026quot;FailState\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;Fail\u0026quot;, \u0026quot;Cause\u0026quot;: \u0026quot;Error\u0026quot;, \u0026quot;Error\u0026quot;: \u0026quot;Error\u0026quot; } } } JSON 形式で書かれた Step Functions の定義ファイルになります． Hello World を出力するだけの内容になっていますが，それを SageMaker ProcessingJob で動かしています．今回は単純な処理ですが，ML モデルを構築するためのパイプラインをここに実装すれば，その内容が SAM で構築されます． パイプラインを構築する際は，Step Functions の Workflow studio で直感的な GUI で簡単に作成できるので，それで作成した後に JSON の定義ファイルをDLすれば同じものを本番環境用にサクッと構築することができます． ProcessingJob を動かす Image は CodeBuild 時に ECR に push したものを使っています． また，ProcessingJobName は一意でないとエラーになるので，Context オブジェクトの $$.Execution.Name を使用しています． AWS CodePipeline CodePipeline は複数のステージというものが用意されていて，それを繋ぎ合わせて一連のパイプラインを構築し CI/CD を自動化するものになります．詳しくは公式の「AWS CodePipeline とは」を見て頂ければと思います．\n今回の場合，Source → Build → Execute (Invoke) のような流れになっています．\nSource: Github のブランチマージトリガーで起動する Build: Docker ImageのBuild と ECR への push を行う Execute (Invoke): Step Functions を起動する 処理が正常に終了すると，Step Functions の実行結果と CloudWatch Logs のログは以下のようになります． CodePipeline を動かす時の注意としては，権限周りのエラーがよく発生するので CodePipeline から何を動かす必要があるかをチェックして動かすアクションの権限を与えてやる必要があります． 今回の場合: CodePipeline では，CodeBuild と Step Functions を動かすためのポリシーが必要 CodeBuild では，ECR と SystemsManager を操作するためのポリシーが必要 Step Functions では，SageMaker の操作と ECR へのアクセスを行うポリシーが必要 エラー周りは参考に挙げたブログが役に立ちました． おわりに 今回は Github Actions と CodePipeline を使って Step Functions を動かす CI/CD パイプラインを構築したお話でした．\nStep Functions の中身をMLモデル学習のパイプラインとすれば，Github Actions の PR が Open とブランチマージをトリガーとして，CodePipeline が走ることで，一連の流れを CI/CD で実現できることになります．\nここにテストを追加したり，例えば，ECS で ML-API が動いている場合には，Step Functions のパイプラインが正常終了した後に，承認プロセスを入れて ECS のタスク定義とサービスの更新を入れることで，デプロイまで持っていくことができるかなと思います！\nもう少し発展させることでより良い開発体験が生み出すことができるのと，MLOps の成熟度も上がって運用の自動化も一歩前進すると考えています．\n参考 CodeBuild・CodePipelineを使ってデリバリーパイプラインを導入（AWSでWebアプリ構築 part3） Solve - Policy has Prohibited field Principal AWS Error Troubleshooting AWS CodeBuild Troubleshooting CodePipeline ","date":"2022-07-18","permalink":"https://masatakashiwagi.github.io/portfolio/post/implement-cicd-github-actions-codepipeline-for-stepfunctions/","tags":["DEV","AWS","MLOPS"],"title":"Github Actions と CodePipeline を使って Step Functions を動かす CI/CD を構築"},{"content":"はじめに Continuous Machine Learning (CML) という機械学習モデルの CI ツールを個人プロジェクトで導入したので，その紹介をしようと思います．\n機械学習プロジェクトでテストを考える時に，大きく3つあると思っています．\nソフトウェアのテスト: 単体テストや結合テストなどコードが意図通りの挙動を示すかどうかを確認するテスト ツール: pytest, unittest \u0026hellip;など 機械学習モデルのテスト: 機械学習モデルが正常に動作するか，評価指標でスコアを得ることができるかなどを確認するテスト ツール: CML (with DVC) \u0026hellip;など データのテスト: データが想定しているスキーマに従っているか，データの分布や範囲が意図したデータかなどを確認するテスト ツール: dbt, Dataform \u0026hellip;など このうち今回は，「機械学習モデルのテスト」に着目してこれをどのように実施するかを紹介しています．\n※ 今回，DVC は使っていません．機械学習モデルは使用するデータに依存する部分がかなり大きいので，DVC と組み合わせて実施するのが望ましいですが，今回は簡易的なテストデータを用意して，CML を実施しています．\nContinuous Machine Learning (CML) とは？ Continuous Machine Learning (CML) は，Iterative.ai が開発している機械学習プロジェクトの CI/CD を実現する OSS であり，特徴としては以下のような点があります．\nモデルの学習と評価の結果をレポート（markdown 形式）にして自動生成できる（メトリクスや図など） Github Actions と連携して，Pull Requests 時に自動的に実行する仕組み 任意のクラウド環境で実験を実行することが可能 これは例えば，次のような課題を解決するためのサポートになると思います．\nPoC でデータサイエンティストが作成したコードをプロダクション環境に載せた際に，モデルの学習と評価が適切に行われているかをどのように確認するか？ モデルが前回から向上していることをどのように確認するか？（これについては DVC との連携も必要） モデルの再学習の際にも同じようなチェックが行われるのか？ etc\u0026hellip; CML は PR ベースでモデルの学習と評価を行い，適切な意思決定に繋げるのに役立つツールだと思います．\n個人プロジェクトへの導入 前回のブログポストのこちらの章で少し触れた部分になります．\n機械学習によるモデル作成を行っている tasks.py の if __name__ == \u0026quot;__main__\u0026quot;: 以下が CML 用に用意したコードになります．\nif __name__ == \"__main__\":以下のコードを抜粋 if __name__ == \u0026quot;__main__\u0026quot;: def plot_yy(y_valid, y_pred, metrics, savepath): \u0026quot;\u0026quot;\u0026quot;Vizualize the results using yy-plot \u0026quot;\u0026quot;\u0026quot; y_max = np.max(y_valid) y_min = np.min(y_valid) # calculate max and min of y_pred predict_y_max = np.max(y_pred) predict_y_min = np.min(y_pred) # use the smallest and largest value of either of both y_valid and y_pred # as the range of the vertical axis horizontal axis axis_max = max(y_max, predict_y_max) axis_min = min(y_min, predict_y_min) # margin of 5% of the length axis_max = axis_max + (axis_max - axis_min) * 0.05 axis_min = axis_min - (axis_max - axis_min) * 0.05 plt.figure(figsize=(10, 6)) plt.subplots_adjust(wspace=0.2, hspace=0.3) plt.scatter(y_pred, y_valid, c='r', s=50, zorder=2, edgecolors=(0, 0, 0), alpha=0.6) plt.plot([axis_min, axis_max], [axis_min, axis_max], c=\u0026quot;#1560bd\u0026quot;) plt.xlabel('Predict Values', fontsize=20) plt.ylabel('True Values', fontsize=20) plt.title(r'RMSE=%.2f' % (metrics), fontsize=15) plt.tick_params(labelsize=20) plt.tight_layout() plt.grid(True) plt.savefig(savepath, dpi=100, bbox_inches='tight', pad_inches=0.1) plt.close() def plot_residual(y_valid, y_pred, savepath): residual = y_pred - y_valid xmax = np.max(y_pred) + (np.max(y_pred) - np.min(y_pred)) * 0.05 xmin = np.min(y_pred) - (np.max(y_pred) - np.min(y_pred)) * 0.05 plt.figure(figsize=(10, 6)) plt.subplots_adjust(wspace=0.2, hspace=0.3) plt.scatter(y_pred, residual, c='r', s=50, zorder=2, edgecolors=(0, 0, 0), alpha=0.6) plt.hlines(y=0, xmin=xmin, xmax=xmax, color='#1560bd') plt.title('Residual Plot', fontsize=20) plt.xlabel('Predict Values', fontsize=20) plt.ylabel('Residuals', fontsize=20) plt.tick_params(labelsize=20) plt.tight_layout() plt.grid(True) plt.savefig(savepath, dpi=100, bbox_inches='tight', pad_inches=0.1) plt.close() params = { \u0026quot;model_id\u0026quot;: \u0026quot;sample_test\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;test_diabetes\u0026quot;, \u0026quot;features\u0026quot;: [\u0026quot;age\u0026quot;, \u0026quot;bmi\u0026quot;, \u0026quot;bp\u0026quot;, \u0026quot;s1\u0026quot;, \u0026quot;s2\u0026quot;, \u0026quot;s3\u0026quot;, \u0026quot;s4\u0026quot;, \u0026quot;s5\u0026quot;, \u0026quot;s6\u0026quot;], \u0026quot;target\u0026quot;: \u0026quot;target\u0026quot; } dataset_path = 'test/data/' + params['dataset_id'] + '.csv' df = pd.read_csv(dataset_path) result = train(df, params) rmse = result['metrics']['rmse'] # Record the metrics outfile = \u0026quot;data/metrics.txt\u0026quot; if not os.path.isdir(\u0026quot;data\u0026quot;): os.mkdir(\u0026quot;data\u0026quot;) with open(outfile, \u0026quot;w\u0026quot;) as f: f.write(\u0026quot;RMSE: \u0026quot; + f\u0026quot;{rmse:.2f}\u0026quot; + \u0026quot;\\n\u0026quot;) # Plot results y_valid = result['y_true'] y_pred = result['y_pred'] savepath_yy = \u0026quot;data/yy_plot.png\u0026quot; plot_yy(y_valid, y_pred, metrics=rmse, savepath=savepath_yy) savepath_residual = \u0026quot;data/residual_plot.png\u0026quot; plot_residual(y_valid, y_pred, savepath=savepath_residual) こちらのコードがレポートにする処理になります．metrics.txt というテキストファイルを一時的に作成し，そこにメトリクスの結果を書き込みます．また，作成した回帰モデルによる実測-予測プロットの図（Y-Y プロット）や残差プロットの図を png ファイルでこちらも一時的に保存し，レポートに出力します．\n# Record the metrics outfile = \u0026quot;data/metrics.txt\u0026quot; if not os.path.isdir(\u0026quot;data\u0026quot;): os.mkdir(\u0026quot;data\u0026quot;) with open(outfile, \u0026quot;w\u0026quot;) as f: f.write(\u0026quot;RMSE: \u0026quot; + f\u0026quot;{rmse:.2f}\u0026quot; + \u0026quot;\\n\u0026quot;) # Plot results y_valid = result['y_true'] y_pred = result['y_pred'] savepath_yy = \u0026quot;data/yy_plot.png\u0026quot; plot_yy(y_valid, y_pred, metrics=rmse, savepath=savepath_yy) savepath_residual = \u0026quot;data/residual_plot.png\u0026quot; plot_residual(y_valid, y_pred, savepath=savepath_residual) レポートに出力したいファイルを用意できたら，CML を使うために cml.yaml ファイルを .github/workflows 以下に作成します．公式のユースケースを参考にしても良いと思います．\n以下は，今回のプロジェクトで実行した CML になります．\nname: train-my-model on: push: paths: - 'async-processing/app/consumer/tasks.py' pull_request: branches: - dev jobs: train-model: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: iterative/setup-cml@v1 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.8' - name: Train model env: repo_token: ${{ secrets.GITHUB_TOKEN }} S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }} S3_PATH_NAME: ${{ secrets.S3_PATH_NAME }} S3_MODEL_PATH_NAME: ${{ secrets.S3_MODEL_PATH_NAME }} run: | cd async-processing docker compose up -d docker compose exec -T consumer python3 consumer/tasks.py # Create CML report echo \u0026quot;## Metrics\u0026quot; \u0026gt;\u0026gt; report.md cat app/data/metrics.txt \u0026gt;\u0026gt; report.md echo \u0026quot;## Plots\u0026quot; \u0026gt;\u0026gt; report.md echo \u0026quot;### YY-plot\u0026quot; \u0026gt;\u0026gt; report.md cml-publish app/data/yy_plot.png --md --title 'YY Plot' \u0026gt;\u0026gt; report.md echo \u0026quot;### Residual-plot\u0026quot; \u0026gt;\u0026gt; report.md cml-publish app/data/residual_plot.png --md --title 'Residual Plot' \u0026gt;\u0026gt; report.md cml-send-comment report.md CML が実行されるタイミングとして，tasks.py が変更された時と dev ブランチに PR が作成された時の2つを設定していますが，こちらは適宜状況に合わせるのが良いと思います．今回は単純な RandomForest のモデルなので簡単に短時間で回すことができますが，画像系のモデル学習など学習に時間もリソースもかかる場合（特にクラウド環境で CML を動かす場合），コード修正の度に実行されるのは適切でないかもしれません．\nrun パートは，docker compose up で立ち上げたコンテナ環境内で tasks.py を実行して，出力されたテキストファイルと図を markdown 形式のファイルに出力しています．ここで，echo を挟むことで header を付けたりもできます．\ncml-publish: レポートに画像を表示させるコマンド cml-send-comment: github の PR にコメントととして markdown レポートを作成するコマンド 参考: CML - Command Reference\n結果はこのような形のレポートになります．\n実際使ってみた感想 個人的に良いと思う部分と微妙だなと思う部分を挙げておきます．\n良い点 作成した学習の結果や評価を PR 上で議論できる点 認識のズレなどを議論できるかなと思います モデルの結果を見て，デプロイするかどうかなど意思決定に繋げることができる点 再現性を一定担保することができる点 微妙な点 必要なものを出力してからレポート作成する必要がある点 結果のテキストファイルや図を出力しておく必要があるので，そこが面倒だったり，何を出すかの検討も必要 どういった基準で CML を実行するか どのタイミングで CML を実行するか？（push 時？ merge 時？） 重たいモデルを実行する場合，全てのデータで学習させてテストすべきか？ etc\u0026hellip; 微妙な点として挙げた内容も一部は，利用する側で決めるべきルールやポリシーだったりするので，ここはどういった情報があれば「意思決定」をする上で判断材料となるのかを整理することでクリアになる部分かもしれないです．\nおわりに 今回は，機械学習モデルのためのテストとして，CML という機械学習プロジェクトで CI/CD を行うツールを使ってみたのでその紹介になります．\nまた，今回は使っていないですが，DVC という同じ Iterative.ai が開発しているデータのバージョン管理を行うツールを CML と組み合わせて使う方法もあるので，ここも次回実験して使ってみたいと思います．これを使うことで前回の結果との差分なども見ることができるので，より良い「CI/CD for ML」が実現できるかなと思います．\nP.S. Twitter でコメント頂いたので，追記しておきます．\n確かに，モデルの学習を Push 時や PR 時に毎回回すのは大変なので，学習時に適用するのではなく，学習済みのモデルに対して適当なサブセットのデータを用意してそれに対する推論結果をレポート出力するのが軽量で試しやすそうだなと思いました．\nプルリクコメントでモデルの評価結果を出力してくれる GitHub Action。学習まで回すのは大変そうなので、サブセットの推論結果くらいに留めた方が良さそう？ https://t.co/5QkaHvQYxt\n\u0026mdash; ken_jimmy (@ken_jimmy) May 2, 2022 参考 Continuous Machine Learning (CML) Data Version Control (DVC) masatakashiwagi/teamaya/async-processing ","date":"2022-05-01","permalink":"https://masatakashiwagi.github.io/portfolio/post/build-continuous-machine-learning/","tags":["MLOPS","ML"],"title":"CML を使った ML モデルの CI/CD"},{"content":"はじめに 最近，機械学習を使ったアプリケーションのバックエンドでどういった処理を行ってモデル作成などを行っているか気になったので，モデル作成時によく行われる非同期処理を FastAPI と RabbitMQ を用いて検証したお話になります．\n機械学習のようなモデル作成に時間がかかる場合，モデル作成を行うリクエストに対して，その情報を受け取ったというレスポンスだけを先に返し，実際の処理は非同期で行われることが多いと思います．この処理を RabbitMQ という OSS の message queuing service を用いて実施した紹介になります．\nあと個人的に RPC (Remote Procedure Call) や Publish/Subscribe の仕組みを理解したいという気持ちもありました．\n以下のリポジトリにソースコードなどを置いてあります．\n概要として，以下のような流れで処理を見ていきました．\nFastAPI に json 形式でリクエストを POST する（モデル作成するためのメッセージ情報） /trainというエンドポイントにまずはデータをPOSTする モデルの学習が行われモデルを S3 に保存する モデルの作成が完了したら，次に/predictというエンドポイントにデータをPOSTする 学習済みのモデルを S3 からロードし，POST されたデータに対して予測確率を返す Message Queuing Serviceとは？ メッセージキューは Producer と呼ばれるクライアントアプリケーションが作成したメッセージを受け取り，メッセージが溜まっていく仕組みです．Producer 側から見ると，メッセージキューにメッセージを配信します．このメッセージを処理する役割として，Consumer (Worker) と呼ばれる別のアプリケーションがあり，Consumer は Queue に接続し，処理するメッセージを受信します．処理し終わったら，返信用のメッセージをクライアント側に送信することもできます．（Queue に入れられたメッセージは，Consumer が取り出すまで保存されます）\nまた，メッセージキューには Exchange と呼ばれる機能があり，どのメッセージをどのように送るかを設定する機能もあります．（厳密には，Producer は直接 Queue に送信するのではなく，Exchange に送信することになります）\n実際にこのメッセージキューの役割を担うものを Broker と呼んだりします．サービスとして RabbitMQ や Redis などがあり，マネージドサービスでは Amazon Simple Queue Service (SQS) があります．\n参考: What is message queuing?\nRabbitMQを使った実装 今回は RabbitMQ を使って実装しました．RabbitMQ は OSS の Message Broker で動作が速く軽量で，複数のメッセージングプロトコルをサポートしています．いくつかの言語で実装可能ですが，python で扱う場合には，pika というライブラリを使うことになります．\n例えば，Celery のような分散タスクキューツールを使うことで非同期処理をより簡単に実装できますが，Celery 自体はメッセージキューを構築することはできないため，RabbitMQ や Redis のような Broker が必要になります．今回はこのあたりの pub/sub の仕組みを理解するために Celery は使わずに RabbitMQ の python ライブラリである pika を使って実装することにしました．\nRabbitMQ は docker コンテナで立ち上げていて，definitions.json という定義ファイルを事前に用意することでそのスキーマに基づいて RabbitMQ を立ち上げることができます．このファイルはコンテナ起動時に読み込まれることになります．\ndefinitions.json { \u0026quot;rabbit_version\u0026quot;: \u0026quot;3.9.14\u0026quot;, \u0026quot;rabbitmq_version\u0026quot;: \u0026quot;3.9.14\u0026quot;, \u0026quot;product_name\u0026quot;: \u0026quot;RabbitMQ\u0026quot;, \u0026quot;product_version\u0026quot;: \u0026quot;3.9.14\u0026quot;, \u0026quot;users\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;guest\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;guest\u0026quot;, \u0026quot;hashing_algorithm\u0026quot;: \u0026quot;rabbit_password_hashing_sha256\u0026quot;, \u0026quot;tags\u0026quot;: \u0026quot;administrator\u0026quot;, \u0026quot;limits\u0026quot;: {} } ], \u0026quot;vhosts\u0026quot;: [{ \u0026quot;name\u0026quot;: \u0026quot;/\u0026quot; }], \u0026quot;permissions\u0026quot;: [ { \u0026quot;user\u0026quot;: \u0026quot;guest\u0026quot;, \u0026quot;vhost\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;configure\u0026quot;: \u0026quot;.*\u0026quot;, \u0026quot;write\u0026quot;: \u0026quot;.*\u0026quot;, \u0026quot;read\u0026quot;: \u0026quot;.*\u0026quot; } ], \u0026quot;topic_permissions\u0026quot;: [], \u0026quot;parameters\u0026quot;: [], \u0026quot;global_parameters\u0026quot;: [], \u0026quot;policies\u0026quot;: [], \u0026quot;queues\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;queue.model.train\u0026quot;, \u0026quot;vhost\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;durable\u0026quot;: true, \u0026quot;auto_delete\u0026quot;: false, \u0026quot;arguments\u0026quot;: { \u0026quot;x-queue-type\u0026quot;: \u0026quot;classic\u0026quot; } }, { \u0026quot;name\u0026quot;: \u0026quot;queue.model.predict\u0026quot;, \u0026quot;vhost\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;durable\u0026quot;: true, \u0026quot;auto_delete\u0026quot;: false, \u0026quot;arguments\u0026quot;: { \u0026quot;x-queue-type\u0026quot;: \u0026quot;classic\u0026quot; } } ], \u0026quot;exchanges\u0026quot;: [], \u0026quot;bindings\u0026quot;: [] } RabbitMQには丁寧なTutorialsがあるので，それを読むと理解が進むと思います！\nシステム構成 非同期処理を行うシステムの構成は図のようになります．Producer/Broker/Consumer とコンテナを3つ用意しています．\n図の右下にある Result Stores はタスクの処理結果を保存するためのものになります．Result Stores には PostgreSQL や MySQL などのDBを使用することもできますし，Redis も使用することができます．Redis は Broker としても使用することができるので，両方を1つで担うことが可能です．今回はモデルを S3 に保存するだけとして，処理結果を DB に保存したりはしていないです．\nProducer: 機械学習タスクを行うためにメッセージをポストするコンテナ（=FastAPI） Broker: メッセージキューの役割を担うコンテナ（=RabbitMQ） Consumer: タスクを実際に実行するコンテナ Storage: モデルを保存するストレージ（=S3） docker-compose.yml は以下のような構成になります．\ndocker-compose.yml version: '3.8' # Common definition x-template: \u0026amp;template volumes: - ~/.gcp:/root/.gcp:cached - ~/.aws:/root/.aws:cached - ./app:/opt/program:cached env_file: - .env environment: TZ: Asia/Tokyo LANG: 'ja_JP.UTF-8' restart: always tty: true services: producer: # FastAPI for producer container_name: producer build: context: . ports: - 5000:5000 command: [\u0026quot;uvicorn\u0026quot;, \u0026quot;main:app\u0026quot;, \u0026quot;--reload\u0026quot;, \u0026quot;--host\u0026quot;, \u0026quot;0.0.0.0\u0026quot;, \u0026quot;--port\u0026quot;, \u0026quot;5000\u0026quot;, \u0026quot;--access-log\u0026quot;] depends_on: - rabbitmq \u0026lt;\u0026lt;: *template consumer: container_name: consumer hostname: consumer build: context: . command: [\u0026quot;python3\u0026quot;, \u0026quot;consumer/consumer.py\u0026quot;, \u0026quot;--num_threads\u0026quot;, \u0026quot;2\u0026quot;] depends_on: - rabbitmq \u0026lt;\u0026lt;: *template rabbitmq: image: rabbitmq:3.9-management container_name: rabbitmq hostname: rabbitmq restart: always volumes: # - ./app/rabbitmq/etc:/etc/rabbitmq/rabbitmq - ./app/rabbitmq/etc/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf - ./app/rabbitmq/etc/definitions.json:/etc/rabbitmq/definitions.json - ./app/rabbitmq/data:/var/lib/rabbitmq - ./app/rabbitmq/logs:/var/log/rabbitmq - ~/.aws:/root/.aws:cached ports: # AMQP protocol port - 5672:5672 # HTTP management UI - 15672:15672 environment: TZ: Asia/Tokyo LANG: 'ja_JP.UTF-8' env_file: - .env # networks: # default: # external: # name: teamaya-network-async 今回のシステムのディレクトリ構成は以下になります．少し冗長な構成になっていますが，./app/producer と ./app/consumer 配下に Producer と Consumer の処理を行うスクリプトがあります．\n. ├── Dockerfile ├── README.md ├── app │ ├── consumer │ │ ├── base.py │ │ ├── consumer.py │ │ └── tasks.py │ ├── logger.py │ ├── main.py │ ├── producer │ │ ├── base.py │ │ ├── producer.py │ │ └── schema.py │ ├── rabbitmq │ │ └── etc │ │ ├── definitions.json │ │ └── rabbitmq.conf │ └── test │ ├── __init__.py │ ├── conftest.py │ ├── data │ │ └── test_diabetes.csv │ └── unit │ ├── __init__.py │ └── test_tasks.py ├── docker-compose.yml ├── requirements.lock └── requirements.txt Producerの実装 それぞれのファイルの説明をしておくと，\nbase.py: RabbitMQ に接続するための初期化や Consumer にメッセージを送信するための処理を実装したファイル producer.py: base.py のクラスを継承して，個別のタスクに合わせて送信するメッセージの実行する API を実装したファイル schema.py: データの入出力のスキーマを定義したファイル FastAPI では入出力を Pydantic というライブラリを用いて Data validation を行います．型ヒントを利用するためのスキーマ定義になります producer.py import ast import uuid from fastapi import APIRouter from logger import get_logger from producer.base import BaseProducer, QueueNames, RepQueueNames from producer.schema import ApiSchemaPredict, ApiSchemaTrain, ProducerResult LOGGER = get_logger() router = APIRouter(prefix='', tags=[\u0026quot;producers\u0026quot;]) class ProducerTrain(BaseProducer): def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames): BaseProducer.__init__(self, queue_name, rep_queue_name) def run(self, params: ApiSchemaTrain): \u0026quot;\u0026quot;\u0026quot;Run to send message to train consumer Args: params (ApiSchemaTrain): schema for train \u0026quot;\u0026quot;\u0026quot; model_id = str(uuid.uuid4()) message = { \u0026quot;model_id\u0026quot;: model_id, \u0026quot;dataset_id\u0026quot;: params.dataset_id, \u0026quot;features\u0026quot;: params.features, \u0026quot;target\u0026quot;: params.target } # self.send_message_to_consumer(message) LOGGER.info(\u0026quot;Produce message for train.\u0026quot;) response = self.send_message_to_consumer(message) response = ast.literal_eval(response.decode()) LOGGER.info(f\u0026quot;Reply Response from consumer: {response}\u0026quot;) return ProducerResult(message=response) class ProducerPredict(BaseProducer): def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames): BaseProducer.__init__(self, queue_name, rep_queue_name) def run(self, params: ApiSchemaPredict): \u0026quot;\u0026quot;\u0026quot;Run to send message to predict consumer Args: params (ApiSchemaPredict): schema for predict \u0026quot;\u0026quot;\u0026quot; message = { \u0026quot;model_id\u0026quot;: params.model_id, \u0026quot;dataset_id\u0026quot;: params.dataset_id, \u0026quot;input_data\u0026quot;: params.input_data } LOGGER.info(\u0026quot;Produce message for predict.\u0026quot;) response = self.send_message_to_consumer(message) response = ast.literal_eval(response.decode()) LOGGER.info(f\u0026quot;Reply Response from consumer: {response}\u0026quot;) return ProducerResult(message=response) @router.post(\u0026quot;/train\u0026quot;, response_model=ProducerResult, name=\u0026quot;train\u0026quot;) async def train(params: ApiSchemaTrain) -\u0026gt; ProducerResult: \u0026quot;\u0026quot;\u0026quot;Train model\u0026quot;\u0026quot;\u0026quot; return ProducerTrain(queue_name='queue.model.train', rep_queue_name='queue.reply.train').run(params) @router.post(\u0026quot;/predict\u0026quot;, response_model=ProducerResult, name=\u0026quot;predict\u0026quot;) async def predict(params: ApiSchemaPredict) -\u0026gt; ProducerResult: \u0026quot;\u0026quot;\u0026quot;Predict model\u0026quot;\u0026quot;\u0026quot; return ProducerPredict(queue_name='queue.model.predict', rep_queue_name='queue.reply.predict').run(params) ProducerTrain と ProducerPredict はタスク実行用のメッセージを送るキューの queue_name と Consumer 側からの返信用のキューである rep_queue_name の2つを引数に取ります．実際の学習や予測処理を行う部分は Consumer 側で実装しています．\nbase.py import json import os import uuid from typing import Literal import pika from logger import get_logger LOGGER = get_logger() # Possible values as queue name QueueNames = Literal['queue.model.train', 'queue.model.predict'] RepQueueNames = Literal['queue.reply.train', 'queue.reply.predict'] class BaseProducer: def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames): self.queue_name = queue_name self.rep_queue_name = rep_queue_name self.pika_params = pika.ConnectionParameters( host=\u0026quot;rabbitmq\u0026quot;, port=os.getenv('RABBITMQ_PORT', 5672), connection_attempts=10, heartbeat=0 ) self.connection = pika.BlockingConnection(self.pika_params) self.channel = self.connection.channel() LOGGER.info('Pika connection initialized.') result = self.channel.queue_declare(queue=self.rep_queue_name, exclusive=True) self.callback_queue = result.method.queue self.channel.basic_consume(queue=self.callback_queue, on_message_callback=self.on_response, auto_ack=True) def on_response(self, ch, method, props, body): if self.corr_id == props.correlation_id: self.response = body def run(self): raise NotImplementedError() def send_message_to_consumer(self, message: dict): \u0026quot;\u0026quot;\u0026quot;Send message Args: message (dict): message info \u0026quot;\u0026quot;\u0026quot; self.response = None self.corr_id = str(uuid.uuid4()) message_json = json.dumps(message) self.channel.basic_publish( exchange=\u0026quot;\u0026quot;, routing_key=self.queue_name, body=message_json, properties=pika.BasicProperties( content_type='application/json', delivery_mode=2, # make message persistent reply_to=self.callback_queue, correlation_id=self.corr_id ) ) LOGGER.info(f\u0026quot;Sent message. [q] '{self.queue_name}' [x] Body: {message_json=}\u0026quot;) while self.response is None: self.connection.process_data_events() self.close() return self.response def close(self): self.channel.close() self.connection.close() __init__ 関数:\nRabbitMQ のサーバーと接続するために pika.BlockingConnection() で host , port などのパラメータを渡してインスタンス化を行う Consumer からの Reply 用に rep_queue_name に指定したキュー名で callback_queue を作成 basic_consume では subscribe するキューが存在すればそれを実行 send_message_to_consumer 関数:\nメッセージを json.dump し，basic_publish の body につめて Exchange に送る Consumer の実装 それぞれのファイルの説明をしておくと，\nbase.py: RabbitMQ に接続してキューにあるメッセージを受信し，処理を実行するベースファイル callback 部分は tasks.py で実装しています consumer.py: スレッド数を決める num_threads をコマンドライン引数に取り，コンテナ上ではこのファイルが実行されます tasks.py: 機械学習によるモデル作成や学習済みモデルをロードして予測を行う処理を実装したファイル callback メソッドに実行したい処理を実装します if __name__ == \u0026quot;__main__\u0026quot;: 以下には Continuous Machine Learning (CML) で利用する CT 用の処理を実装しています．（CML に関しては別でブログを書こうと思います） tasks.py import json import os import sys import traceback from typing import Any, Dict import matplotlib.pyplot as plt import numpy as np import pandas as pd import pika from logger import get_logger from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from base import BaseConsumer, EvalMetrics, QueueNames LOGGER = get_logger() S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME') S3_PATH_NAME = os.getenv('S3_PATH_NAME') S3_MODEL_PATH_NAME = os.getenv('S3_MODEL_PATH_NAME') class TrainConsumer(BaseConsumer): def __init__(self, queue_name: QueueNames): BaseConsumer.__init__(self, queue_name) def callback(self, ch, method, props, body): params = self.body2dict(body) payload = { 'status': 'TASK_RECEIVED', 'model_id': params['model_id'] } response = json.dumps(payload) ch.basic_publish( exchange='', routing_key=props.reply_to, properties=pika.BasicProperties(correlation_id=props.correlation_id), body=response ) # ch.basic_ack(delivery_tag=method.delivery_tag) self.download_from_s3(S3_BUCKET_NAME, S3_PATH_NAME, 'data/', params['dataset_id'] + '.csv') LOGGER.info(\u0026quot;Download dataset from S3.\u0026quot;) dataset_path = 'data/' + params['dataset_id'] + '.csv' df = pd.read_csv(dataset_path) LOGGER.info(\u0026quot;Read csv file and transform to dataframe.\u0026quot;) try: result = train(df, params) # save model model_path = 'data/model.pkl' self.save_model(result['model'], model_path) LOGGER.info(\u0026quot;Save trained model to local.\u0026quot;) # upload model to cloud storage model_id = params['model_id'] self.upload_to_s3(S3_BUCKET_NAME, S3_MODEL_PATH_NAME + f'{model_id}/', 'data/', 'model.pkl') LOGGER.info(\u0026quot;Upload trained model to S3.\u0026quot;) LOGGER.info(\u0026quot;TASK_COMPLETED\u0026quot;) except Exception as e: _, _, tb = sys.exc_info() LOGGER.error( f\u0026quot;Exception Error: {e} || Type: {str(type(e))} || Traceback Message: {traceback.format_tb(tb)}\u0026quot;) LOGGER.error(\u0026quot;TASK_ERROR\u0026quot;) class PredictConsumer(BaseConsumer): def __init__(self, queue_name: QueueNames): BaseConsumer.__init__(self, queue_name) def callback(self, ch, method, props, body): params = self.body2dict(body) model_id = params['model_id'] self.download_from_s3(S3_BUCKET_NAME, S3_MODEL_PATH_NAME + f'{model_id}/', 'data/', 'model.pkl') LOGGER.info(\u0026quot;Download model file from S3.\u0026quot;) model_path = 'data/model.pkl' model = self.load_model(model_path) LOGGER.info(\u0026quot;Load model for prediction.\u0026quot;) try: result = predict(model, params) payload = { 'status': 'TASK_COMPLETED', 'pred_proba': result['pred_proba'] } response = json.dumps(payload) except Exception as e: _, _, tb = sys.exc_info() LOGGER.error( f\u0026quot;Exception Error: {e} || Type: {str(type(e))} || Traceback Message: {traceback.format_tb(tb)}\u0026quot;) payload = { 'status': 'TASK_ERROR', 'pred_proba': None } response = json.dumps(payload) ch.basic_publish( exchange='', routing_key=props.reply_to, properties=pika.BasicProperties(correlation_id=props.correlation_id), body=response ) # ch.basic_ack(delivery_tag=method.delivery_tag) def train(df: pd.DataFrame, params: dict) -\u0026gt; Dict[str, Any]: \u0026quot;\u0026quot;\u0026quot;Train machine learning model (RandomForestRegressor) Args: df (pd.DataFrame): dataset for training model params (dict): parameters for training \u0026quot;\u0026quot;\u0026quot; features = params['features'] target = params['target'] X, y = df[features], df[target].values # train/test split X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42) LOGGER.info(\u0026quot;Start model training.\u0026quot;) # machine learning model: RandomForestRegressor reg_model = RandomForestRegressor(max_depth=3, random_state=42, n_estimators=100) reg_model.fit(X_train, y_train) LOGGER.info(\u0026quot;Model fit for training.\u0026quot;) # evaluate model pred = reg_model.predict(X_valid) # evaluate metrics eval_metrics = EvalMetrics() rmse = eval_metrics.rmse_score(y_valid, pred) LOGGER.info(\u0026quot;Evaluate metrics=RMSE for valid dataset : %.3f\u0026quot; % rmse) LOGGER.info(\u0026quot;Finish model training.\u0026quot;) result = { 'y_pred': pred, 'y_true': y_valid, 'metrics': {'rmse': rmse}, 'model': reg_model } return result def predict(model: object, params: dict) -\u0026gt; Dict[str, Any]: \u0026quot;\u0026quot;\u0026quot;Prediction for dataset using trained model Args: model (object): trained model params (dict): parameters for prediction Returns: float: predict probability \u0026quot;\u0026quot;\u0026quot; input_data = params['input_data'] pred_proba = model.predict(pd.DataFrame([input_data])) result = { 'pred_proba': pred_proba[0] } return result 学習パート 今回，機械学習モデルは何でもよかったので，RandomForest で回帰を行う処理にしています 学習済みモデルは S3 に保存しているので，この処理を実行する場合は .env ファイルに自身で利用している AWS のバケット情報などを載せて下さい S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME') S3_PATH_NAME = os.getenv('S3_PATH_NAME') S3_MODEL_PATH_NAME = os.getenv('S3_MODEL_PATH_NAME') モデル学習時のメタ情報も DB に残しておくのが良いと思いますが，今回はその部分は実装していないです🙏\n予測パート S3 に保存したモデルをロードして，与えらたデータに対して予測を行います モデル ID は学習時に発行された UUID をコピーして貼り付ける必要があるのですが，出力されたログから拾うのでちょっといけてないですがモックなのでご勘弁を\u0026hellip; consumer.py from concurrent.futures import ThreadPoolExecutor import click import tasks @click.command() @click.option(\u0026quot;--num_threads\u0026quot;, type=int, help='the number of threads', default=1) @click.option(\u0026quot;--max_workers\u0026quot;, type=int, help='the number of max workers', default=None) def main(num_threads: int, max_workers: int): # Consumer execution with ThreadPoolExecutor(max_workers=max_workers) as executor: for _ in range(num_threads): for task in [ tasks.TrainConsumer(queue_name='queue.model.train'), tasks.PredictConsumer(queue_name='queue.model.predict') ]: executor.submit(task.run) if __name__ == \u0026quot;__main__\u0026quot;: main() 引数に指定したスレッド数に応じて Consumer が複数立ち上がります．\n実行結果 docker compose up でコンテナを起動して，http://localhost:5000/docs にアクセスすると Swagger による表示がされます．FastAPI はデフォルトで OpenAPI を自動生成してくれ，Swagger や ReDoc で表示することができます．\nこの辺は個人的にとても便利だなと思っていて，データを簡単に GET/POST することで動作を確認することできます．\nSwagger の画面 ReDoc の画面 学習編 /train にリクエストを POST します．事前に S3 に保存したデータセット名を dataset_id に，使用する特徴量（説明変数）を features に，目的変数を target に指定します．\ncurl -X 'POST' \\ 'http://localhost:5000/train' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;features\u0026quot;: [\u0026quot;age\u0026quot;, \u0026quot;bmi\u0026quot;, \u0026quot;bp\u0026quot;, \u0026quot;s1\u0026quot;, \u0026quot;s2\u0026quot;, \u0026quot;s3\u0026quot;, \u0026quot;s4\u0026quot;, \u0026quot;s5\u0026quot;, \u0026quot;s6\u0026quot;], \u0026quot;target\u0026quot;: \u0026quot;target\u0026quot; }' 出力されるログは以下のような感じになります．\n4行目は Producer がキューに対して送信したメッセージ（Consumer に渡したい情報） 7行目（中略後1行目）は Consumer からの Reply メッセージ 最後の Consumer からのログは学習処理を実行中に出力されるログ producer | [2022-04-24 15:49:09] [ INFO] Created channel=1 producer | [2022-04-24 15:49:09] [ INFO] Pika connection initialized. producer | [2022-04-24 15:49:09] [ INFO] Produce message for train. producer | [2022-04-24 15:49:09] [ INFO] Sent message. [q] 'queue.model.train' [x] Body: message_json='{\u0026quot;model_id\u0026quot;: \u0026quot;c7632288-442d-44c5-9102-31ccda2af6b7\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;features\u0026quot;: [\u0026quot;age\u0026quot;, \u0026quot;bmi\u0026quot;, \u0026quot;bp\u0026quot;, \u0026quot;s1\u0026quot;, \u0026quot;s2\u0026quot;, \u0026quot;s3\u0026quot;, \u0026quot;s4\u0026quot;, \u0026quot;s5\u0026quot;, \u0026quot;s6\u0026quot;], \u0026quot;target\u0026quot;: \u0026quot;target\u0026quot;}' consumer | [2022-04-24 15:49:09] [ INFO] Convert message to dict type. ~中略~ producer | [2022-04-24 15:49:09] [ INFO] Reply Response from consumer: {'status': 'TASK_RECEIVED', 'model_id': 'c7632288-442d-44c5-9102-31ccda2af6b7'} producer | INFO: 172.24.0.1:57222 - \u0026quot;POST /train HTTP/1.1\u0026quot; 200 OK rabbitmq | 2022-04-24 06:49:09.367236+00:00 [info] \u0026lt;0.5900.0\u0026gt; closing AMQP connection \u0026lt;0.5900.0\u0026gt; (172.24.0.4:46266 -\u0026gt; 172.24.0.2:5672, vhost: '/', user: 'guest') consumer | [2022-04-24 15:49:09] [ INFO] Found credentials in shared credentials file: ~/.aws/credentials consumer | [2022-04-24 15:49:09] [ INFO] Download dataset from S3. consumer | [2022-04-24 15:49:09] [ INFO] Read csv file and transform to dataframe. consumer | [2022-04-24 15:49:09] [ INFO] Start model training. consumer | [2022-04-24 15:49:09] [ INFO] Model fit for training. consumer | [2022-04-24 15:49:09] [ INFO] Evaluate metrics=RMSE for valid dataset : 53.039 consumer | [2022-04-24 15:49:09] [ INFO] Finish model training. consumer | [2022-04-24 15:49:09] [ INFO] Save trained model to local. consumer | [2022-04-24 15:49:10] [ INFO] Upload trained model to S3. consumer | [2022-04-24 15:49:10] [ INFO] TASK_COMPLETED 予測編 /predict にリクエストを POST します．model_id を元に学習済みモデルを S3 からロードします．input_data には，モデルに入力するデータを辞書形式で特徴量とその値という組で渡します．\n※ model_id は学習編のログ出力にある model_id を使用する必要があります．\ncurl -X 'POST' \\ 'http://localhost:5000/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;model_id\u0026quot;: \u0026quot;c7632288-442d-44c5-9102-31ccda2af6b7\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;input_data\u0026quot;: { \u0026quot;age\u0026quot;: 0.038076, \u0026quot;bmi\u0026quot;: 0.061696, \u0026quot;bp\u0026quot;: 0.021872, \u0026quot;s1\u0026quot;: -0.044223, \u0026quot;s2\u0026quot;: -0.034821, \u0026quot;s3\u0026quot;: -0.043401, \u0026quot;s4\u0026quot;: -0.002592, \u0026quot;s5\u0026quot;: 0.019908, \u0026quot;s6\u0026quot;: -0.017646 } }' 出力されるログは以下のような感じになります．\n4行目は Producer がキューに対して送信したメッセージ（Consumer に渡したい情報） 5行目の Consumer からのログは予測処理を実行中に出力されるログ （中略後1行目）は Consumer からの Reply メッセージで，予測結果が入っています producer | [2022-04-24 18:00:16] [ INFO] Created channel=1 producer | [2022-04-24 18:00:16] [ INFO] Pika connection initialized. producer | [2022-04-24 18:00:16] [ INFO] Produce message for predict. producer | [2022-04-24 18:00:16] [ INFO] Sent message. [q] 'queue.model.predict' [x] Body: message_json='{\u0026quot;model_id\u0026quot;: \u0026quot;c7632288-442d-44c5-9102-31ccda2af6b7\u0026quot;, \u0026quot;dataset_id\u0026quot;: \u0026quot;diabetes\u0026quot;, \u0026quot;input_data\u0026quot;: {\u0026quot;age\u0026quot;: 0.038076, \u0026quot;bmi\u0026quot;: 0.061696, \u0026quot;bp\u0026quot;: 0.021872, \u0026quot;s1\u0026quot;: -0.044223, \u0026quot;s2\u0026quot;: -0.034821, \u0026quot;s3\u0026quot;: -0.043401, \u0026quot;s4\u0026quot;: -0.002592, \u0026quot;s5\u0026quot;: 0.019908, \u0026quot;s6\u0026quot;: -0.017646}}' consumer | [2022-04-24 18:00:16] [ INFO] Convert message to dict type. consumer | [2022-04-24 18:00:16] [ INFO] Download model file from S3. consumer | [2022-04-24 18:00:16] [ INFO] Load model for prediction. ~中略~ producer | [2022-04-24 18:00:16] [ INFO] Reply Response from consumer: {'status': 'TASK_COMPLETED', 'pred_proba': 208.6445780005619} rabbitmq | 2022-04-24 09:00:16.565636+00:00 [info] \u0026lt;0.8348.0\u0026gt; closing AMQP connection \u0026lt;0.8348.0\u0026gt; (172.24.0.4:46664 -\u0026gt; 172.24.0.2:5672, vhost: '/', user: 'guest') producer | INFO: 172.24.0.1:57624 - \u0026quot;POST /predict HTTP/1.1\u0026quot; 200 OK おわりに FastAPI と RabbitMQ を用いて WebAPI 形式で，機械学習タスクの非同期処理を行う検証をしました．非同期処理だったり，RPC や Pub/Sub の仕組みを少しは理解できたかなと思います．\n今回は DB にメタデータを保存したり DB 周りの処理は実装していないので，この辺も時間があれば実装できればと思います\u0026hellip;\n非同期処理を行う上でメインの役割を果たした RabbitMQ についてもコメントすると，OSS で簡単に非同期処理を行える便利な技術だと思います．Producer/Exchange/Queue/Consumer の関係性も Tutorials の図などでイメージしやすくなるので，サンプルコードを見ながら比較的容易に実装することができました．\n一方で，Consumer から Producer に Reply メッセージを送る場合に，どのように実装すればいいかが分かりづらく，個人的にはハマりポイントでした．\nあと，なにげに FastAPI もほとんど使ったことなかったので良い勉強になりました！\n最後に今後やりたいことについて列挙しておくと\u0026hellip;\nAWS SQS を使った非同期処理の実装 Celery を使った非同期処理の実装 Broker として Redis を用いた実装 DB を使ったメタデータの保存 etc\u0026hellip; 参考 FastAPI RabbitMQ RabbitMQ Tutorials Asynchronous message-based communication Deep Learning: Scaling your neural networks with containerization and a message broker katanaml/katana-skipper ","date":"2022-04-24","permalink":"https://masatakashiwagi.github.io/portfolio/post/async-ml-processing/","tags":["DEV","ML"],"title":"FastAPI と RabbitMQ を用いた機械学習タスクの非同期処理"},{"content":"はじめに 最近個人的に MLOps という領域に高い関心があって，日々情報収集をしたりしているのですが，各社が実務で取り組んでいる事例や MLOps の各領域で使用されている技術やツールなど個別の Tips が点在していて見つけるのが大変だなという印象があります．\n特に実際のユースケースが整理されていると嬉しいなーという気持ちから，じゃあ自分で整理すればいいのでは？と思い「MLOps Practices」という Website を作って公開することにしました．\nMLOps Practicesとは？ 海外には，ApplyingML や Awesome MLOps という Website/Repository がありトピック毎に各社の事例が載っていたり，色々と網羅的に整理されておりサーベイの参考になるものがあります．\nこれと似たものがあったらいいなーと思って軽く探したところ，見当たらなかったので，それだったら自分のリファレンス用も兼ねて作ってみようかなという気持ちから MLOps Practices というWebsiteを作成し，公開しました．\nKnowledge には，各社で実際に導入されていた or されている事例を整理しています．特に会社のテックブログやイベントでの登壇資料を紹介するようにしています．これは実運用されているケースを大事にしたいと思ったので，その部分にフォーカスを当てています．\nTips（こちらは未整備）には，各ツールの使い方など個人ブログなどからも収集しようと考えてますが，数が膨大になるので要検討という感じです．\nまだまだ始めたばかりで十分に整理網羅出来ていないですが，細々と続けていきたいと思います．もし，「手伝ってもいいよー！」という方が居ましたら是非お声がけ下さい！一緒に進めて行けたらと思います．\nそれ以外にも追加の事例などありましたら，Issue を作成後に PR を出して貰えたら大変嬉しいです！\nおわりに 徐々に内容を充実させて行ければと思うので，今後も定期的に更新していきたいと思います！また，一緒に更新 \u0026amp; 運用していってくれる人も募集中ですので，興味があればご連絡下さい！\nRepository にスター追加して貰えると励みになりますので，よろしくお願いします🙇\n各社のMLOpsに対する実践事例を収集して整理するRepositoryを作ってWebsiteに公開したので，良ければご利用くださいー！https://t.co/JMOQGuDtf0\n\u0026mdash; asteriam (@asteriam_fp) January 22, 2022 参考 ApplyingML Awesome MLOps MLOps Practices Website: https://masatakashiwagi.github.io/mlops-practices/ Repository: https://github.com/masatakashiwagi/mlops-practices ","date":"2022-02-05","permalink":"https://masatakashiwagi.github.io/portfolio/post/start-mlops-practices-project/","tags":["POEM","MLOPS"],"title":"各社の MLOps 事例を集めた MLOps Practices という Website を公開しました"},{"content":"はじめに Step Functions で SageMaker のリソースを特にカスタムコンテナイメージで使う場合，単純に InstanceType として \u0026ldquo;ml.g4dn.xlarge\u0026rdquo; などの GPU マシンを設定するだけでは GPU を使った学習はできなくて，使いたいDockerfile に少し手を加える必要があります．\n今回は備忘録も兼ねて Dockerfile の中身を紹介しながら，GPU 環境での動作確認をしたいと思います．\n以下の手順で動作確認を行っています．\nローカルで Dockerfile と動作確認用の Python スクリプトを作成する ローカルで build し，AWS ECR に build したイメージを push する push したイメージの URI を SageMaker Training Job の TrainingImage に指定する Step Functions を実行する CloudWatch Logs を確認する 今回の動作確認フローは以下の図のようなイメージです．\n自作の「Dockerfile.gpu」ファイル 今回は Tensorflow-gpu のベースイメージを使っています．そのイメージに「nvidia-docker」を追加でインストールすることで Step Functions で GPU 用のカスタムコンテナイメージを使って SageMaker を動作させることができます．\nTensorFlow Docker Imagesから好きな GPU 用のイメージを選択して下さい．今回は「tensorflow/tensorflow:2.6.1-gpu」を使用することにします．Optional Features にも記載されていますが，nvidia-docker が必要だよとのことなので，この通りにします．\n-gpu tags are based on Nvidia CUDA. You need nvidia-docker to run them. NOTE: GPU versions of TensorFlow 1.13 and above (this includes the latest- tags) require an NVidia driver that supports CUDA 10. See NVidia\u0026rsquo;s support matrix.\n大事な部分は以下の4行になります．以前はこちらの記事にも書かれていますが，nvidia-container-toolkit をインストールする必要があったみたいですが，今は nvidia-docker2 をインストールすることで，nvidia-container-toolkit も一緒にインストールされるみたいで，よりシンプルになっています．\nRUN distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list RUN sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-docker2 今回使用した Dockerfile 全体は以下になります．\n# Dockerfile.gpu FROM tensorflow/tensorflow:2.6.1-gpu # Set some environment variables. # PYTHONUNBUFFERED keeps Python from buffering our standard # output stream, which means that logs can be delivered to the user quickly. ENV PYTHONUNBUFFERED=TRUE # PYTHONDONTWRITEBYTECODE keeps Python from writing the .pyc files which # are unnecessary in this case. ENV PYTHONDONTWRITEBYTECODE=TRUE # DEBIAN_FRONTEND prevent from stoping docker build with tzdata ENV DEBIAN_FRONTEND=noninteractive RUN apt-get -y update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ curl \\ sudo \\ libmecab-dev \\ python3.8 \\ python3-distutils \\ python3-six \\ git \\ file \\ wget \\ \u0026amp;\u0026amp; apt-get clean \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # ここが今回のポイントで，nvidia-dockerをインストールします RUN distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list RUN sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-docker2 # 必要なライブラリーをrequirements.lockを使ってインストールします COPY requirements.lock /tmp/requirements.lock RUN python3 -m pip install -U pip \\ \u0026amp;\u0026amp; python3 -m pip install -r /tmp/requirements.lock \\ \u0026amp;\u0026amp; python3 -m pip install sagemaker \\ \u0026amp;\u0026amp; python3 -m pip install sagemaker-training \\ \u0026amp;\u0026amp; rm /tmp/requirements.lock \\ \u0026amp;\u0026amp; rm -rf /root/.cache # Timezone jst RUN ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime # Locale Japanese ENV LC_ALL=ja_JP.UTF-8 # Set up the program in the image ENV PROGRAM_DIR=/opt/program COPY app $PROGRAM_DIR WORKDIR $PROGRAM_DIR ENV PATH=\u0026quot;/opt/program:${PATH}\u0026quot; RUN chmod +x $PROGRAM_DIR/hello_gpu.py CMD [\u0026quot;python3\u0026quot;] また，SageMaker で GPU を認識しているかを確認するための Python スクリプトは以下になります．\n# hello_gpu.py import tensorflow as tf from tensorflow.python.client import device_lib def main(): # TensorflowのGPU確認 print(f'GPUs Available: {tf.test.is_gpu_available()}') print(f\u0026quot;Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\u0026quot;) print(f'{device_lib.list_local_devices()}') if __name__ == \u0026quot;__main__\u0026quot;: main() これらのファイルを用意して，ローカルで build を行います，build した後はそのイメージを ECR に push することで，Step Functions で使用することができます．\n※ ECR への push 方法や設定は今回割愛します．\nStep Functions の設定 Step Functions とは，AWS が提供する各種サービスを組み合わせたパイプラインを構築するためのワークフローサービスになります．機械学習向けのアクションも用意されていて，今回使用する SageMaker Training Job もその一つになります．\n設定は yaml ファイルのような形式で AWS のコンソール画面上で打ち込んでいきます．今回実施する内容の記述は以下の通りになります．\n{ \u0026quot;Comment\u0026quot;: \u0026quot;Check GPU env\u0026quot;, \u0026quot;StartAt\u0026quot;: \u0026quot;Hello-GPU\u0026quot;, \u0026quot;States\u0026quot;: { \u0026quot;Hello-GPU\u0026quot;: { \u0026quot;Comment\u0026quot;: \u0026quot;GPUの動作確認\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;Task\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:states:::sagemaker:createTrainingJob.sync\u0026quot;, \u0026quot;Parameters\u0026quot;: { \u0026quot;RoleArn\u0026quot;: \u0026quot;\u0026lt;SageMaker Training Jobの実行権限がアタッチされているロール\u0026gt;\u0026quot;, \u0026quot;TrainingJobName\u0026quot;: \u0026quot;sample-training-job\u0026quot;, \u0026quot;AlgorithmSpecification\u0026quot;: { \u0026quot;EnableSageMakerMetricsTimeSeries\u0026quot;: true, \u0026quot;TrainingImage\u0026quot;: \u0026quot;\u0026lt;アカウントID\u0026gt;.dkr.ecr.ap-northeast-1.amazonaws.com/sample:latest-gpu\u0026quot;, \u0026quot;TrainingInputMode\u0026quot;: \u0026quot;File\u0026quot; }, \u0026quot;EnableInterContainerTrafficEncryption\u0026quot;: true, \u0026quot;EnableManagedSpotTraining\u0026quot;: true, \u0026quot;Environment\u0026quot;: { \u0026quot;SAGEMAKER_PROGRAM\u0026quot;: \u0026quot;/opt/program/hello_gpu.py\u0026quot; }, \u0026quot;ResourceConfig\u0026quot;: { \u0026quot;InstanceCount\u0026quot;: 1, \u0026quot;InstanceType\u0026quot;: \u0026quot;ml.g4dn.xlarge\u0026quot;, \u0026quot;VolumeSizeInGB\u0026quot;: 20 }, \u0026quot;StoppingCondition\u0026quot;: { \u0026quot;MaxRuntimeInSeconds\u0026quot;: 12345, \u0026quot;MaxWaitTimeInSeconds\u0026quot;: 12345 } }, \u0026quot;End\u0026quot;: true } } } 細かい設定内容に関しては，CreateTrainingJob というドキュメントを参考下さい．\nここで，Environment（環境変数）の「SAGEMAKER_PROGRAM」について説明しておきます．この変数に指定したプログラムは Training Job のエントリーポイントにすることができます．\n元々は以下のコマンドが実行されるのですが（train.py があればそれが対象となる），実行したいプログラムのパスを指定することで任意のプログラムを実行することができます．\ndocker run \u0026lt;イメージ\u0026gt; train\nただし，実行権限を与えておく必要があるので，Dockerfile 内で RUN chmod +x $PROGRAM_DIR/hello_gpu.py としています．\nあとは，実行結果を CloudWatch Logs で確認して，以下の内容がログに出力されていれば大丈夫です．\nGPUs Available: True Num GPUs Available: 1\nおわりに 今回は，Step Functions で SageMaker の Training Job をカスタムコンテナイメージを使って GPU 環境で動かす方法を紹介しました．この方法を使えば，深層学習などの GPU 環境を必要とした学習もパイプラインに組み込むことが可能になります．また，Training Job を使った学習ができれば，実験結果は SageMaker Experiments に保存されるので，再現性を担保することもできます．\nStep Functions でカスタムコンテナイメージを使って GPU 環境で学習させたい場合には，参考にして頂ければと思います．\n参考 Available SageMaker Studio Instance Types Amazon SageMakerの料金 NVIDIA Dockerって今どうなってるの？ (20.09 版) What is AWS Step Functions? ","date":"2022-01-30","permalink":"https://masatakashiwagi.github.io/portfolio/post/aws-stepfunctions-gpu-setting-for-sagemaker-jobs/","tags":["AWS","DEV"],"title":"Step Functions で自作 Dockerfile を使って SageMaker の GPU マシンを動かす方法"},{"content":"はじめに Step Functions で SageMaker ProceesingJob を使ってカスタムコンテナを実行した際に，その実行スクリプト内で ExperimentAnalytics の API を使用していたところ，「ValueError: Must setup local AWS configuration with a region supported by SageMaker.」というエラーが発生したので，その対処方法をメモしておきます．\n結論から言うと，エラー内容にある通り「region」の指定を行うことで解決できます．\n方法としては2つあります．\n「boto_session と sagemaker_client」の「region_name」を指定する 環境変数に「AWS_DEFAULT_REGION」を設定する 2つ目の方法の Step Functions の定義ファイルに環境変数: AWS_DEFAULT_REGION を1行追記するのが簡単かと思います．\nConfiguration Error SageMaker Experiments に保存されている実験結果は sagemaker.analytics.ExperimentAnalytics の API 使うことで取得することができます．今回，Step Functions で SageMaker ProceesingJob を使ってカスタムコンテナを実行した際に，以下のエラーが発生しました．\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\nConfiguration は公式ドキュメントを見ると以下のことが書かれています．\nConfiguration - Overview Boto3 looks at various configuration locations until it finds configuration values. Boto3 adheres to the following lookup order when searching through sources for configuration values:\nA Config object that\u0026rsquo;s created and passed as the config parameter when creating a client Environment variables The ~/.aws/config file 上から順番に優先度が高いものになっていて，下位で設定している内容よりも上位で設定した内容が反映されます．\n今回の場合は，1番目と2番目は特段設定していないので，3番目が採用されて問題ないかなと思っていましたが，上述のエラーが発生しました．AWS のクレデンシャル情報（config, credentials）はローカルの ~/.aws/ 配下に置かれており，build 時にローカルにあるファイルを volumes マウントしてコンテナ内と同期しています．\nこの設定では上手くいかなかったので，1番目 or 2番目の設定を行ったところ正常に動作したので，こちらの方法を記載しておきます．もし，volumes マウントの方法で上手くいく方法があれば教えて下さい！\nConfig object を boto3 client に渡す方法 公式ドキュメントに記載されている優先度が1番高い方法になります．botocore.config.Config をインスタンス化して使うことで解決する方法になりますが，今回はわざわざ Config object を使わずに region_name を指定する方法を説明します．\nimport boto3 import sagemaker # sessionとclientの設定を行う boto_session = boto3.session.Session(region_name=\u0026quot;ap-northeast-1\u0026quot;) sagemaker_client = boto_session.client(service_name='sagemaker', region_name=\u0026quot;ap-northeast-1\u0026quot;) sagemaker_session = sagemaker.session.Session(boto_session=boto_session, sagemaker_client=sagemaker_client) # ExperimentAnalyticsをインスタンス化する trial_component_analytics = sagemaker.analytics.ExperimentAnalytics( experiment_name='sample-experiments01', sagemaker_session=sagemaker_session ) # データフレーム化 analytics_tables = trial_component_analytics.dataframe() コードの流れは以下になります．\nboto_session と sagemaker_client を作成する sagemaker.session.Session の引数にそれぞれを渡す sagemaker_session を sagemaker.analytics.ExperimentAnalytics の引数に渡す 取得した実験結果をデータフレーム化する ここで，最初の「boto_session と sagemaker_client を作成する」部分で，「boto3.session.Session の region_name」と，この session を使った「client の region_name」に該当する region を指定する必要があります．この2つをセットしておくことで，今回発生したエラーを回避することができます．\nこの場合はカスタムコンテナで実行するスクリプトの修正変更が必要になってきますが，次に説明する環境変数に渡す方法はこの辺りの修正は必要ないので，簡単かなと思います．\n環境変数を設定する方法 Step Functions のワークフローを定義する json ファイルの Environment 変数に「AWS_DEFAULT_REGION」を設定する方法になります．\nStep Functions の定義ファイルのパラメータ部分は下記のような感じです．（今回は SageMaker ProcessingJob を使って実行しています）\n\u0026quot;Parameters\u0026quot;: { \u0026quot;AppSpecification\u0026quot;: { \u0026quot;ImageUri\u0026quot;: \u0026quot;hogehoge.dkr.ecr.ap-northeast-1.amazonaws.com/mlops-experiments:latest\u0026quot;, \u0026quot;ContainerEntrypoint\u0026quot;: [ \u0026quot;python3\u0026quot;, \u0026quot;/opt/ml/code/get_experiments.py\u0026quot; ] }, \u0026quot;Environment\u0026quot;: { \u0026quot;AWS_DEFAULT_REGION\u0026quot;: \u0026quot;ap-northeast-1\u0026quot; } } スクリプトの中身は以下になります．session の設定が必要なくなります．\nimport sagemaker # ExperimentAnalyticsをインスタンス化する trial_component_analytics = sagemaker.analytics.ExperimentAnalytics( experiment_name='sample-experiments01' ) # データフレーム化 analytics_tables = trial_component_analytics.dataframe() おわりに 今回は，Step Functions で SageMaker ProceesingJob を使った際に発生したエラーの対処方法を備忘録として残したものになります．\nConfiguration の設定に優先順位があることを知ったので，この辺りは今回に限らず注意が必要だなと思いました．今回のエラーに対する対処方法は複数あるので，開発している状況に合わせて使い分けていければと思います．\nあと，個人的には実行している Step Functions の region をセットして欲しい気持ちもありますが，まーこれは状況次第なので，なんとも言えない気もします\u0026hellip;\n参考 Boto3 Docs - Configuration How to fix aws region error \u0026ldquo;ValueError: Must setup local AWS configuration with a region supported by SageMaker\u0026rdquo; ","date":"2021-12-26","permalink":"https://masatakashiwagi.github.io/portfolio/post/value-error-local-aws-configuration/","tags":["DEV","AWS"],"title":"ValueError: Must setup local AWS configuration の対処方法"},{"content":"はじめに 今回は，teamaya という個人プロジェクトで進めているデータ連携の話になります．コードは以下のリポジトリに置いてあるので，ご自由に使用下さい！\n具体的には，手元にあるスプレッドシートのデータを BigQuery の特定のテーブルに連携するまでの話になります．\n普段家計簿のデータをスプレッドシートに手入力で管理してるんですが，そのデータを BigQuery に集めて色々と検証できると良いなーという思いから，データ連携を始めました．加えて，可視化も良くしたい思いもあり，Data Studio でダッシュボードを作ったりもしています．\nデータ連携をするだけであれば，Embulk を単体実行するのでこと足りますが，今回はサーバーモードで立ち上げた Digdag UI を使ってワークフローを実行しています．スケジュール実行，ワークフロー管理や履歴管理などが UI からだとしやすく，使い心地などを知るためにも使用しました．\nまた，Docker 環境で実行できるように構成しています．Docker 化することで，簡単に別環境に持っていくことができますし，スクラップ\u0026amp;ビルドがしやすいのもあります．\n今回は以下の2種類の方法でデータを連携する方法を紹介します．内容的には既に技術記事に書かれているものが多いと思いますが，今回は Docker コンテナで各タスクが実行できるようにしているので，その辺りを参考頂けたらと思います．\nDocker コンテナ内から Embulk を直接実行して，データを転送する方法 Digdag UI からワークフローを実行して，データを転送する方法 こちらも裏側では，Embulkが実行されます． 今回のデータ連携フローのアーキテクチャーは以下のような感じです．\nアンドリュー・カーネギーの以下の名言にもあるように，機械学習エコシステムを自分で作っていくために，一歩一歩進めています！\n最も高い目標を達成するには、一歩一歩進むしかないという事実を、頭に入れておかなければならない。\nBigQuery とスプレッドシートの設定 GCP のアカウント登録方法は割愛しますが，gmail があれば簡単に登録できます．登録が完了したら，適当なプロジェクトを作成して下さい．\nGoogle Sheets API の有効化を行う 「APIとサービス」 → 「ライブラリ」と画面遷移し，検索窓に「Google Sheets API」と入力して検索すると，スプレッドシートの API を有効化できる画面に遷移するので，有効化を行います．ここで有効化しておかないと，この後スプレッドシートを使用したデータ連携が出来ないので注意下さい！\nサービスアカウントの作成 それが終わったら，サービスアカウントを作成します．「IAM と管理」 → 「サービスアカウント」へアクセスした後，必要な情報を入力し，キーの作成から JSON を選択してキーの作成を行います．そうすると，サービスアカウントの JSON ファイルがダウンロードされるので，これを ~/.gcp 配下に置いておきます．\nサービスアカウントのメールアドレスをスプレッドシートに登録 データ連携したいスプレッドシートを開き，右上の共有ボタンから「ユーザーやグループを追加」の枠にダウンロードしたサービスアカウントのメールアドレスをコピー\u0026amp;ペーストして，送信をクリックします．そうすることで，このスプレッドシートのデータを登録したサービスアカウントで転送することができます．\nここで，メールアドレスの許可をしていない場合，Embulk 実行時に以下のエラーが発生します．\nError: (ClientError) forbidden: The caller does not have permission\nBigQuery にデータセットを作成 データを格納するために，事前にデータセットを作成しておく必要があるので，データセット ID を適当に決めて，データセットの作成を行っておきます．\n1. Embulk を直接実行してデータ転送を行う場合 この方法は，Docker コンテナ内からEmbulkを直接実行して，スプレッドシートのデータをBigQueryのテーブルに転送する方法になります．\nEmbulk の細かい説明は割愛しますが，簡単に言えば，バルクデータローダーの役割として BigQuery などのデータレイク/データウェアハウスにデータ転送を行うことができます．\nデータ転送を行うために用意するものとしては，以下になります．\nGemfile Liquid ファイル Dockerfile \u0026amp; docker-compose.yml ファイル Gemfile を用意する Embulk のプラグインを Gemfile/Gemfile.lock でバージョン管理するために用意します．Embulk には，データの Input/Output のプラグインがあリ，これを使うことで様々なデータソースからターゲットにデータを連携することができます．\nInput: embulk-input-google_spreadsheets Output: embulk-output-bigquery source 'https://rubygems.org/' # No versions are specified for 'embulk' to use the gem embedded in embulk.jar. # Note that prerelease versions (e.g. \u0026quot;0.9.0.beta\u0026quot;) do not match the statement. # Specify the exact prerelease version (like '= 0.9.0.beta') for prereleases. gem 'embulk' # input spreadsheets plugin gem 'embulk-input-google_spreadsheets' # ouput bigquery plugin gem 'embulk-output-bigquery' gem 'tzinfo-data' Liquid ファイルを作成する Embulk の設定ファイルとして Liquid ファイルを作成します．YAML ファイルに設定を記述することもできますが，以下のメリットで Liquid ファイルを使用しています．\n変数を設定することができる 同じ設定内容を共通ファイルとして使うことができる etc\u0026hellip; BigQuery とスプレッドシートの情報は，.env ファイルを作成して，環境変数として管理しています．これらの変数を Liquid ファイルで使用しています．\nin: type: google_spreadsheets auth_method: service_account {% comment %} GCPのサービスアカウントのJSONファイルパス {% endcomment %} json_keyfile: {{ env.GCP_SERVICE_JSON }} {% comment %} スプレッドシートのURL {% endcomment %} spreadsheets_url: {{ env.SPREADSHEETS_TABLE }} default_timezone: 'Asia/Tokyo' {% comment %} スプレッドシートのワークシートタイトル {% endcomment %} worksheet_title: year_purchase_amount_2019 {% comment %} headerを指定している場合は2行目からとなる {% endcomment %} start_row: 2 {% comment %} カラム名と型を指定する {% endcomment %} columns: - {name: id, type: long} - {name: date, type: timestamp, format: '%Y/%m/%d', timezone: 'Asia/Tokyo'} - {name: category, type: string} - {name: purchaser, type: string} - {name: purchase_amount, type: long} - {name: memo, type: string} out: type: bigquery mode: replace auth_method: service_account json_keyfile: {{ env.GCP_SERVICE_JSON }} {% comment %} BigQueryのプロジェクト名 {% endcomment %} project: {{ env.BIGQUERY_PROJECT }} {% comment %} BigQueryのデータセット名 {% endcomment %} dataset: {{ env.BIGQUERY_PURCHASE_AMOUNT_DATASET }} {% comment %} BigQueryのテーブル名 {% endcomment %} table: daily_purchase_amount auto_create_table: true source_format: NEWLINE_DELIMITED_JSON default_timezone: 'Asia/Tokyo' default_timestamp_format: '%Y-%m-%d' formatter: {type: jsonl} encoders: - {type: gzip} retries: 3 env ファイルの説明も補足でしておきます．適宜設定している環境に合わせて修正します．\nSPREADSHEETS_TABLE=\u0026lt;該当するスプレッドシートのURL: https://docs.google.com/spreadsheets/d/hogehoge\u0026gt; BIGQUERY_PROJECT=\u0026lt;BigQueryのプロジェクト名\u0026gt; BIGQUERY_PURCHASE_AMOUNT_DATASET=\u0026lt;BigQueryのデータセット名\u0026gt; GCP_SERVICE_JSON=/root/.gcp/hoge.json Dockerfile \u0026amp; docker-compose.yml ファイルを作成する 今回は docker 環境から実行するので，Dockerfile と docker-compose.yml ファイルを作成します（後ほど使う Digdag の内容も記載されています）．\nFROM openjdk:8-alpine LABEL MAINTAINER=masatakashiwagi ENV DIGDAG_VERSION=\u0026quot;0.9.42\u0026quot; ENV EMBULK_VERSION=\u0026quot;0.9.23\u0026quot; RUN apk --update add --virtual build-dependencies \\ curl \\ tzdata \\ coreutils \\ bash \\ \u0026amp;\u0026amp; curl --create-dirs -o /bin/digdag -L \u0026quot;https://dl.digdag.io/digdag-${DIGDAG_VERSION}\u0026quot; \\ \u0026amp;\u0026amp; curl --create-dirs -o /bin/embulk -L \u0026quot;https://dl.embulk.org/embulk-$EMBULK_VERSION.jar\u0026quot; \\ \u0026amp;\u0026amp; chmod +x /bin/digdag \\ \u0026amp;\u0026amp; chmod +x /bin/embulk \\ \u0026amp;\u0026amp; cp /usr/share/zoneinfo/Asia/Tokyo /etc/localtime \\ \u0026amp;\u0026amp; apk del build-dependencies --purge ENV PATH=\u0026quot;$PATH:/bin\u0026quot; # Install libc6-compat for Embulk Plugins to use JNI # cf: https://github.com/jruby/jruby/wiki/JRuby-on-Alpine-Linux # https://github.com/classmethod/docker-embulk RUN apk --update add libc6-compat # Copy Embulk configuration COPY ./embulk/task /opt/workflow/embulk/task # Make bundle WORKDIR /opt/workflow/embulk RUN embulk mkbundle bundle # Copy Gemfile file # This is the workaround, because jruby directory is not created COPY ./embulk/bundle/Gemfile /opt/workflow/embulk/bundle COPY ./embulk/bundle/Gemfile.lock /opt/workflow/embulk/bundle WORKDIR /opt/workflow/embulk/bundle # Install Embulk Plugins RUN embulk bundle # Set up Digdag Server COPY ./digdag /opt/workflow/digdag # ADD https://github.com/ufoscout/docker-compose-wait/releases/download/2.9.0/wait /bin/wait # RUN chmod +x /bin/wait WORKDIR /opt/workflow CMD [\u0026quot;tail\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/dev/null\u0026quot;] bundle 辺りでまわりくどいやり方をしていますが，RUN embulk bundle を実行した際に上手くインストールできなかったため，ワークアラウンドとして，mkbundle した後に Gemfile/Gemfile.lock を docker コンテナ内に COPY した上で，RUN embulk bundle を行っています．\nEmbulk を docker コンテナで実行する方法 では，実際に Embulk の実行を行うために，まずは Embulk のコンテナサービスを立ち上げます．\n# コンテナの立ち上げ docker compose up -d embulk バックグラウンドでコンテナを起動しておきます．その後，dry-run を行うために preview コマンドを実行します．\n# dry-run docker exec embulk sh /bin/embulk preview -b embulk/bundle embulk/task/spreadsheet/export_hab_purchase_amount.yml.liquid dry-run 実行後の結果を載せておきます．\ndry-run が大丈夫だった場合，本番実行を行います．本番は run コマンドを実行します．\n# production-run docker exec embulk sh /bin/embulk run -b embulk/bundle embulk/task/spreadsheet/export_hab_purchase_amount.yml.liquid 本番実行が上手くいくと BigQuery にデータが入っていることを確認できます．\n2. Digdag UI からワークフローを実行してデータ転送を行う場合 次に説明するこちらの方法は，digdag server を立ち上げて，UI 上からワークフローを実行して，スプレッドシートのデータを BigQuery のテーブルに転送する方法になります（裏で Embulk が動いています）．\nDigdag の細かい説明は割愛しますが，簡単に言えば，設定ファイルでバッチのワークフロー実行を定義・管理できるワークフローエンジンになります．スケジュール実行や失敗時の通知などを行うことができます．\nDigdag のワークフローからデータ転送を行うために用意するものとしては，以下になります．\ndig ファイル serverのproperties ファイル docker-compose.yml ファイル 上記に加えて，1で説明した Embulk 実行に必要なファイルも用意します．\ndig ファイルを用意する 個別のワークフローを単発実行する場合は，digdag run hoge.dig で良いのですが，今回はUIから実行したいので，厳密には dig ファイルの中身が要ります．記述する内容としては，ワークフローで実行していくタスクをコードに落としていきます．\nDigdag では，エラーの場合や成功した場合に，どういった処理をするのかを書くことができるので，例えば，それぞれの場合で slack に通知を行えたりもできます（今回は slack 通知は実装できていませんが，今後実装していきたいです！）．リトライ回数の設定やスケジュール実行の設定もここでできます．\n+task: _retry: 1 sh\u0026gt;: /bin/embulk run -b /opt/workflow/embulk/bundle /opt/workflow/embulk/task/spreadsheet/export_hab_purchase_amount.yml.liquid _error: echo\u0026gt;: workflow error... +success: echo\u0026gt;: workflow success! 今回は以下のワークフローになっています．\ntask: リトライ回数: 1回 Embulk の実行 エラーの場合は workflow error... を echo する success: workflow success! を echo する success は task が正常に終了した場合に，実行されることになります．\nserver の properties ファイルを用意する サーバーモードで起動するために，引数にオプションを指定する必要があるのですが，それらの設定を server.properties ファイルに集約しています．設定内容は色々とあるので詳しくは公式ドキュメント: server-mode-commandsを参照下さい．\nこのファイルには，サーバーの情報やデータベースの情報を記載しています．\nserver.bind = 0.0.0.0 server.port = 65432 server.admin.bind = 0.0.0.0 server.admin.port = 65433 server.access-log.pattern = json server.access-log.path = /var/log/digdag/access_logs log-server.type = local # database情報 # database.type = memory database.type = postgresql database.user = digdag database.password = digdag database.host = postgres database.port = 5432 database.database = digdag database.maximumPoolSize = 32 サーバーモードで起動すると，ワークフローの情報を保存するために，データベースの設定が必要になってきます．今回は PostgreSQL を別コンテナで立てて，Digdag コンテナと接続することにしています．ちなみに，これらの情報をインメモリで保存することもできます（この場合は，database.type = memory として下さい）．\ndocker-compose.yml ファイルを作成する 色々と試して上手くいかなかったのですが，最終的には以下の内容で落ち着きました．service 共通の定義は x-template にまとめています．service は3つありますが，Digdag を使う場合は digdag と postgres のみを立ち上げて使います．\ndocker の depends_on は依存関係（起動順序）を指定できますが，DB 起動後のアプリ起動までを制御できるわけではないので，postgres の起動完了前に digdag がアクセスしてしまい起動失敗する事があります．\nControl startup and shutdown order in Compose このため，condition: service_started と設定することで，postgres が起動後に digdag が立ち上がるようにしています．\ndepends_on: postgres: condition: service_started また，tty を true に設定しているのは，コンテナが正常終了して止まらないようにするためになります．\nversion: '3.8' # Common definition x-template: \u0026amp;template volumes: - ~/.gcp:/root/.gcp:cached - /tmp:/tmp env_file: - .env services: digdag: container_name: digdag build: . tty: true ports: - 65432:65432 - 65433:65433 volumes: - /var/run/docker.sock:/var/run/docker.sock command: [\u0026quot;java\u0026quot;, \u0026quot;-jar\u0026quot;, \u0026quot;/bin/digdag\u0026quot;, \u0026quot;server\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;digdag/server.properties\u0026quot;, \u0026quot;--log\u0026quot;, \u0026quot;/var/log/digdag/digdag_server.log\u0026quot;, \u0026quot;--task-log\u0026quot;, \u0026quot;/var/log/digdag/task_logs\u0026quot;] depends_on: postgres: condition: service_started \u0026lt;\u0026lt;: *template postgres: image: postgres:13.1-alpine container_name: postgres ports: - 5432:5432 environment: POSTGRES_DB: digdag POSTGRES_USER: digdag POSTGRES_PASSWORD: digdag volumes: - /tmp/data:/var/lib/postgresql/data tty: true \u0026lt;\u0026lt;: *template embulk: container_name: embulk build: . \u0026lt;\u0026lt;: *template networks: default: external: name: teamaya Digdag を docker コンテナで実行する方法 では，Digdag UI を使うために Digdag をサーバーモードで起動します．\n# コンテナの立ち上げ docker compose up -d digdag バックグラウンドでコンテナを起動しておきます．docker ps コマンドでコンテナが立ち上がっていることを確認したら，http://localhost:65432/ で UI にアクセスします．以下の画面が表示されたら OK です．\nNew project から Name を設定したら，Add file をクリックして，dig ファイルのコードをコピー\u0026amp;ペーストします．貼り付けたら，Save で内容を保存します．そしたら，Workflows のタブを選択し，先ほど追加したワークフローが表示されているのでクリックします．右上の Run ボタンを押して実行が完了すると，下図のような結果になります．\nStatus が Success になっていれば，正常終了で BigQuery にデータが入っていると思うので，確認してみて下さい．\nおわりに 今回は，手元にあるスプレッドシートのデータを Digdag/Embulk を用いて BigQuery に連携するを紹介しました．\nやっぱり，UI で直感的に状況や情報がわかるのはメリットだなと思います．複数のワークフローが動作したりする環境だとそれらも管理できるので良いと思います．スケジュール実行やエラーや成功時の通知設定なども出来るので活用したいと思います．\nまた今回は，データ連携に Digdag を用いましたが，他にも Apache Airflow やそのマネージドサービスである Cloud Composer を使っても同等のことができると思います．この辺りも別途試していきたいと思います．\n個人的な次のステップとしては，Dataform や dbt を使った「データの品質管理」に興味があるので，データを Transform したり，テストしたりして考えていきたいです．また，データレイク/データウェアハウス/データマートの設計などを併せて学習していきたいと思います！\nこれとは別軸で機械学習パイプラインの設計もしていこうかなと考えています！\n参考 Digdag 入門 EmbulkとDigdagとデータ分析基盤と digdagを DockerizeしてECS上で運用することにしました - 雑なメモ Docker Compose の depends_on の使い方まとめ DockerのTTYって何? ","date":"2021-12-22","permalink":"https://masatakashiwagi.github.io/portfolio/post/integrate-spreadsheets-to-bigquery-with-digdag/","tags":["DEV","DATA"],"title":"スプレッドシートから BigQuery へ Digdag を使ったデータ連携"},{"content":"はじめに 最近，データエンジニアリングと MLOps の領域への関心が個人的に高まっていて，何か個人プロジェクトで出来ないかと考えていたところ，普段スプレッドシートで記録している家計簿のデータを使って，データエンジニアリングとMLOpsの技術検証をしようと言う考えに至りました．\nそもそも，なんで上記領域への関心が高まってきたかというと，以前までは，ある問題設定に対して，どういったアプローチでその問題を解くかといったモデリング部分に興味があり，特徴量エンジニアリングや新しいアルゴリズムを試すといった部分が楽しいと感じていました．一方で最近は，そこで作ったモデルはそのままでは機能せず，何らかのシステムに載せて動かし続けることで真価を発揮すると思っていますし，動かし続けて使っていかないと意味がなく，せっかく作ったモデルがもったいないなーという気持ちがあります．\n機械学習モデルのシステム化のためには，どういったことを知っておくといいのかを考えた場合，データ連携を含めたデータを貯める部分とモデル作成後の MLOps 部分を理解しておくことが大事だと思うので，ここをより深めて行きたいと思った次第です．\nデータ連携周りは自分自身でモデルを作る際にも，「簡単にデータを取り出せる，品質が保たれているデータ」というのは大事な部分であり，世の中的にもデータを活用する流れはこれからも進んでいき，データ基盤を作れる人の価値は上がっていくと思うので，じゃあそこを自分でも出来るようになっておこうという思いがあります．\nもう一つのMLOpsは構成要素が色々あり技術的にも興味深い（サービス名とかは知ってるけど，どうやって使うの？的な部分もあります）のと，まだまだこの分野の知見は日本でも多くないなという印象なので，しっかり理解しておくと今後より活きてくるかなと思っています．\nそういった流れの中で，理解も含めて個人プロジェクトでできることをしたいというお気持ちがです．\n技術スタックとしては，普段 BigQuery 以外は業務でもあまり触れる機会がない Google Cloud Platform (GCP) のサービスと OSS を組み合わせてできることをしようと考えています．\nプロジェクト名: teamaya プロジェクトを始めるにあたりプロジェクト名を決めるとワクワクするので，最初に決めることにしました．沖縄好きなので，沖縄の言葉を組み合わせて何か名前を付けたいと考えていて，沖縄県と東京都のハーフである妻にも相談しながら今回の「teamaya」と言う名前にしました．\ngithub のリポジトリは以下になります．（スター貰えると泣いて喜びます😂）\nteamaya は \u0026ldquo;team\u0026rdquo; と \u0026ldquo;maya\u0026rdquo; を組み合わせた造語です．team は「〔動物に引かせて～を〕運ぶ、運搬する」という意味があり，maya（マヤー）は沖縄の方言で猫という意味があるので，猫にデータを運んで貰うという意味を込めてこの名前にしました．\nちなみに，リポジトリにあるアイコンのハイビスカス🌺を付けた猫😸は妻に書いて貰いました！\nどういったプロジェクトなのか 今のところ考えているのは，大きく2つになります．\nデータ連携 ローカルのデータをクラウド上に連携 連携したデータをELTツールで変換 \u0026amp; データの品質管理 データの可視化 機械学習システムの構築 パイプライン構築 実験管理 モニタリング \u0026amp; 通知 まず，データ連携については．ローカルからクラウドへの連携，特に特定のデータソースから BigQuery に連携する部分を実施して，データウェアハウス（DWH）を作ろうと思っています．そこからELTツールでデータマート（DM）を作成し，BI ツール（Data Studio など）でデータの可視化を行うといった流れを検証しようと考えています．\n個人的には，Dataform や dbt といったサービスでELTパイプラインを作って，データ変換やデータの品質管理を一通り試してみたいと思っています．\n機械学習システムの構築に関しては，機械学習モデル作成のためのパイプラインや実験管理などを行いつつ，コードのテストやデータ・予測結果のモニタリングなどをできるようにしたいのと，Feast などのfeature store を試したりもしたいなと思っています．\n現状では上記2つを構築していきたいと思ってますが，進めていく中で興味が湧いたものを適宜取り入れていきたいなと思っています．\nおわりに 今回はプロジェクトを新しく始めたので，その紹介になります．ローカルからクラウドにデータ連携する部分については，既に作成しているので別途内容を紹介したいと思います．\n試行錯誤しながら面白そうなものは色々と取り入れて進めて行きたいと思うので，もし面白いツールなどがあれば Twitter で教えて頂ければ嬉しいです．\n参考 MLOps.toys ","date":"2021-12-12","permalink":"https://masatakashiwagi.github.io/portfolio/post/personal-project-teamaya/","tags":["DEV","POEM"],"title":"teamaya という個人プロジェクトをはじめてみた"},{"content":"はじめに Buy Me a Coffee というコーヒー1杯をクリエイター活動のサポートという形で寄付するサービスがあります．このサービスの存在は以前から他のエンジニアの個人ブログなどで知っていたのですが，@hurutoriya さんが投稿されたこちらの記事を見て，僕も同意することが多かったので，自分の個人ブログでも導入してみたというお話です．\nクリエイターの活動をサポートするサービス Buy Me a Coffee 以外にも最近は色々とクリエイターの活動をサポートする寄付サービスがあるので紹介しておきます．\nBuy Me a Coffee: コーヒー1杯（実際には$5）をクリエイター活動のためにサポートするサービス Ko-fi: 名前の通り，サポートしたい人に対してコーヒー代を送るというサービス OFUSE: 1文字2円で OFUSE レターや OFUSE コメントを書いてサポートするというサービス Buy Me a Coffee を導入しようと思った理由 hurutoriya さんが書かれた内容と概ね一緒ですが，お金が欲しいからというわけではなく（お金が欲しいなら広告を貼ると思います），感謝を伝える1つの手段として，コーヒーを1杯プレゼントするよーという表現が良いなと感じています．\nいいね！なども記事を書いた側からすると読んでくれた人からの嬉しいリアクションの1つだと思います．ただ，いいね！以上に読んだ人にとって価値がある技術記事などに対しては，このようなサポートで感謝を伝える方法があってもいいのかなと思います．サポートして貰った側からすると，誰かのためになってるという感覚をより感じることができますし，何よりもっと有益な内容を届けていこうというモチベーションにも繋がるなと思います．\nこれらの理由と僕もより良い記事を届けていきたいという想いから，今回 Buy Me a Coffee を導入してみようと思いました．\nおわりに 実装については，ブランドサイトからロゴやグラフィックがダウンロードすることができます．また，挿入する文字やそのフォント・色など自由にカスタマイズできる Generate ページがあり，そこから Generate を実行すると，HTML に使用できる image code が出来上がるので，それをコピーして簡単にサイトに導入することができます．\n今回は，Buy Me a Coffee のサポートセクションをページ最下部に導入して，その導入したお気持ちを書いたポエムになります．\n参考 投げ銭サービスのBuy me a cofee をBlog に導入してみた コーヒー1杯で支援するサービス「ko-fi」と、開発者がコーヒーを奢られる仕組みの話☕ さまざまな収益化機能をひとつに。クリエイター応援プラットフォーム「OFUSE（オフセ）」誕生。 ","date":"2021-12-07","permalink":"https://masatakashiwagi.github.io/portfolio/post/implement-buy-me-a-coffee-option/","tags":["POEM"],"title":"Buy Me a Coffee をブログに追加した話"},{"content":"はじめに 普段は Python の SciPy というライブラリを用いて，AB テスト実施後の2群間における有意差を調べるために検定を行っていますが，Python を使っていない or このようなライブラリに触れたことがない人でも簡単に検定が行えるようにスプレッドシートを使って Mann-Whitney の U 検定を実施したものになります．\n公開中のスプレッドシート → Mann-Whitney U-test in spreadsheet\nコピーしてご自由にお使い下さい（全自動でない部分があるので，ご留意下さい）．\nMann-Whitney の U 検定とは Mann-Whitney（マン・ホイットニー）の U 検定（ウィルコクソンの順位和検定）とは，2つの母集団が特定の分布であることを仮定しないで，「2つの分布の重なり具合」を検定します．（ノンパラメトリック方法の一つ）\nこれは，2つの母集団の中央値の差に注目しています． → こちらの記事を見ると，中央値の検定というわけではないみたいです．（自分も誤って理解していました）\nU 検定の特徴としては，外れ値の影響を受けにくいなどが挙げられます．一方でよく使われる t 検定の場合，平均値を見ているので外れ値があるとその影響を受けます．そのため，外れ値除去などの対処が必要なケースが発生します．\nここで，U 検定には以下の仮定があります．\n2つの母集団は互いに独立 2つの母集団の分布が正規分布であると仮定できない 2つの母集団のサンプルサイズが同数でなくても良い また，帰無仮説は以下になります．\n帰無仮説: 2群間に差がない（2つの分布が等しい） 検定を行う手順 検定を行う手順を紹介します．まずA群とB群の2つの群を考えます．\nそれぞれのサンプルサイズを n1, n2 とした場合に，2つの群を混ぜたデータ (n1+n2) を用意します． 1のデータを昇順に並び替えます． 並び替えたものに対して，順位を割り当てます（ランク付け）．もし同順位を持つ要素が存在する場合は，順位の平均を計算し，その順位の平均を各要素に割り当てます． A 群に属するサンプルの順位和を計算する（=R1） 同様に B 群に属するサンプルの順位和を計算する（=R2） ここまで計算すると，検定統計量（U値）は以下になります．\n$$ U_1 = n_1n_2 + \\frac{n_1(n_1 + 1)}{2} - R_1 $$ $$ U_2 = n_2n_1 + \\frac{n_2(n_2 + 1)}{2} - R_2 $$ $$ U = min(U_1, U_2) $$\nU が計算できたら，Mann-Whitney 検定表を用いて有意差5%で棄却できるかどうかを確認します．\nα=0.05 の表を眺めて，今回のサンプルサイズ n1, n2 に該当する値と計算した U 値の大小関係を比較して，計算した値が小さい場合には，帰無仮説を棄却します，つまり有意差ありとなります．逆に計算した値の方が大きい場合には，帰無仮説を棄却できないので，有意差なしとなります．\nここで，サンプルサイズが n1\u0026gt;20 または n2\u0026gt;20 の時は，検定統計量 U を標準化してz値を求めて，標準正規分布で近似する方法を用います．平均値，標準偏差，z値は以下の計算式で求めます．\n$$ \\mu_u = \\frac{n_1n_2}{2} $$ $$ \\sigma_u = \\sqrt{\\frac{n_1n_2(n_1+n_2+1)}{12}} $$ $$ z = \\frac{U - \\mu_u}{\\sigma_u} $$\nz 値が計算できたら，標準正規分布表を用いて，該当するp値を見に行きます．\n(z 値をスプレッドシートの組み込み関数である NORMSDIST に代入して，1から引くことで p 値を計算しています: 式=1-NORMSDIST(z))\np≧α の時，帰無仮説を棄却できない p\u0026lt;α の時，帰無仮説を棄却する．つまり，有意差ありとなる スプレッドシートで U 検定を行う 公開しているスプレッドシートはこちらになります．（再掲）\nサンプルデータとしてグループ AB の身長のデータを載せています．こちらのデータを検定したい2群のデータに適宜変更して頂くと，#検定を行う手順 で紹介した方法に則って p 値の計算がされます．\nサンプルサイズが n\u0026lt;=20 の場合は，U値での評価になるので，その場合はリンクにあるマン・ホイットニーの U 検定表を用いて，該当する値から検定結果を見積って貰うと良いです．\n※ 補足:\nサンプルデータのデータ順序は意識しないで問題ありません，順序並び替えで自動的に昇順で並び変わります．ただし，順位については，スプレッドシートのマウスオーバーでセルの右下に表示される黒い部分をデータが存在している部分まで下にズラして貰う必要があります．（スプレッドシートを完璧に使いこなしているわけではないので，特に順位をつけてる部分が自動化できていないです．もしご存知の方は，方法を教えて貰えると大変助かります\u0026#x1f64f;）\nおわりに 今回は，スプレッドシートを使って Mann-Whitney の U 検定を試してみた内容になります．Python を使っていない非エンジニアの方でも検定を行えるようにスプレッドシートに実装しました．AB テストを実施して検定するまでを誰でも簡単にできるようになれば良いなと思ってます．実装していて，スプレッドシートって意外と組み込みの関数が用意されていることを改めて知ることができました\u0026#x1f604;\nまた，普段ライブラリを何気なく使っていますが，内部の計算方法やどうゆう手続きで出力されるのか，またその値をちゃんと理解して使っていかないとなーということを改めて感じました．例えば，scipy.stats.mannwhitneyu は p 値を出すだけであれば良いが，U 値を使いたい場合には少し使いづらいと感じました．\nP.S. t 検定についてもスプレッドシートで実施できるようにしているので，また紹介したいと思います．\n参考 Mann–Whitney U test Mann-Whitney Table 標準正規分布表 スプレッドシート - NORMSDIST Divine, et al. (2018) Mann-Whitney 検定は中央値の検定ではない ","date":"2021-11-16","permalink":"https://masatakashiwagi.github.io/portfolio/post/mann-whitney-utest-in-spreadsheet/","tags":["DS","STATS"],"title":"スプレッドシートで行う Mann-Whitney の U 検定"},{"content":"はじめに AWS の SageMaker 上で SageMaker Python SDK を使用して独自の機械学習モデルを作成することができますが，その際に学習や評価が行える Estimator という SageMaker の interface があります．\n一方で，SageMaker Experiments で実験管理を行いたい場合には，この Estimator に色々と渡してあげる必要があります．\nその中でも学習時に出力される loss の値や評価メトリクスを記録するためには，Estimator の metric_definitions に正規表現を記述してログから上手く取得する必要があります．\nこれをより簡単にするために，CustomCallback 関数を作成した話になります．\nCallback 関数のカスタマイズ Tensorflow，厳密には Keras の Callback 関数をカスタマイズします．tf.keras.callbacks.Callback クラスを継承した CustomCallBack(tf.keras.callbacks.Callback) クラスを作成します．この作成したクラスを model.fit 時に引数の callbacks に渡してやることで使用することができます．\n今回は SageMaker Experiments で使うことを想定したもので，Estimator の metric_definitions に渡す Regex として，以下のようなログが出力されて欲しいとします．（メトリクスは RMSE とした場合を想定） MetricDefinitions はこちらが参考になります → Define Metrics\nsagemaker.estimator.Estimator( ..., metric_definitions={ {'Name': 'Train Loss', 'Regex': 'train_loss: (.*?);'}, {'Name': 'Validation Loss', 'Regex': 'val_loss: (.*?);'}, {'Name': 'Train Metrics', 'Regex': 'train_root_mean_squared_error: (.*?);'}, {'Name': 'Validation Metrics', 'Regex': 'val_root_mean_squared_error: (.*?);'}, } ) しかしながら，Keras でモデルを作成する際のデフォルトでは，学習時の Loss は「loss」，メトリクスは「root_mean_squared_error」で prefix が無い状態になります．これを Callback 関数をカスタマイズすることで prefix に「train_」を付けて，Regex で簡単に取得したいという気持ちです．\nTensorflow 公式ドキュメントの Writing your own callbacks や tf.keras.callbacks.Callback を参考に作成しました．\nimport tensorflow as tf class CustomCallback(tf.keras.callbacks.Callback): def on_train_begin(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of training. \u0026quot;\u0026quot;\u0026quot; def on_train_end(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of training. \u0026quot;\u0026quot;\u0026quot; def on_epoch_begin(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the start of an epoch. \u0026quot;\u0026quot;\u0026quot; def on_epoch_end(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of an epoch. \u0026quot;\u0026quot;\u0026quot; def on_train_batch_begin(self, batch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of a training batch in fit methods. \u0026quot;\u0026quot;\u0026quot; def on_train_batch_end(self, batch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of a training batch in fit methods. \u0026quot;\u0026quot;\u0026quot; 上記コードの中で必要なものを修正すれば大丈夫です．今回は学習の開始終了とエポックの終了時に呼ぶように修正しました．\nimport tensorflow as tf import datetime class CustomCallBack(tf.keras.callbacks.Callback): def on_train_begin(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of training. \u0026quot;\u0026quot;\u0026quot; print(f\u0026quot;Start training - {str(datetime.datetime.now())}\u0026quot;) # get parameters self.epochs = self.params['epochs'] # the epoch when training is stopped self.stopped_epoch = 0 # initialize the best loss as infinity self.best_loss = np.Inf # list of best metrics values self.best_metrics_values_list = [] def on_train_end(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of training. \u0026quot;\u0026quot;\u0026quot; if self.stopped_epoch \u0026gt; 0: best_values = ' - '.join(self.best_metrics_values_list) print(f\u0026quot;Epoch {self.stopped_epoch + 1}: early stopping\u0026quot;) print(f'Final results: {best_values}') print(f'Finish training - {str(datetime.datetime.now())}') def on_epoch_end(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of an epoch. \u0026quot;\u0026quot;\u0026quot; keys = list(logs.keys()) metrics_values_list = [] for key in keys: if key.startswith('val_'): metrics_values_list.append(f\u0026quot;{key}: {logs.get(key):.4f};\u0026quot;) else: metrics_values_list.append(f\u0026quot;train_{key}: {logs.get(key):.4f};\u0026quot;) values = ' - '.join(metrics_values_list) print(f\u0026quot;Epoch {epoch+1}/{self.epochs} - {values}\u0026quot;) current_loss = logs.get('val_loss') if np.less(current_loss, self.best_loss): self.best_loss = current_loss self.best_metrics_values_list = metrics_values_list else: self.stopped_epoch = epoch # fit時にcallbacksに作成したカスタマイズクラスをインスタンス化したものを渡す model.fit( ..., callbacks=[CustomCallBack()], ) 出力は以下のような感じになります．今回は print 文で出力していますが，logger を用意して logger.info を使うのも良いかと思います．\nStart training - 2021-11-09 23:48:12.787257 Epoch 1/10 - train_loss: 4.6889; - train_root_mean_squared_error: 2.1654; - val_loss: 11.1416; - val_root_mean_squared_error: 3.3379; ... Finish training - 2021-11-09 23:48:15.095133 おわりに 今回は SageMaker Experiments で実験管理を行う上でログ出力の形を修正したいという動機から CallBack 関数をカスタマイズしました．Callback 関数の中身を知るためにソースコードを読んだりして勉強になりました．Tensorflow のフレームワークは拡張性があり，カスタマイズの方法もドキュメントに整備されているので，比較的容易に修正できると思います．今回は時間経過や予測時間の表示は省いてしまったので，余裕があればログにこれらを出力するようにしていきたいです．\n追記 2022/02/24: 更新 earlystopping に対応する形式に CallBack 関数を修正しました．current_loss = logs.get('val_loss') で logs から get する value は callbacks.EarlyStopping(monitor='val_loss') で monitor に指定している値になります．\n参考 Sagemaker Training APIs - Estimator Writing your own callbacks tf.keras.callbacks.Callback ","date":"2021-11-10","permalink":"https://masatakashiwagi.github.io/portfolio/post/customize-tf-callback/","tags":["DEV","ML"],"title":"Tensorflow の Callback 関数をカスタマイズ"},{"content":"はじめに 6月から友人の@navitacionさんと一緒に Podcast の配信を開始しました．テック系の話題など自分たちが興味のある内容をあれこれと話すものになります．\nAnchor という無料で配信できるプラットフォームを使用しており，チャンネル名は「Double-M2.fm」になります．\n各 Episode の要約もありますので，興味を持って頂いた方はこちらから見ることができます．\nこの記事では，navi さんと始めたキッカケと実際の配信方法をまとめています．配信方法はあまり検索しても記事が無かったりしたので，これから始めようとしている人の参考になればと，そして Podcast 配信者が一人でも多く増えればなーと思います．\n配信のキッカケ 配信のキッカケは初回配信時にもチラッと触れたのですが，大きく2つ（+1）あります！\nコロナ禍で，オフラインイベントが無くなったことで，イベント後に参加者同士で雑談したり，情報交換したりする機会が無くなってしまったので，それを定期的にしたいという想いから アウトプットを意識するようになって，それを継続的に行っていく場の一つとして，音声による方法もあるのではという想いから 話のタネとしていいかなと笑 一つずつ説明していくと， まず①について，これは思っている人が多いんじゃないかなと思いますが，コロナ禍でオンラインでのイベントは多く開催されており，オフライン時に比べたら移動する手間もなくハードルが下がって，参加人数の制限も実質無制限で，抽選漏れの懸念も無くなりと良いことも多くある一方で，イベント後に登壇者以外の人同士や登壇者との会話がほぼほぼ無くなって，雑談とか情報交換したりする機会がかなり減ったと個人的に感じています．\nそういった状況で，会社の人以外の人と雑談など会話する時間が圧倒的に減ったというのもあり，定期的に情報交換したり意見を言い合える場があると良いなーということで今回 Podcast を始めました． （最近だと，Twitter が音声会話サービスの Space を始めて，そこで気軽に会話が生まれるのがスゴく良いなと感じてます！）\n②については，かなーーりサボり気味だったのですが，アウトプットちゃんとして行かないとなーという気持ちが高まり，それを強制的に行う一つの方法だったりもします！ Podcast で話すために，何かしらの話題を探したり，書籍・論文などを読んで調べたりとトピックの内容を整理する必要があると思います．また，内容を自分の言葉で相手に説明することで理解の助けになると思っています．（人に説明してみると，思ったよりわかってないなーと感じることあるあるだと思います）\nアウトプットの方法の一つとして，ブログにまとめるということ以外に音声でもやろうかなといったところです． あと，普段から「これ！ Podcast で話せるかも」とった意識をするだけで理解の仕方が変わると思うので，この習慣を付けたい狙いも個人的にあります！\n最後に③について，これは Twitter 上ではお互い認知していても，リアルだと会ったことない人との話のネタの一つにしようかなという魂胆です笑\n配信方法 配信するプラットフォームですが，これは僕が聞いている他の Podcast 配信者のを参考にして，Anchorにしました．Anchor は携帯でも簡単に録音することができて，それを配信できちゃったりもします．\n以下が僕たちが配信している各種構成になります．\n※ 全て無料で行うために，これらの構成になっています，有料でもいい場合はここで紹介したような複数ツールを使わなくて済むと思います．\nメール: Gmail 配信プラットフォーム: Anchor 録音: Zencastr 画面共有: Google Meet スクリプト: Scrapbox + GitHub 編集: Audacity 音楽 (BGM): Evoke Music アイコン: Canva 1. メール 各種サイトにアカウント登録する際にはメールアドレスが必須なので，まず共通で利用するメールアドレスを取得しました． 簡単に作成できるので，Gmail を新規作成し利用しています． 2. 配信プラットフォーム 最初にも書きましたが，配信プラットフォームは Anchor を採用しています． アカウントを作成し，Podcast name, description などを設定するだけで大丈夫です．無料で使うことができるのでおすすめです！ New Episode から録音したり，既にある音源をアップロードすることも可能になっています．また，BGM なども用意されています． ただ，遠隔地にいる人同士での複数人録音は出来なさそうだったので，僕たちは録音は後述の別ツールを使うことにしました． 音源をアップロードすると，スケジュールでの投稿予約ができます．そして，最初の Episode が登録されると，自分たちだけの Public Site が生成され，そこで新しい Podcast を聴くことができます． WHERE TO LISTEN の部分ですが，新しい Podcast が配信されたら，RSS のクローラーが検知して，色んなアプリで聴くことができます！ただし，Spotify と Apple Podcasts に関しては，RSS の URL を登録する必要があります．詳しくはこちらのnote記事が参考になるかと思います．ちなみに，RSS の URL は配信が開始したら表示される Settings → Distribution から確認することができます．\n3. 録音 録音は Anchor 上では行わず，Zencastr というツールを使いました．また Zencastr は複数人の録音も行うことができ，非常に便利です！\nプランは無料の Hobbyist と有料の Professional があります．僕たちは無料プランを使っていますが，制限としては以下になります．\n無料プランの制限： 1．ゲストは2名まで 2．1ヶ月あたり8時間の録音時間 基本的に二人での配信かつ週1回30分程度の録音時間なので，無料プランで全く問題ありません．また，Zencastr では画面の録画もできるみたいです．（※コロナの期間は，無料プランでもゲストと録音時間が無制限になっているみたいです）\nこちらも Anchor 同様，アカウント作成後，Create New Episode で新規作成を行うと画面と音声の録画・音声録画（画面表示あり）・音声録画のみの3パターンから選択します．\nタイトルを入れて作成すると，下記のような画面が出ます．\nここで，Invite ブをクリックすると同時に会話する人のメールアドレスにリンクを送付する形で招待することができます．Invite した人が入ってくると，そのまま通話状態になり録音を開始することで，そのまま両方の音声を録音することができます．\nあとは，録音終了後参加していた人の音声を MP3 でダウンロードすることができます．（これは管理者のみ可能な操作）\n僕たちは使わなかったですが，Mac でネット通話の音声を録音する方法（Soundflower, LadioCast, GarageBand）の記事を紹介しておきます．→ Mac でネット通話の音声を録音する方法（Soundflower, LadioCast, GarageBand）\n4. 画面共有 スクリプトなり資料なりを見ながら会話がすることが多いので，画面共有が必要になってきます．そのために，Google Meet を使って画面共有しながら会話するようにしています．\n5. スクリプト（台本・要約など） 収録を行う前に，事前に30分程度会話して何を話すか決めています．その際に，Scrapbox を活用して台本を作成したり，ネタ帳なども雑多に書いています．あとは，共有しておきたいことを Scrapbox に書いて基本的にはここを見ながらいつも収録しています．\n参考にした記事 → 初めてのポッドキャスト、試して気づいた「声」の醍醐味と9つのティップス\nScrapbox の他には，GitHub も使ってます．こちらは，Organization を作成し，そこに Podcast 用のリポジトリをさらに作成して，自己紹介ページや各配信の収録内容の要約を書き記しています．ページの更新時には issue を作成し，何をしたか記録が残るようにしています．\n6. 編集 編集に関しては，特別何かしているわけではないです．なるべく時間をかけず無理なく進めていきたいと思っているので，行っている内容としては下記2点です．\n音量調整・BGM 挿入 収録中に予期しない割り込みが入って，収録を止めた際のトリミング・不要な会話の削除 一通り自分たちの会話を聞いて，気になる部分があればトリミングなどしているぐらいになります．\n編集ソフトは Audacity を使っていて，昔からある音楽編集ソフトになります．Windows・Mac どちらも使用することができます． Mac の場合だと初めから入っている GarageBand なども使えるみたいです！\n7. 音楽 (BGM) Podcast を配信する時には，自分たちの音声に加えて BGM を追加しています．ただ，自分たちのコンテンツに BGM を使用する場合には，著作権などが絡んでくるので，安易に好きな曲を使用することはできません．楽曲1つ1つ確認するのは大変なので，今回は著作権フリーで AI が曲を作成してくれる Evoke Music というサイトの音源を使用しています．\nこのサイトでは，キーワードを指定することで，そのキーワードに合った曲を AI が自動で作成してくれます．1曲あたり数分程度の曲になります．\n※ β版の時は無料で使えてましたが，今は有料になってしまったみたいです．\n8. アイコン（カバーアートなど） Anchor にはカバーアートが設定できるので，そのアイコンなどを作成するために，Canva というデザインサイトを利用しました．Canva は豊富なテンプレートデザインがあるので，それをベースに自分でいい感じに編集するだけでかなりオシャレなロゴなどを作成することができます．\n無料プランだと保存時の画像サイズや解像度などを変更できないので，少し残念ですが，かなりオススメのサイトです．有料だともっと出来る幅が増えそうですが，現状だと無料プランでも十分かなと思っています．\nおわりに 今回は6月から開始した Podcast について，やろうと思ったキッカケとその配信方法をまとめました．配信方法については意外と記事がなかったので，もしこれから配信しようと考えている人の参考になれば嬉しいです！\n今回紹介した方法以外にも，もっと良い配信方法があると思うので，ご存知の方は是非教えて欲しいです！！一人でも多くのテック系 Podcast が増えて盛り上がると良いなと思います！！\nP.S. 細かい設定や登録など聞きたいことがある場合には，遠慮なく Twitter などでご連絡頂ければと思います．\n","date":"2021-06-13","permalink":"https://masatakashiwagi.github.io/portfolio/post/podcast-broadcast-method/","tags":["POEM"],"title":"Podcast による配信のキッカケとその方法"},{"content":"Kaggle-Shopee コンペの振り返り 2021/03/09~2021/05/11まで開催していたShopee コンペの振り返りになります．\n2週間程度しか手を動かせなかったですが，久しぶりに参加したので備忘録として記録を残しておきます．最終的な結果は179th/2464で銅メダルで，特に凝ったことは何もしていなかったので，妥当かなと思います．このコンペは上位10チーム中7チームが日本人チームで，日本人のレベルの高さを改めて実感できるコンペでした！\n概要 コンペの内容は簡単に言うと，画像とテキスト情報を用いて、2つの画像の類似性を比較し，どのアイテムが同じ商品であるかを予測するコンペになります．\n開催期間: 2021/03/09 ~ 2021/05/11 参加チーム数: 2464 予測対象: posting_id 列にマッチする全ての posting_id を予測する．ただし，posting_id は必ず self-match し，グループの上限は50個となっている． データ: 投稿ID，画像，商品のタイトル，画像の知覚ハッシュ (perceptual hash)，ラベルグループID 評価指標: F1-Score その他: コードコンペ My Solution 画像特徴量・テキスト特徴量・画像の phash 値を concat して結果をユニーク化したものを最終的な予測値としました． 何も複雑なことはしていないモデルですが，結果的に銅メダルを取ることができました． Image Model eca_nfnet_l0 loss: ArcFace pooling: AdaptiveAvgPool2d scheduler: CosineAnnealingLR loss(criterion): CrossEntropyLoss size: 512*512 eca_nfnet_l1 loss: CurricularFace pooling: MAC scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512 efficientnet_b3 loss: ArcFace pooling: GeM scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512 swin_small_patch4_window7_224 loss: CurricularFace scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 224*224 common augmentation by albumentations. HorizontalFlip VerticalFlip Rotate RandomBrightness optimizer Adam 少し工夫したポイント 画像特徴量を抽出する部分で，いくつか工夫した点をあげます（CNN Image Retrieval を参考にしました）．\nloss に ArcFace と CurricularFace を用いた ArcFace を使っている人が多かったが，CurricularFace も使いました．スコア的にはCurricularFace の方がよかったです． いくつかのモデルで pooling 層に GeM と MAC を用いた Google Landmark Retrieval Challenge で上手くいっていた GeM や MAC などのプーリング手法を用いました． loss (criterion) に FocalLoss を用いた 最終的に得られた特徴量を concat した後，ZCAWhitening による次元削減を行った 有効でなかったもの 一方で，上手くいかなかった内容としては以下になります． resnext50_32x4d swin_small_patch4_window7_224 with ArcFace CosFace, AdaCos PCA Whitening (worse than ZCAWhitening) Text Model paraphrase-xlm-r-multilingual-v1 loss: ArcFace scheduler: linear schedule with warmup loss(criterion): CrossEntropyLoss optimizer: AdamW TF-IDF text の方のモデルは特に改良する時間が取れなかったので，ほとんど手を付けれてなかったです． transformer と TF-IDF で得られた特徴量それぞれに対して，Cosine Similarity を計算し，text の prediction を作成しました．\n最終的な予測値は画像特徴量とテキスト特徴量に加えて，画像の phash 値を追加して，ユニークを取った値としています．\n反省 post-processing が全然できていなかった 他の解法を見るに，post-processing でスコアが伸びているので，この部分は結構大事だったんだなと感じています． 6位の解法にもありましたが，今回のコンペでは，label_group の長さが2以上であることから，「予測した結果の posting_id が1つしかない場合，強制的に似たものを持ってきて，2つにする」というアイデアで LB がかなり上がるみたい Image と Text を Multi-modal 的にモデルに組み込んで学習することができていなかった グラフ理論全然わかってないです笑 細かい部分 他の optimizer を試す SAM とか Database-side feature augmentation (DBA) / Query Extension (QE) 全然知らなかったので，End-to-end Learning of Deep Visual Representations for Image Retrieval を読んで勉強したい 閾値の調整 テキストモデルの追加 ここからは上位の解法を書きたいと思います．数もそれなりにあるので，載せるのは上位5つにします．最後に上位5つ以外にも Discussions に投稿されている解法のリンクを載せておきます．\n1st Place Solution 解法はこちらになります: 1st Place Solution - From Embeddings to Matches\nModel Image: 2つのモデル eca_nfnet_l1 * 2 Text: 5つのモデル xlm-roberta-large xlm-roberta-base cahya/bert-base-indonesian-1.5G indobenchmark/indobert-large-p1 bert-base-multilingual-uncased loss: ArcFace pooling した後に batch normalization と feature-wise normalization を行った ArcFace のチューニングを行った 学習段階で徐々に margin を大きくした（埋め込み表現の quality に影響する） 画像に対する margin: 0.8~1.0 テキストに対する margin: 0.6~0.8 margin を大きくすると，モデルの収束に問題が出たので，以下を行った warmup steps を大きくする cosinehead に対する learning rate をより大きくする gradient clipping を行う class-size-adaptive margin も試したが，不均衡性が小さかったため，改善は少しだけだった image: class_size^-0.1 text: class_size^-0.2 global average pooling の後に FC 層を追加するとモデルが悪くなるが，feature-wise normalization の前に batch normalization を追加するとスコアが改善した Features Combining Image \u0026amp; Text Matches マッチングさせる方法をいくつかトライした 画像の embedding によるマッチングとテキストの embedding によるマッチングを結合する 画像の embedding とテキストの embedding を concat してからマッチングする 1と2を組み合わせてマッチングする これは，画像の embedding が強く示唆するアイテム・テキストの embedding が強く示唆するアイテム・画像とテキストの embedding がどちらも適度に示唆するアイテムを取り入れることができる Iterative Neighborhood Blending (INB) QE/DBA とは少し異なる，embedding からマッチする商品を検索するパイプラインを作った k-Nearest Neighbor Search: 近隣探索ラリブラリ: faiss (k=51: 50(自身以外)) + 1(自身)) Threshold: コサイン類似度をコサイン距離(=1-コサイン類似度)に変換し，distance \u0026lt; threshold を満たす (matches, distances) のペアを取得した 閾値処理を行う場合，1つのクエリに対して少なくとも2つ以上のマッチがあることがコンペで保証されているので，distance が min2-threshold を超えた場合にのみ，二番目に近いマッチを除外する Neighborhood Blending: kNN によるサーチと min2 による閾値処理をした後，各アイテムの (matches, similarities) ペアを取得し，グラフを作成する 各ノードはアイテム，エッジの重みは2つのノード間の類似性を示す 近傍のみが繋がっており，閾値条件と min2 条件を満たさないノードは切断される 近傍のアイテムの情報を使いたいので，クエリアイテムの embedding を改良し，よりクラスタを明確にする 重みとして類似性を持つ近傍の embedding を加重合計し，それをクエリ embedding に追加する 上図を簡単に説明すると，最初 A は [B,C,D] と繋がっているが，加重合計した結果閾値=0.5の場合，C との接続が切れて，A は [B,D] のみと接続していることがわかる この NB の処理を評価指標の改善が止まるまで繰り返し実行する 最終的な NB のパイプラインは以下になる About Threhsold Tuning: 調整する閾値は全部で10種類ある stage1 の text, image, combination の閾値が3つ stage2 の閾値が1つ stage3 の閾値が1つ 直接最終結合部分に繋がる stage1 の text, image の閾値が2つ stage1~3 の min2 閾値が3つ 最終的には stage2,3 の閾値を2つ調整するだけでよかった Visualizations of Embeddings before/after INB INB の効果を可視化したノートブックが公開されている (SHOPEE) Embedding Visualizations before/after INB 見るからに各点が凝集されたクラスタを形成していることがわかる Others 画像に対する CutMix(p=0.1) 画像の augmentation horizontal flip のみがよかった madgrad optimizer: https://github.com/facebookresearch/madgrad 学習データ全体を使った学習 2nd Place Solution 解法はこちらになります: 2nd place solution (matching prediction by GAT \u0026amp; LGB), 2nd Place Solution Code\nSummary 1st stage: 画像・テキスト・画像+テキストデータに対するコサイン類似度を得るために Metric Learning によるモデルを学習する 2nd stage: 同じ label group に属するアイテムのペアかどうかを識別するために\u0026quot;メタ\u0026quot;モデルを学習する Model image: 2つのモデル backbone nfnet-F0 ViT loss: CurricularFace optimizer: SAM embedding の結合: F.normalize(torch.cat([F.normalize(emb1), F.normalize(emb2)], axis=1)) text: 3つのモデル + TF-IDF indonesian-bert multilingual-bert paraphrase-xlm image+text: nfnet-F0 と indonesian-bert の FC 層の embedding を結合したもの Training Tips: label group のサイズに基づいた Sample Weighting 1 / (label group size) ** 0.4 Features Graph features それぞれのアイテムの top-K コサイン類似度の平均と分散 K=5, 10, 15, 30, etc 標準化（mean=0, std=1） Pagerank Others テキストの長さ #の記号 Levenshtein距離 画像ファイルのサイズ 画像の高さと幅 Query Extension Ensemble Methodology ローカルデータでそれぞれのモデルのベストな閾値を計算する 上記閾値でそれぞれのモデルの予測値を差し引く 差し引かれた予測値を合計する 合計値\u0026gt;0の場合，アイテムペアが同じグループに属するとする お互いにエッジを持たないペアを削除する A-B はあるが，B-A がない場合は両方を削除する Post-Processing 中間中心性が最も高いエッジを再帰的に削除する（Graph-Based） Others Performance and Memory Tunings CuDF, cupy, cugraph: GPU を有効に使うためには大事 ForestInference: 40分かかる CPU での推論が2分になる Not worked Graph-based の特徴量 固有ベクトルの中心性と jaccard End-to-end model Local feature matching 3rd Place Solution 解法はこちらになります: 3rd Place Solution (Triplet loss, Boosting, Clustering)\nModel image: 2つのモデル backbone efficientnet_b2 ViT (DINO) text: 2つのモデル (異なるtokenizersとCLIP) indonesian-bert multilingual-bert common loss: TripletLoss (margin=0.1) 各エポック，label_groups から重複したペアを作成し，各バッチで label_groups から1ペアが挿入される．バッチサイズは128を使っていたため，バッチ毎に64の重複を取得し，ランダムな5点でそれらの各々を比較する 5fold CV で，validation スコアを計算する時は，validation-fold の中から候補を選ぶのではなく，学習サンプル全体から候補を探した この方法は CV/LB の gap を抑えて，相関を取ることが出来る oof を用意して，2nd-level の予測に使用する CV から得られた5つのモデルで，テストデータに対して推論を行った より速く推論するためには，validation なしの1つのモデルでテストデータにfitさせること ArcFace は上手くいかなかった Features 複数モデルを用いて，異なる表現方法を作成するのが良い 全ての embeddings から候補となる近しいものを結合し，ペアが実際に重複しているかどうかにかかわらず，binary target を使用してペアオブジェクトのサンプルを作成する 3M点のペアがあり，重複率は4%だった これらに対して，GBM(=CatBoost) を作成し，embedding 毎に計算する pairwise-distances (コサイン類似度，ユークリッド距離など) 両方の点周辺の密度 points ranks 最終的には500個特徴量を作成し，CV や LB を使いながら，重複確率による閾値の候補を出す → LB:0.76+ Post-Processing クラスタリングを行ったが，embeddings を使うのではなく，pairwise-distance を使う 重複推定のために GBM(=CatBoost) の確率を使い，類似度を求める 凝集クラスタリングのアイデアを採用 各単一点をクラスタとして開始し，平均的なクラスタサイズが閾値に等しくなるまでそれらをマージしていく クラスタが単一の場合，最も近傍のクラスタにマージする → LB:0.79 Others 最適化するために half precision(torch AMP) を使って学習と推論を行った 画像モデルの場合，画像の読み込みとリサイズに NVIDIA DALI を使った GBM の推論には，Rapids ForestInference Library を使った 上記の方法を使わないと，2時間で全てのモデルを推論することは不可能だった 4th Place Solution 解法はこちらになります: 4th Place Solution\nModel image: backbone に nfnet と efficientnet pooling: GeM＆Avg pooling embedding が大きいほど，LB のスコアがよくなった loss: ArcFace margin の調整が大事だった text: BERTベース + TF-IDF TF-IDF: かなり大きい embedding を生成し，notebook 上ではメモリの制約を受けるため，Random Sparse Projection で次元削減を行った loss: ArcFace margin の調整が大事だった Ensemble 画像の embedding・テキスト（BERTベース）の embedding・TF-IDF の embedding を結合して，正規化した これらのベクトル表現のそれぞれに対して，pairwise コサイン類似度を計算し，3つの行列を作成した 最初に二乗し，その後加重平均を取って行列を結合した Post-Processing 閾値の調整 Rank2 matching もし，A が Rank2 に B を持っており，B は Rank2 に A を持っている場合，お互いに追加する Rank2 と Rank3 の違いが大きい場合，Rank2 の ID を追加する 少なくとも1つの他とのマッチング（Rank2 のコサイン類似度が極端に小さくなければ） Query Expansion マッチしていない行との再マッチング その他上位解法のリンク Kaggle ShopeeコンペPrivate LB待機枠＆プチ反省会 5th Place Solution 6th place solution 7th place solution 8th Place Solution Overview Public 16th / Private 10th Solution 11th Place Solution 14th Place Gold - Image Text Decision Boundary 15th place solution Public 13th / Private 16th solution 18th place solution - You really don\u0026rsquo;t need big models 19 place. Voting and similarity chain 26th Place Solution : Effective Cluster Separation and Neighbour Search [31st Place] Object Detection approach Public 39th / Private 37th Solution 48th Place Silver - Simple Baseline Public 56th / Private 57th Solution 62th Place Solution (Stacking Logistic Regression) 72nd place solution ","date":"2021-05-14","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-shopee-solution/","tags":["KAGGLE"],"title":"Kaggle-Shopee コンペの振り返りとソリューション"},{"content":"はじめに 今まで仕事では，開発環境として IntelliJ を使っていたのですが，最近は VSCode の人気が高く Extensions も便利なものが多くあるということで，個人的な作業をする時は VSCode を使ってみようと思って使っています． そんな中で，タイトルにもあるように VSCode から git push しようとしたら，\u0026lt;アカウント名\u0026gt;@github.com: Permission denied (publickey). とエラーが出たので，それを解消して VSCode で git push できるようにした備忘録になります．\nエラー原因（SSH 接続エラー） 「Permission denied (publickey)」とあり，GitHub に SSH 接続するために，公開鍵を登録しておかないといけないのですが，それをしていなかったので，エラーが発生したということになります．\n以下のコマンドを打つことで接続を確認することができます．\nssh -T git@github.com \u0026gt; git@github.com: Permission denied (publickey). ではどうすればいいかというと，鍵を生成して GitHub に登録すればいいということになります．\n公開鍵と秘密鍵の作成 詳細な作成方法はこちらの記事が参考になります．\n簡単に手順を載せておきます．\ncd ~/.ssh ssh-keygen -t rsa -b 4096 -C \u0026quot;\u0026lt;メールアドレス\u0026gt;\u0026quot; -f github_rsa # オプションをいくつか設定して，鍵を生成 # 以下実行結果（一部マスクしてます） Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in github_rsa. Your public key has been saved in github_rsa.pub. The key fingerprint is: SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \u0026lt;メールアドレス\u0026gt; The key's randomart image is: +---[RSA 4096]----+ |=== | |.B o . | |o.. . * . o | |. . . B +. oo .o*| | . o * OSo.oooo*+| | . = + = o ..*..| | E . . o . . ..| | . . | | | +----[SHA256]-----+ 鍵の種類を RSA にし，鍵の長さを4096にしています．ファイル名は github_rsa と設定しました．\nGitHub に生成した公開鍵を登録 cd ~/.ssh ssh-add -K github_rsa # 秘密鍵をssh-agentデーモンに登録 pbcopy \u0026lt; github_rsa.pub # pbcopyコマンドで公開鍵の中身をクリップボードにコピー この後は，コピーした公開鍵の中身を GitHub に登録します．\nGitHub のアカウントから Settings に進み，SSH and GPG keys を選択し，New SSH key を押して，先程コピーした中身をペーストし，名前を決めて保存します．\nSSH 接続確認 保存が完了したら，SSH 接続できるか確認するために，以下のコマンドを打って確認します．\nssh -T git@github.com \u0026gt;Hi \u0026lt;ユーザー名\u0026gt;! You've successfully authenticated, but GitHub does not provide shell access. Remote 設定の上書き ここまで来たら後一息で，最後に remote の設定を上書きします． 以下のような感じでリポジトリ名を書いて，実行すればOK．\ngit remote set-url origin git@github.com:\u0026lt;ユーザー名\u0026gt;/\u0026lt;リポジトリ名\u0026gt;.git VSCode からgit push 今までの設定が完了していれば，上記画像の手順で VSCode の画面から簡単に git に commit や push などの操作を行うことができます．\nおわりに まだまだ VSCode 初心者なので，使いやすい Extensions を取り入れて開発環境をカスタマイズしていきたいと思います！\n参考 初めてのgitは5ステップで完了 GitHubでssh接続する手順~公開鍵・秘密鍵の生成から~ ","date":"2021-03-26","permalink":"https://masatakashiwagi.github.io/portfolio/post/vscode-git-connect/","tags":["DEV"],"title":"VSCode と git を連携して push できるようにするまで"},{"content":"はじめに Pytorch でモデルを作成していた際に，「RuntimeError: CUDA error: device-side assert triggered」が発生し，原因がよくわからなかったので，調べたことをメモしておきます．\nエラー発生の原因 調べてみると，原因としては以下のようなものがあります．\nライブラリの Version が違う ラベル/クラスの数とネットワークの入出力の shape が異なる Loss 関数の入力が正確でない などなど\u0026hellip;\nよくあるのが，下2つかなと思います．\nラベル/クラスの数とネットワークの入出力の shape が異なる 想定しているラベルもしくはクラス数とネットワークの出力のクラス数が異なる場合，この場合は FC 層の最後に nn.Linear(input, num_class) を入れて調整する必要があります．\nLoss 関数の入力が正確でない 僕が遭遇したのはこちらのパターンになります．\n例えば，BCELoss を考えた場合，計算するためには値としては0~1を取る必要があります．そのため普通は最終出力に Sigmoid関数 or Softmax関数 を入れるかと思います．\nそれ以外にも Loss の設計で以下のようにしておくと良いかと思います．\nclass BCELoss(nn.Module): def __init__(self): super().__init__() self.bce = nn.BCELoss() def forward(self, input, target): input = torch.where(torch.isnan(input), torch.zeros_like(input), input) input = torch.where(torch.isinf(input), torch.zeros_like(input), input) input = torch.where(input\u0026gt;1, torch.ones_like(input), input) # 1を超える場合には1にする target = target.float() return self.bce(input, target) 他の解決方法 他にも調べていると解決方法として CUDA の設定を以下にすると良いなどもありましたが，解決するかどうかはよくわからないです．\nCUDA_LAUNCH_BLOCKING=1 おわりに 今回は，Pytorch でのモデル作成時に発生したエラーについて整理しました，モデル作成時にはモデルの In/Out や Loss 関数の定義をきちんと理解し把握しておく必要があると改めて感じました．同様のエラーが起きた場合には，この辺りをまずは調べてみるのが良さそうです．\n参考 CUDA error 59: Device-side assert triggered ","date":"2021-02-01","permalink":"https://masatakashiwagi.github.io/portfolio/post/cuda-error-device-side-assert-triggered/","tags":["DEV","ML"],"title":"RuntimeError: CUDA error: Device-side assert triggered の解決方法"},{"content":"Kaggle-MoA コンペに Team 90\u0026rsquo;s で初参加 このブログは2020/9/4~12/1まで開催していた MoA コンペでの取り組みを紹介する内容です． （コンペの詳細な内容については割愛します）\n今回のコンペでは，同世代のメンバーでチームを組んで取り組みました！チーム結成の経緯は，Twitter でお互いが90年生まれということを知って，同世代で Kaggle チーム組んで戦いたいねーという感じだったと思います．それが少し前のことで当時取り組める良い感じのコンペがなかったのですが，今回テーブルデータのコンペで取り組めそうということで始まりました．\nチームでの取り組みはとにかく学びが多く，終盤までモチベーションを保つことができたのが大きかったです．\nまた，議論することで理解なども深まっていくので，コンペを通してより成長できたんじゃないかなと思います．\n今回の僕たちのチームでの取り組み方を紹介すると，\n1. 情報は Slack で共有 2. 分析方針や実験結果は Github の issue で管理 3. 毎週末に2時間程度のディスカッション\nといった感じです．\n3番目の週末のディスカッションは強制ではなく，参加可能な人が参加する形式で運用してました． （と言いつつもみんな真面目に毎回参加してました笑）\n今回はチームでの取り組み方針の具体的な内容について少しだけ掘り下げます．\n1. 情報は Slack で共有 Slack をどうゆう感じで活用していたのかというと，コンペの Discussion や Notebook の内容について疑問点などを話し合ったり，それ以外にも進め方の相談や雑談などを基本的に行っていました． あとは submit する時は一言声をかけるなどの submit 管理もしていました．\nこうゆうのがあれば良かったなーというところでは，新着の Discussion や Notebook を Kaggle から連携して通知する仕組みを用意しておければ尚良かったのかなと思いました．\n2. 分析方針や実験結果は Github の issue で管理 Github をどうゆう感じで活用していたのかというと，分析での実験毎に1つの issue を立てて，そこでどうゆう実験をしたのか submit した結果のスコアがどうだったのかなどを記録として残してました． また，共通で使える特徴量生成のコードだったり，CV の切り方のコードなどの共有も行ってました．\nその他には Discussion の内容を整理したり，情報をまとめるために活用したりしていました．\n3. 毎週末に2時間程度のディスカッション 週末に Google meet でオンラインディスカッションでしていて，そこで何をしていたかというと，基本的には，今週何をしたのかを各々共有したり，わからない部分を話し合ってどうゆうふうに次進めて行くかなどをチームで考えていました．あとは，次の週でどうゆうことをするかの方向性を決めて終わる感じでした．\nもちろん雑談や仕事での苦労を労ったりもしていました笑\n最終順位 最終順位は4373チーム中34位の銀メダルで、金メダルまであともう少しのところまで行ったので，とても悔しい結果となりました．\n個人的には Inference の処理がエラーで通らない状況に最後の3日ぐらいでなって泣きそうになりました．チームメンバーには weight0 の状態で非常に申し訳なかったなと思ってます泣\n学習時に回していたノートブックでは，スコアがチーム内で作ったモデルの中でも上位5つ以内に入っていたので，アンサンブル時には効いてただろうなと思うと尚更です．\n個人的な成長としては，テーブルデータに対して NeuralNet が有効に作用する場面について多少理解が深まったと感じています．\n今回の MoA では，マルチラベルの予測だったので，一度に大量のクラスを予測する場合には NN が有効でかつ GBDT 系と比較して計算速度も速いんだなと感じました．\nまた，特徴量的にも交互作用的な部分はNN内部の中間層の組み方などで実現できるので，GBDT 系みたく大量に特徴量を用意しなくても対処できるのが大きいのかなと思っています．（今回のケースだと GBDT で大量のモデルを作るとなると速度的な部分で特徴量が膨大になるとかなり厳しい感じでした）\nあとは，NN の実装を Pytorch で行ったこともあり，Pytorch の扱い方がわかるようになったのは大きいと思っています．（仕事では Tensorflow だったりするので\u0026hellip;）\nPytorch での実装に関してはもっと進めて行きたいのとコードの整理も合わせてして行きたいので，次に参加予定のコンペではその辺りも意識して挑めたらなーと思います．\n","date":"2020-12-11","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-moa/","tags":["KAGGLE"],"title":"Kaggle-MoA の振り返り"},{"content":"ポートフォリオ作成 こちらの Qiita の記事で Hugo を使って簡単にポートフォリオを作成できるというのを見かけたので，以前まで使っていた personal page を移植しました． 移植した際に少し詰まった部分があるので，Tips としてこの記事で紹介します．\nこの記事は以前に使用していた Hugo Theme の内容になります\n最初は Hugo Theme Cactus Plus というテーマで作成していたのですが，再度作り直してます． （再作成した理由は，少しだけ凝ったテーマを使って見たくなったためです笑） 作り直したテーマは Hugo Future Imperfect Slim になります．\nHugo はシンプルなデザインが多いので非常にオススメです．\n基本的な構築方法は上記 Qiita の記事に沿って行っています． 別途追加した要素としては，最初に作成した Hugo Theme Cactus Plus と作り直した Hugo Future Imperfect Slim それぞれありますので，この際どちらも紹介します．\nHugo Theme Cactus Plus\nメニューの追加設定 Custom-CSSの設定（custom-cssの設定は簡単に設定できますので，今回は割愛します） Hugo Future Imperfect Slim\nfaviconの設定 github.ioでサイトをhostした場合のpath設定 個人的には，1回目に作成したテーマより2回目の方が簡単でした．\nHugo Theme Cactus Plus メニューの追加設定 Hugo Theme Cactus Plus のテーマでは，デフォルトで About/Archive/Tags の3つがメニューとして存在しています．今回はそこに Projects を新しく追加しましたので，その方法を記載します． 実施することとしては，以下の4ステップになります．\ncontent 配下に projects ディレクトリを作成し，_index.md ファイルを配置する． 記事などのページ情報は content で管理します．\n├── content │ ├── about │ ├── posts │ └── projects │ └── _index.md themes/layouts/partials 配下にある nav.html に Projects のリンクを追記する． これは Tags などのリンクをコピーして，name の部分は projects に修正すれば大丈夫です． メニューバーに Projects を表示させるために，この部分を修正する必要があります．\nthemes/layouts/section 配下に about.html をコピーして，projects.html に rename する．ここに追加することで，セクションのトップページとして扱われることになります．\n最後に，コマンドラインで hugo を実行する． hugo コマンドを実行することで，必要なものが自動生成・反映されます．\n以上でメニューを追加することができます． （他のテーマでは，もう少し簡単にメニュー追加が可能なものもあります）\nHugo Future Imperfect Slim favicon の設定 favicon を設定する方法は，下記の3ステップになります．\nまず，下記のデフォルトの config.toml の内容のうち，favicon と faviconVersion を変更します．\n[params.meta] description = \u0026quot;A theme by HTML5 UP, ported by Julio Pescador. Slimmed and enhanced by Patrick Collins. Multilingual by StatnMap. Powered by Hugo.\u0026quot; author = \u0026quot;HTML5UP and Hugo\u0026quot; favicon = false \u0026lt;-- trueに変更する svg = true faviconVersion = \u0026quot;1\u0026quot; \u0026lt;-- 1を削除する msColor = \u0026quot;#ffffff\u0026quot; iOSColor = \u0026quot;#ffffff\u0026quot; config.toml を修正したら，static 配下に favicon ディレクトリを作成する．\nstatic/favicon 配下に favicon.ico と favicon-32x32.png を配置する． なぜ favicon-32x32 かというと？\nlayouts/partials/meta.html の rel=icon に以下が記載されている favicon-32x32 favicon-16x16 site.webmanifest なので，これに合わせて名前を変更するか webmanifest を新しく作成し，その中に諸々の内容を記載する必要がある．\ngithub.io でサイトを host した場合の path 設定 今回作成したサイトを github.io で host した場合に発生した内容です． 各メニューの URL として，https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/home/ などとなって欲しいのですが，https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/portfolio/home/ と portfolio が重なってしまうエラーが発生しました．\n上記エラーを回避する方法の紹介になります． config.toml ファイルに各メニューの設定をする箇所があります．\n[menu] [[menu.main]] name = \u0026quot;Home\u0026quot; identifier = \u0026quot;home\u0026quot; url = \u0026quot;/\u0026quot; \u0026lt;-- /を削除する pre = \u0026quot;\u0026lt;i class='fa fa-home'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 1 [[menu.main]] name = \u0026quot;About\u0026quot; identifier = \u0026quot;about\u0026quot; url = \u0026quot;/about/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-id-card'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 2 [[menu.main]] name = \u0026quot;Blog\u0026quot; identifier = \u0026quot;blog\u0026quot; url = \u0026quot;/blog/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-newspaper'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 3 url の部分で先頭の/を削除することで，上記問題を回避することできます．\nおわりに 参考となる記事などがあまりなかったので，試行錯誤しながら行いました．そのため，もっと簡単にする方法が他にもあるかもしれないですので，もし他にあれば，Twitter などでコメント頂けると大変助かります．\n","date":"2020-07-13","permalink":"https://masatakashiwagi.github.io/portfolio/post/hugo-portfolio/","tags":["DEV"],"title":"Hugo を使ったポートフォリオ作成"},{"content":"はじめに Kaggle を始めて半年ほど経ち，個人的にこの半年で得たものを整理するという意味で「kaggle その2 Advent Calendar 2019」の20日目を担当します．\n簡単な自己紹介として，普段は都内のベンチャー企業で主に製造業のお客さんを相手にデータ分析の仕事と自社製品の開発を8:2ぐらいの割合で担当しています．\n実は JTC から転職して今の会社は2社目で，働いて2年弱になります．\nちなみに前職の JTC では，マーケティングオートメーションツール導入などの SE 的なことをしてました．なので，バリバリデータサイエンスをしていたわけではないです笑\nKaggle は転職した時ぐらいから知って，すぐやり始めたいと思ってたのですが，色々と仕事プライベート共に余裕が無くて満を辞して半年ほど前から本格的に参加し始めました！\nKaggle を始めて得たもの 今回はそんな半年ほど前から Kaggle を始めて得たものとして，大きく3つあり，それについて書きます．\nデータサイエンス関連の知識 実務へのフィードバック 人との繋がり それぞれ簡単ですが，書いていきたいと思います．\n1. データサイエンス関連の知識 よく言われていることですが，Kaggle は宝の山であり世界中のデータサイエンティスト・機械学習エンジニアの知恵や情報が特にコードレベルで共有されているのが魅力の一つです．掘れば掘るほど色々と出て来るので，これを活用しない手はないなという印象です．\nその中でも，特に個人的に得て良かった知見としては以下の4つかなと思います．\n1つ目は，\n特徴量エンジニアリング ドメイン知識に基づく特徴量エンジニアリングが有効であることはもちろん知っていましたが，それを作る発想であったり組み立て方が非常に勉強になってます． また関連して，どの単位で集約した特徴量を作るか，カテゴリカルデータやカウントデータの扱い，エンコーディングの仕方であったりと特徴量の作り方は非常に参考になってます． 2つ目は，\nパイプライン設計 パイプライン設計は Kaggle を始めてから特に意識させられた部分になります．参考になる情報をいくつか上げておきます[1, 2, 3]． これを意識して良かったこととしては\u0026hellip;\n特徴量の管理が楽になる 試行錯誤した結果をログという形で後から確認できる 結果の再現性も容易になる 計算回した後は寝てられる笑 他にも色々と良いことはあるので，是非オススメしたいです！後々の再利用のためにも整理しておくと，一から全てを作り出さなくても良いので，有用かと思います．\n3つ目は，\nバリデーションの重要性 モデルの汎化性能を考える上では大事な要素で，Kaggle では特に pubulic LB で上位に入っていても，バリデーションをきちんとしていないと private LB で大きく shake down してしまう結果になることがよく？あるのかなという印象です． 学習データの結果が良くてもテストデータで全然良くないとなると使い物にならないので、この辺りは実務でも活きてくる部分になります．運用段階で全然使えないモデルが出来上がるのを回避できる方法の1つ． 4つ目は，\nNN をテーブルデータで使う方法 Neural Network は画像認識の領域で使われてますが，それをテーブルデータに使う方法が Kaggle では見かけます． テーブルコンペでは，GBDT 系のアルゴリズムの方がまだまだ精度的には良いですが，モデルの多様性や特徴量抽出の自動化的な部分で NN モデルも十分に活用できると思ってます． この辺りはもっと kernel などで理解して自分の武器にしていきたいなという感じです．ただ，前までは選択肢にもなかった気がするので．様々な手法を見た結果得られた知見かなと． 2. 実務へのフィードバック Kaggle で実施する内容と実務での内容が必ずしも直結するわけではないですが，分析スキルの向上は実務でも大きく活きてます．\n例えば，実務でデータ受領後，EDA を進める中でデータの勘所を掴むのが以前より早くなったのと，何をどうすれば良いかを掴むのが以前よりスピードが上がったと感じてます．それによって，案件を進めていくスピードが上がったので，色々と試行錯誤できる時間を確保できるようになったと思います．\nまた，Kaggle で有効な手法を製品へフィードバックすることも進めているので，自分自身だけでなく会社へも還元できつつあるのかなという感じです．\n一方で Kaggle が楽しすぎて，仕事中でもコンペのことが気になって手を動かしたくなったり，休日だいたい費やしてるので出不精になったりしてます笑\nTwitter でも書きましたが，良くも悪くも世界中の人たちと競い合って評価されるので，負けたくない精神は会社でも発揮されてます！\n人との繋がり Kaggle を始めてから，もくもく会や勉強会に参加する頻度が増えたかなと感じてます．大体最後には懇親会があるので，コンペの話や仕事の話で盛り上がって色々と情報交換が出来ていて楽しい限りです．\nあとは，コンペ終了後の反省会に参加することでコンペでの苦しみなどを共有できるのも良いコミュニティーだなと思います．そういった場で社内以外のデータサイエンティスト・機械学習エンジニアの方々とお話しできるのは非常に良い刺激になります．自分が知らないことを知ってる人がめっちゃいるので，勉強になりまくりです．\n前々から社外での繋がりを増やしたいと思ってたところで Kaggle という共通の話題があるので，比較的話がしやすい環境ができて Kaggle 様様です．もっと Kaggle での繋がりを増やして，お互い切磋琢磨できる環境に持っていきたいですね．\nおわりに まだまだ半年しか経っていないですが，濃い経験や知見を Kaggle を通して得られているので，これからも継続していきたいです！\n人との繋がり的には，もう少し仕事面でも色々と相談できる関係性を作って，どんなことをやっててどうゆう課題や問題意識があるかとか聞いてみたいです．\nあとは幸いにも，勉強会で知り合った方とコンペでチームを組んで頂けるようになったので，次はチームで参加する楽しみも味わいます！\n最後に Kaggle では、まさに以下の話を体現できるのではと思ってます！\n頭で理屈をわかったところで，体感を伴わない知識は活用できない（ハリガネサービス）\nP.S. DSB コンペでメダルを獲得します！\n参考 [1] Kaggleで使えるFeather形式を利用した特徴量管理法 [2] データ分析コンペで使っているワイの学習・推論パイプラインを晒します [3] hakubishin3/kaggle_ieee ","date":"2019-12-20","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-get-something/","tags":["POEM","KAGGLE"],"title":"Kaggle を始めて半年経って個人的に得たもの"},{"content":"はじめに 今回は，先日初めて見たファイル形式の Excel Binary Workbook (xlsb) に関して，python で csv にパースする話です．\n.xlsx はよくある Excel ファイルの形式ですが，それのバイナリー形式である .xlsb に関しての話です．（今まで見たことなかった拡張子です笑）\nExcel の闇や Excel との格闘は色々ありますが，今回はそこをグッと堪えて進めたいと思います笑\n.xlsb とは Weblio 辞書によると以下のように記載されています．\n.xlsbとは，Excel 2007で作成したブックを「XML形式でないバイナリブック」として保存する際に用いられる拡張子である．.xlsbでブックを保存した場合はファイル全体がバイナリ形式で保存され，XMLベースである.xlsxなどのファイル形式で保存した場合と比べて，ファイルサイズを数分の1程度に抑えることができる．\n受け取ったファイルは .xlsb 形式でも100MBぐらいあったため，.xlsx 形式だとかなり容量が大きく，ファイルを開くと処理が重たくなることが想像できるので，圧縮したのだと考えられます．\nExcelを扱えるpythonライブラリ openpyxl 定番の openpyxl です．\n上記ページにも記載されてますが，Excel ファイルの拡張子である .xlsx を扱うことができます．\nopenpyxl is a Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files.\nいつものようにこのライブラリで処理しようとしたところ，下記のようなエラーが発生しました．\nopenpyxl.utils.exceptions.InvalidFileException: openpyxl does not support binary format .xlsb, please convert this file to .xlsx format if you want to open it with openpyxl もう一度 openpyxl の説明を見ると，確かに扱える拡張子は xlsx/xlsm/xltx/xltm となっているので，.xlsb は扱えないのが分かります．\nそこで，.xlsb の拡張子が扱えるライブラリを調べたところ，pyxlsb というのがあるみたいなので，それを使うことにしました．\npyxlsb pyxlsb は公式の説明にあるように，xlsb 形式を扱える python ライブラリになります．\npyxlsb is an Excel 2007-2010 Binary Workbook (xlsb) parser for Python.\npip でインストールすることができます．\npip install pyxlsb 公式のサンプルコードを記載しておきます．\nimport csv from pyxlsb import open_workbook with open_workbook('Book1.xlsb') as wb: for name in wb.sheets: with wb.get_sheet(name) as sheet, open(name + '.csv', 'w') as f: writer = csv.writer(f) for row in sheet.rows(): writer.writerow([c.v for c in row]) もし pandas のデータフレームに変換したい場合は，参考ページのコードで可能となります．\nただし，時刻変換に関して少し注意が必要なので，記載しておくと公式にもある通り，日付は float に変換されてしまうため，convert_date 関数を使う必要があります．\nNote that dates will appear as floats. You must use the convert_date(date) method from the corresponding Workbook instance to turn them into datetime.\nなので，元のファイルに時刻が入っている場合には上記変換をコードの中に入れて処理する必要がありますのでご注意下さい．\nprint(wb.convert_date(41235.45578)) \u0026gt;\u0026gt;\u0026gt; datetime.datetime(2012, 11, 22, 10, 56, 19) おわりに 今回は個人的に嵌ってしまった .xlsb 形式のファイルを扱う方法を紹介しましたが，出来ればデータ分析をするようなデータを Excel ファイルで扱いたくないのが本音です😅\nもちろん簡単なデータの可視化とか表計算で Excel が活躍する場面は多々あると思うので，使い分けて行くのが良いのではと思います．\n参考 Stack Overflow - Read XLSB File in Pandas Python 追記 2020/01/05: 更新 pandas の「version=1.0.0」で .xlsb ファイルをロードできるようになったみたいです．方法は pd.read_excel の引数で engine=\u0026quot;pyxlsb\u0026quot; と指定するだけです．\n# Returns a DataFrame pd.read_excel(\u0026quot;path_to_file.xlsb\u0026quot;, engine=\u0026quot;pyxlsb\u0026quot;) 参考: https://pandas.pydata.org/docs/user_guide/io.html#io-xlsb\n","date":"2019-10-05","permalink":"https://masatakashiwagi.github.io/portfolio/post/excel-processing-using-python/","tags":["DEV"],"title":"Excel Binary Workbook を Python で処理"}]