[{"content":"はじめに 最近，データエンジニアリングとMLOpsの領域への関心が個人的に高まっていて，何か個人プロジェクトで出来ないかと考えていたところ，普段スプレッドシートで記録している家計簿のデータを使って，データエンジニアリングとMLOpsの技術検証をしようと言う考えに至りました．\n技術スタックとしては，普段BigQuery以外は業務でもあまり触れる機会がないGoogle Cloud Platform (GCP) のサービスとOSSを組み合わせてできることをしようと考えています．\nプロジェクト名: teamaya プロジェクトを始めるにあたりプロジェクト名を決めるとワクワクするので，最初に決めることにしました．沖縄好きなので，沖縄の言葉を組み合わせて何か名前を付けたいと考えていて，沖縄県と東京都のハーフである妻にも相談しながら今回の「teamaya」と言う名前にしました．\ngithubのリポジトリは以下になります．（スター貰えると泣いて喜びます😂）\n teamayaは\u0026quot;team\u0026ldquo;と\u0026rdquo;maya\u0026ldquo;を組み合わせた造語です．teamは「〔動物に引かせて～を〕運ぶ、運搬する」という意味があり，maya（マヤー）は沖縄の方言で猫という意味があるので，猫にデータを運んで貰うという意味を込めてこの名前にしました．\nちなみに，リポジトリにあるアイコンのハイビスカス🌺を付けた猫😸は妻に書いて貰いました！\nどういったプロジェクトなのか 今のところ考えているのは，大きく2つになります．\n データ連携  ローカルのデータをクラウド上に連携 連携したデータをELTツールで変換 \u0026amp; データの品質管理 データの可視化   機械学習システムの構築  パイプライン構築 実験管理 モニタリング \u0026amp; 通知    まず，データ連携については．ローカルからクラウドへの連携，特に特定のデータソースからBigQueryに連携する部分を実施して，データウェアハウス（DWH）を作ろうと思っています．そこからELTツールでデータマート（DM）を作成し，BIツール（Data Studioなど）でデータの可視化を行うといった流れを検証しようと考えています．\n個人的には，DataformやdbtといったサービスでELTパイプラインを作って，データ変換やデータの品質管理を一通り試してみたいと思っています．\n機械学習システムの構築に関しては，機械学習モデル作成のためのパイプラインや実験管理などを行いつつ，コードのテストやデータ・予測結果のモニタリングなどをできるようにしたいのと，Feastなどのfeature storeを試したりもしたいなと思っています．\n現状では上記2つを構築していきたいと思ってますが，進めていく中で興味が湧いたものを適宜取り入れていきたいなと思っています．\nおわりに 今回はプロジェクトを新しく始めたので，その紹介になります．ローカルからクラウドにデータ連携する部分については，既に作成しているので別途内容を紹介したいと思います．\n試行錯誤しながら面白そうなものは色々と取り入れて進めて行きたいと思うので，もし面白いツールなどがあればTwitterで教えて頂ければ嬉しいです．\n参考  MLOps.toys  ","date":"2021-12-12","permalink":"https://masatakashiwagi.github.io/portfolio/post/personal-project-teamaya/","tags":["Dev","Poem"],"title":"teamayaという個人プロジェクトをはじめてみた"},{"content":"はじめに Buy Me a Coffeeというコーヒー1杯をクリエイター活動のサポートという形で寄付するサービスがあります．このサービスの存在は以前から他のエンジニアの個人ブログなどで知っていたのですが，@hurutoriyaさんが投稿されたこちらの記事を見て，僕も同意することが多かったので，自分の個人ブログでも導入してみたというお話です．\nクリエイターの活動をサポートするサービス Buy Me a Coffee以外にも最近は色々とクリエイターの活動をサポートする寄付サービスがあるので紹介しておきます．\n Buy Me a Coffee: コーヒー1杯（実際には$5）をクリエイター活動のためにサポートするサービス Ko-fi: 名前の通り，サポートしたい人に対してコーヒー代を送るというサービス OFUSE: 1文字2円でOFUSEレターやOFUSEコメントを書いてサポートするというサービス  Buy Me a Coffeeを導入しようと思った理由 @hurutoriyaさんが書かれた内容と概ね一緒ですが，お金が欲しいからというわけではなく（お金が欲しいなら広告を貼ると思います），感謝を伝える1つの手段として，コーヒーを1杯プレゼントするよーという表現が良いなと感じています．\nいいね！なども記事を書いた側からすると読んでくれた人からの嬉しいリアクションの1つだと思います．ただ，いいね！以上に読んだ人にとって価値がある技術記事などに対しては，このようなサポートで感謝を伝える方法があってもいいのかなと思います．サポートして貰った側からすると，誰かのためになってるという感覚をより感じることができますし，何よりもっと有益な内容を届けていこうというモチベーションにも繋がるなと思います．\nこれらの理由と僕もより良い記事を届けていきたいという想いから，今回Buy Me a Coffeeを導入してみようと思いました．\nおわりに 実装については，こちらのサイトからロゴやグラフィックがダウンロードすることができます．また，挿入する文字やそのフォント・色など自由にカスタマイズできるGenerateページがあり，そこからGenerateを実行すると，HTMLに使用できるimage codeが出来上がるので，それをコピーして簡単にサイトに導入することができます．\n今回は，Buy Me a Coffeeのサポートセクションをページ最下部に導入して，その導入したお気持ちを書いたポエムになります．\n参考  投げ銭サービスのBuy me a cofee をBlog に導入してみた コーヒー1杯で支援するサービス「ko-fi」と、開発者がコーヒーを奢られる仕組みの話☕ さまざまな収益化機能をひとつに。クリエイター応援プラットフォーム「OFUSE（オフセ）」誕生。  ","date":"2021-12-07","permalink":"https://masatakashiwagi.github.io/portfolio/post/implement-buy-me-a-coffee/","tags":["Poem"],"title":"Buy Me a Coffeeをブログに追加したお話"},{"content":"はじめに 普段はPythonのSciPyというライブラリを用いて，ABテスト実施後の2群間における有意差を調べるために検定を行っていますが，Pythonを使っていないorこのようなライブラリに触れたことがない人でも簡単に検定が行えるようにスプレッドシートを使ってMann-WhitneyのU検定を実施したものになります．\n公開中のスプレッドシート → Mann-Whitney U-test in spreadsheet\nコピーしてご自由にお使い下さい（全自動でない部分があるので，ご留意下さい）．\nMann-WhitneyのU検定とは Mann-Whitney（マン・ホイットニー）のU検定（ウィルコクソンの順位和検定）とは，2つの母集団が特定の分布であることを仮定しないで，2つの分布の重なり具合を検定します．（ノンパラメトリック方法の一つ）\nこれは，2つの母集団の中央値の差に注目しています． → こちらの記事を見ると，中央値の検定というわけではないみたいです．（自分も誤って理解していました）\nU検定の特徴としては，外れ値の影響を受けにくいなどが挙げられます．一方でよく使われるt検定の場合，平均値を見ているので外れ値があるとその影響を受けます．そのため，外れ値除去などの対処が必要なケースが発生します．\nここで，U検定には以下の仮定があります．\n 2つの母集団は互いに独立 2つの母集団の分布が正規分布であると仮定できない 2つの母集団のサンプルサイズが同数でなくても良い  また，帰無仮説は以下になります．\n 帰無仮説: 2群間に差がない（2つの分布が等しい）  検定を行う手順 検定を行う手順を紹介します．まずA群とB群の2つの群を考えます．\n それぞれのサンプルサイズをn1\u0008, n2とした場合に，2つの群を混ぜたデータ(n1+n2)を用意します． 1のデータを昇順に並び替えます． 並び替えたものに対して，順位を割り当てます（ランク付け）．もし同順位を持つ要素が存在する場合は，順位の平均を計算し，その順位の平均を各要素に割り当てます． A群に属するサンプルの順位和を計算する（=R1） 同様にB群に属するサンプルの順位和を計算する（=R2）  ここまで計算すると，検定統計量（U値）は以下になります．\n$$ U_1 = n_1n_2 + \\frac{n_1(n_1 + 1)}{2} - R_1 $$ $$ U_2 = n_2n_1 + \\frac{n_2(n_2 + 1)}{2} - R_2 $$ $$ U = min(U_1, U_2) $$\nUが計算できたら，Mann-Whitney検定表を用いて有意差5%で棄却できるかどうかを確認します．\nα=0.05の表を眺めて，今回のサンプルサイズn1, n2に該当する値と計算したU値の大小関係を比較して，計算した値が小さい場合には，帰無仮説を棄却します，つまり有意差ありとなります．逆に計算した値の方が大きい場合には，帰無仮説を棄却できないので，有意差なしとなります．\nここで，サンプルサイズがn1\u0026gt;20またはn2\u0026gt;20の時は，検定統計量Uを標準化してz値を求めて，標準正規分布で近似する方法を用います．平均値，標準偏差，z値は以下の計算式で求めます．\n$$ \\mu_u = \\frac{n_1n_2}{2} $$ $$ \\sigma_u = \\sqrt{\\frac{n_1n_2(n_1+n_2+1)}{12}} $$ $$ z = \\frac{U - \\mu_u}{\\sigma_u} $$\nz値が計算できたら，標準正規分布表を用いて，該当するp値を見に行きます．\n(z値をスプレッドシートの組み込み関数であるNORMSDISTに代入して，1から引くことでp値を計算しています: 式=1-NORMSDIST(z))\n p≧αの時，帰無仮説を棄却できない p\u0026lt;αの時，帰無仮説を棄却する．つまり，有意差ありとなる  スプレッドシートでU検定を行う 公開しているスプレッドシートはこちらになります．（再掲）\nサンプルデータとしてグループABの身長のデータを載せています．こちらのデータを検定したい2群のデータに適宜変更して頂くと，#検定を行う手順 で紹介した方法に則ってp値の計算がされます．\nサンプルサイズがn\u0026lt;=20の場合は，U値での評価になるので，その場合はリンクにあるマン・ホイットニーのＵ検定表を用いて，該当する値から検定結果を見積って貰うと良いです．\n※ 補足:\nサンプルデータのデータ順序は意識しないで問題ありません，順序並び替えで自動的に昇順で並び変わります．ただし，順位については，スプレッドシートのマウスオーバーでセルの右下に表示される黒い部分をデータが存在している部分まで下にズラして貰う必要があります．（スプレッドシートを完璧に使いこなしているわけではないので，特に順位をつけてる部分が自動化できていないです．もしご存知の方は，方法を教えて貰えると大変助かります🙏）\nおわりに 今回は，スプレッドシートを使ってMann-WhitneyのU検定を試してみた内容になります．Pythonを使っていない非エンジニアの方でも検定を行えるようにスプレッドシートに実装しました．ABテストを実施して検定するまでを誰でも簡単にできるようになれば良いなと思ってます．実装していて，スプレッドシートって意外と組み込みの関数が用意されていることを改めて知ることができました😄\nまた，普段ライブラリを何気なく使っていますが，内部の計算方法やどうゆう手続きで出力されるのか，またその値をちゃんと理解して使っていかないとなーということを改めて感じました．例えば，scipy.stats.mannwhitneyuはp値を出すだけであれば良いが，U値を使いたい場合には少し使いづらいと感じました．\nP.S. t検定についてもスプレッドシートで実施できるようにしているので，また紹介したいと思います．\n参考  Mann–Whitney U test Mann-Whitney Table 標準正規分布表 スプレッドシート - NORMSDIST Divine, et al. (2018) Mann-Whitney検定は中央値の検定ではない  ","date":"2021-11-16","permalink":"https://masatakashiwagi.github.io/portfolio/post/mann-whitney-utest-in-spreadsheet/","tags":["DataScience","Statistics"],"title":"スプレッドシートで行うMann-WhitneyのU検定"},{"content":"はじめに AWSのSageMaker上でSageMaker Python SDKを使用して独自の機械学習モデルを作成することができますが，その際に学習や評価が行える Estimator というSageMakerのinterfaceがあります．\n一方で，SageMaker Experimentsで実験管理を行いたい場合には，このEstimatorに色々と渡してあげる必要があります．\nその中でも学習時に出力されるlossの値や評価メトリクスを記録するためには，Estimatorのmetric_definitionsに正規表現を記述してログから上手く取得する必要があります．\nこれをより簡単にするために，CustomCallback関数を作成した話になります．\nCallback関数のカスタマイズ Tensorflow，厳密にはKerasのCallback関数をカスタマイズします．tf.keras.callbacks.Callbackクラスを継承したCustomCallBack(tf.keras.callbacks.Callback)クラスを作成します．この作成したクラスをmodel.fit時に引数のcallbacksに渡してやることで使用することができます．\n今回はSageMaker Experimentsで使うことを想定したもので，Estimatorのmetric_definitionsに渡すRegexとして，以下のようなログが出力されて欲しいとします．（メトリクスはRMSEとした場合を想定） MetricDefinitionsはこちらが参考になります→Define Metrics\nsagemaker.estimator.Estimator( ..., metric_definitions={ {'Name': 'Train Loss', 'Regex': 'train_loss: (.*?);'}, {'Name': 'Validation Loss', 'Regex': 'val_loss: (.*?);'}, {'Name': 'Train Metrics', 'Regex': 'train_root_mean_squared_error: (.*?);'}, {'Name': 'Validation Metrics', 'Regex': 'val_root_mean_squared_error: (.*?);'}, } )  しかしながら，Kerasでモデルを作成する際のデフォルトでは，学習時のLossはloss，メトリクスはroot_mean_squared_errorでprefixが無い状態になります．これをCallback関数をカスタマイズすることでprefixにtrain_を付けて，Regexで簡単に取得したいという気持ちです．\nTensorflow公式ドキュメントのWriting your own callbacksやtf.keras.callbacks.Callbackを参考に作成しました．\nimport tensorflow as tf class CustomCallback(tf.keras.callbacks.Callback): def on_train_begin(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of training. \u0026quot;\u0026quot;\u0026quot; def on_train_end(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of training. \u0026quot;\u0026quot;\u0026quot; def on_epoch_begin(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the start of an epoch. \u0026quot;\u0026quot;\u0026quot; def on_epoch_end(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of an epoch. \u0026quot;\u0026quot;\u0026quot; def on_train_batch_begin(self, batch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of a training batch in fit methods. \u0026quot;\u0026quot;\u0026quot; def on_train_batch_end(self, batch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of a training batch in fit methods. \u0026quot;\u0026quot;\u0026quot;  上記コードの中で必要なものを修正すれば大丈夫です．今回は学習の開始終了とエポックの終了時に呼ぶように修正しました．\nimport tensorflow as tf import datetime class CustomCallBack(tf.keras.callbacks.Callback): def on_train_begin(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the beginning of training. \u0026quot;\u0026quot;\u0026quot; print(f\u0026quot;Start training - {str(datetime.datetime.now())}\u0026quot;) # get parameters self.epochs = self.params['epochs'] def on_train_end(self, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of training. \u0026quot;\u0026quot;\u0026quot; print(f\u0026quot;Finish training - {str(datetime.datetime.now())}\u0026quot;) def on_epoch_end(self, epoch, logs=None): \u0026quot;\u0026quot;\u0026quot;Called at the end of an epoch. \u0026quot;\u0026quot;\u0026quot; keys = list(logs.keys()) metrics_values_list = [] for key in keys: if key.startswith('val_'): metrics_values_list.append(f\u0026quot;{key}: {logs.get(key):.4f};\u0026quot;) else: metrics_values_list.append(f\u0026quot;train_{key}: {logs.get(key):.4f};\u0026quot;) values = ' - '.join(metrics_values_list) print(f\u0026quot;Epoch {epoch+1}/{self.epochs} - {values}\u0026quot;) # fit時にcallbacksに作成したカスタマイズクラスをインスタンス化したものを渡す model.fit( ..., callbacks=[CustomCallBack()], )  出力は以下のような感じになります．今回はprint文で出力していますが，loggerを用意してlogger.infoを使うのも良いかと思います．\nStart training - 2021-11-09 23:48:12.787257 Epoch 1/10 - train_loss: 4.6889; - train_root_mean_squared_error: 2.1654; - val_loss: 11.1416; - val_root_mean_squared_error: 3.3379; ... Finish training - 2021-11-09 23:48:15.095133  おわりに 今回はSageMaker Experimentsで実験管理を行う上でログ出力の形を修正したいという動機からCallBack関数をカスタマイズしました．Callback関数の中身を知るためにソースコードを読んだりして勉強になりました．Tensorflowのフレームワークは拡張性があり，カスタマイズの方法もドキュメントに整備されているので，比較的容易に修正できると思います．今回は時間経過や予測時間の表示は省いてしまったので，余裕があればログにこれらを出力するようにしていきたいです．\n参考  Sagemaker Training APIs - Estimator Writing your own callbacks tf.keras.callbacks.Callback  ","date":"2021-11-10","permalink":"https://masatakashiwagi.github.io/portfolio/post/customize-tf-callback/","tags":["Dev","MachineLearning"],"title":"TensorflowのCallback関数をカスタマイズ"},{"content":"はじめに 6月から友人の@navitacionさんと一緒にPodcastの配信を開始しました．テック系の話題など自分たちが興味のある内容をあれこれと話すものになります．\nAnchorという無料で配信できるプラットフォームを使用しており，チャンネル名は「Double-M2.fm」になります．\n 各Episodeの要約もありますので，興味を持って頂いた方はこちらから見ることができます．\n この記事では，naviさんと始めたキッカケと実際の配信方法をまとめています．配信方法はあまり検索しても記事が無かったりしたので，これから始めようとしている人の参考になればと，そしてPodcast配信者が一人でも多く増えればなーと思います．\n配信のキッカケ 配信のキッカケは初回配信時にもチラッと触れたのですが，大きく2つ（+1）あります！\n コロナ禍で，オフラインイベントが無くなったことで，イベント後に参加者同士で雑談したり，情報交換したりする機会が無くなってしまったので，それを定期的にしたいという想いから アウトプットを意識するようになって，それを継続的に行っていく場の一つとして，音声による方法もあるのではという想いから 話のタネとしていいかなと笑  一つずつ説明していくと， まず①について，これは思っている人が多いんじゃないかなと思いますが，コロナ禍でオンラインでのイベントは多く開催されており，オフライン時に比べたら移動する手間もなくハードルが下がって，参加人数の制限も実質無制限で，抽選漏れの懸念も無くなりと良いことも多くある一方で，イベント後に登壇者以外の人同士や登壇者との会話がほぼほぼ無くなって，雑談とか情報交換したりする機会がかなり減ったと個人的に感じています．\nそういった状況で，会社の人以外の人と雑談など会話する時間が圧倒的に減ったというのもあり，定期的に情報交換したり意見を言い合える場があると良いなーということで今回Podcastを始めました． （最近だと，Twitterが音声会話サービスのSpaceを始めて，そこで気軽に会話が生まれるのがスゴく良いなと感じてます！）\n②については，かなーーりサボり気味だったのですが，アウトプットちゃんとして行かないとなーという気持ちが高まり，それを強制的に行う一つの方法だったりもします！ Podcastで話すために，何かしらの話題を探したり，書籍・論文などを読んで調べたりとトピックの内容を整理する必要があると思います．また，内容を自分の言葉で相手に説明することで理解の助けになると思っています．（人に説明してみると，思ったよりわかってないなーと感じることあるあるだと思います）\nアウトプットの方法の一つとして，ブログにまとめるということ以外に音声でもやろうかなといったところです． あと，普段から「これ！Podcastで話せるかも」とった意識をするだけで理解の仕方が変わると思うので，この習慣を付けたい狙いも個人的にあります！\n最後に③について，これはTwitter上ではお互い認知していても，リアルだと会ったことない人との話のネタの一つにしようかなという魂胆です笑\n配信方法 配信するプラットフォームですが，これは僕が聞いている他のPodcast配信者のを参考にして，Anchorにしました．Anchorは携帯でも簡単に録音することができて，それを配信できちゃったりもします．\n以下が僕たちが配信している各種構成になります．\n※ 全て無料で行うために，これらの構成になっています，有料でもいい場合はここで紹介したような複数ツールを使わなくて済むと思います．\n メール: Gmail 配信プラットフォーム: Anchor 録音: Zencastr 画面共有: Google Meet スクリプト: Scrapbox + GitHub 編集: Audacity 音楽 (BGM): Evoke Music アイコン: Canva  1. メール  各種サイトにアカウント登録する際にはメールアドレスが必須なので，まず共通で利用するメールアドレスを取得しました． 簡単に作成できるので，Gmailを新規作成し利用しています．  2. 配信プラットフォーム  最初にも書きましたが，配信プラットフォームはAnchorを採用しています． アカウントを作成し，Podcast name, descriptionなどを設定するだけで大丈夫です．無料で使うことができるのでおすすめです！ New Episodeから録音したり，既にある音源をアップロードすることも可能になっています．また，BGMなども用意されています． ただ，遠隔地にいる人同士での複数人録音は出来なさそうだったので，僕たちは録音は後述の別ツールを使うことにしました． 音源をアップロードすると，スケジュールでの投稿予約ができます．そして，最初のEpisodeが登録されると，自分たちだけのPublic Siteが生成され，そこで新しいPodcastを聴くことができます．  WHERE TO LISTENの部分ですが，新しいPodcastが配信されたら，RSSのクローラーが検知して，色んなアプリで聴くことができます！ただし，SpotifyとApple Podcastsに関しては，RSSのURLを登録する必要があります．詳しくはこちらのnote記事が参考になるかと思います．ちなみに，RSSのURLは配信が開始したら表示されるSettings-\u0026gt;Distributionから確認することができます．\n3. 録音 録音はAnchor上では行わず，Zencastrというツールを使いました．またZencastrは複数人の録音も行うことができ，非常に便利です！\n プランは無料のHobbyistと有料のProfessionalがあります．僕たちは無料プランを使っていますが，制限としては以下になります．\n 無料プランの制限：  1．ゲストは2名まで 2．1ヶ月あたり8時間の録音時間    基本的に二人での配信かつ週1回30分程度の録音時間なので，無料プランで全く問題ありません．また，Zencastrでは画面の録画もできるみたいです．（※コロナの期間は，無料プランでもゲストと録音時間が無制限になっているみたいです）\nこちらもAnchor同様，アカウント作成後，Create New Episodeで新規作成を行うと画面と音声の録画・音声録画（画面表示あり）・音声録画のみの3パターンから選択します．\nタイトルを入れて作成すると，下記のような画面が出ます．\nここで，Inviteタブをクリックすると同時に会話する人のメールアドレスにリンクを送付する形で招待することができます．Inviteした人が入ってくると，そのまま通話状態になり録音を開始することで，そのまま両方の音声を録音することができます．\nあとは，録音終了後参加していた人の音声をMP3でダウンロードすることができます．（これは管理者のみ可能な操作）\n僕たちは使わなかったですが，Macでネット通話の音声を録音する方法（Soundflower, LadioCast, GarageBand）の記事を紹介しておきます．→ Macでネット通話の音声を録音する方法（Soundflower, LadioCast, GarageBand）\n4. 画面共有 スクリプトなり資料なりを見ながら会話がすることが多いので，画面共有が必要になってきます．そのために，Google Meetを使って画面共有しながら会話するようにしています．\n5. スクリプト（台本・要約など） 収録を行う前に，事前に30分程度会話して何を話すか決めています．その際に，Scrapboxを活用して台本を作成したり，ネタ帳なども雑多に書いています．あとは，共有しておきたいことをScrapboxに書いて基本的にはここを見ながらいつも収録しています．\n参考にした記事 → 初めてのポッドキャスト、試して気づいた「声」の醍醐味と9つのティップス\nScrapboxの他には，GitHubも使ってます．こちらは，Organizationを作成し，そこにPodcast用のrepositoryをさらに作成して，自己紹介ページや各配信の収録内容の要約を書き記しています．ページの更新時にはissueを作成し，何をしたか記録が残るようにしています．\n6. 編集 編集に関しては，特別何かしているわけではないです．なるべく時間をかけず無理なく進めていきたいと思っているので，行っている内容としては下記2点です．\n 音量調整・BGM挿入 収録中に予期しない割り込みが入って，収録を止めた際のトリミング・不要な会話の削除  一通り自分たちの会話を聞いて，気になる部分があればトリミングなどしているぐらいになります．\n編集ソフトはAudacityを使っていて，昔からある音楽編集ソフトになります．Windows・Macどちらも使用することができます． Macの場合だと初めから入っているGarageBandなども使えるみたいです！\n7. 音楽 (BGM) Podcastを配信する時には，自分たちの音声に加えてBGMを追加しています．ただ，自分たちのコンテンツにBGMを使用する場合には，著作権などが絡んでくるので，安易に好きな曲を使用することはできません．楽曲1つ1つ確認するのは大変なので，今回は著作権フリーでAIが曲を作成してくれるEvoke Musicというサイトの音源を使用しています．\nこのサイトでは，キーワードを指定することで，そのキーワードに合った曲をAIが自動で作成してくれます．1曲あたり数分程度の曲になります．\n※ β版の時は無料で使えてましたが，今は有料になってしまったみたいです．\n8. アイコン（カバーアートなど） Anchorにはカバーアートが設定できるので，そのアイコンなどを作成するために，Canvaというデザインサイトを利用しました．Canvaは豊富なテンプレートデザインがあるので，それをベースに自分でいい感じに編集するだけでかなりオシャレなロゴなどを作成することができます．\n無料プランだと保存時の画像サイズや解像度などを変更できないので，少し残念ですが，かなりオススメのサイトです．有料だともっと出来る幅が増えそうですが，現状だと無料プランでも十分かなと思っています．\nおわりに 今回は6月から開始したPodcastについて，やろうと思ったキッカケとその配信方法をまとめました．配信方法については意外と記事がなかったので，もしこれから配信しようと考えている人の参考になれば嬉しいです！\n今回紹介した方法以外にも，もっと良い配信方法があると思うので，ご存知の方は是非教えて欲しいです！！一人でも多くのテック系Podcastが増えて盛り上がると良いなと思います！！\nP.S. 細かい設定や登録など聞きたいことがある場合には，遠慮なくTwitterなどでご連絡頂ければと思います．\n","date":"2021-06-13","permalink":"https://masatakashiwagi.github.io/portfolio/post/podcast-broadcast-method/","tags":["Poem"],"title":"Podcastによる配信のキッカケとその方法"},{"content":"Kaggle-Shopeeコンペの振り返り 2021/03/09~2021/05/11まで開催していたShopeeコンペの振り返りになります．\n2週間程度しか手を動かせなかったですが，久しぶりに参加したので備忘録として記録を残しておきます．\n最終的な結果は179th/2464で銅メダルで，特に凝ったことは何もしていなかったので，妥当かなと思います．\nこのコンペは上位10チーム中7チームが日本人チームで，日本人のレベルの高さを改めて実感できるコンペでした！\n概要 コンペの内容は簡単に言うと，画像とテキスト情報を用いて、2つの画像の類似性を比較し，どのアイテムが同じ商品であるかを予測するコンペになります．\n 開催期間: 2021/03/09 ~ 2021/05/11 参加チーム数: 2464 予測対象: posting_id列にマッチする全てのposting_idを予測する．ただし，posting_idは必ずself-matchし，グループの上限は50個となっている． データ: 投稿ID，画像，商品のタイトル，画像の知覚ハッシュ(perceptual hash)，ラベルグループID 評価指標: F1-Score その他: コードコンペ  My Solution  画像特徴量・テキスト特徴量・画像のphash値をconcatして結果をユニーク化したものを最終的な予測値としました． 何も複雑なことはしていないモデルですが，結果的に銅メダルを取ることができました．  Image Model  eca_nfnet_l0  loss: ArcFace pooling: AdaptiveAvgPool2d scheduler: CosineAnnealingLR loss(criterion): CrossEntropyLoss size: 512*512   eca_nfnet_l1  loss: CurricularFace pooling: MAC scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512   efficientnet_b3  loss: ArcFace pooling: GeM scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512   swin_small_patch4_window7_224  loss: CurricularFace scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 224*224   common  augmentation by albumentations.  HorizontalFlip VerticalFlip Rotate RandomBrightness   optimizer  Adam      少し工夫したポイント  画像特徴量を抽出する部分で，いくつか工夫した点をあげます．  CNN Image Retrievalを参考にしました．     lossにArcFaceとCurricularFaceを用いた  ArcFaceを使っている人が多かったが，CurricularFaceも使いました．スコア的にはCurricularFaceの方がよかったです．   いくつかのモデルでpooling層にGeMとMACを用いた  Google Landmark Retrieval Challengeで上手くいっていたGeMやMACなどのプーリング手法を用いました．   loss(criterion)にFocalLossを用いた 最終的に得られた特徴量をconcatした後，ZCAWhiteningによる次元削減を行った  有効でなかったもの  一方で，上手くいかなかった内容としては以下になります．  resnext50_32x4d swin_small_patch4_window7_224 with ArcFace CosFace, AdaCos PCA Whitening (worse than ZCAWhitening)    Text Model  paraphrase-xlm-r-multilingual-v1  loss: ArcFace scheduler: linear schedule with warmup loss(criterion): CrossEntropyLoss optimizer: AdamW   TF-IDF  textの方のモデルは特に改良する時間が取れなかったので，ほとんど手を付けれてなかったです．\n transformerとTF-IDFで得られた特徴量それぞれに対して，Cosine Similarityを計算し，テキストのpredictionを作成しました．  最終的な予測値は画像特徴量とテキスト特徴量に加えて，画像のphash値を追加して，ユニークを取った値としています．\n反省  post-processingが全然できていなかった  他の解法を見るに，post-processingでスコアが伸びているので，この部分は結構大事だったんだなと感じています． 6位の解法にもありましたが，今回のコンペでは，label_groupの長さが2以上であることから，「予測した結果のposting_idが1つしかない場合，強制的に似たものを持ってきて，2つにする」というアイデアでLBがかなり上がるみたい   ImageとTextをMulti-modal的にモデルに組み込んで学習することができていなかった グラフ理論全然わかってないです笑 細かい部分  他のoptimizerを試す  SAMとか   Database-side feature augmentation (DBA) / Query Extension (QE)  全然知らなかったので，End-to-end Learning of Deep Visual Representations for Image Retrievalを読んで勉強したい   閾値の調整 テキストモデルの追加     ここからは上位の解法を書きたいと思います．数もそれなりにあるので，載せるのは上位5つにします．最後に上位5つ以外にもDiscussionsに投稿されている解法のリンクを載せておきます．\n1st Place Solution 解法はこちらになります: 1st Place Solution - From Embeddings to Matches\nModel  Image: 2つのモデル  eca_nfnet_l1 * 2   Text: 5つのモデル  xlm-roberta-large xlm-roberta-base cahya/bert-base-indonesian-1.5G indobenchmark/indobert-large-p1 bert-base-multilingual-uncased   loss: ArcFace poolingした後にbatch normalizationとfeature-wise normalizationを行った ArcFaceのチューニングを行った  学習段階で徐々にmarginを大きくした（埋め込み表現のqualityに影響する）  画像に対するmargin: 0.8~1.0 テキストに対するmargin: 0.6~0.8   marginを大きくすると，モデルの収束に問題が出たので，以下を行った  warmup stepsを大きくする cosineheadに対するlearning rateをより大きくする gradient clippingを行う   class-size-adaptive marginも試したが，不均衡性が小さかったため，改善は少しだけだった  image: class_size^-0.1 text: class_size^-0.2     global average poolingの後にFC層を追加するとモデルが悪くなるが，feature-wise normalizationの前にbatch normalizationを追加するとスコアが改善した  Features  Combining Image \u0026amp; Text Matches  マッチングさせる方法をいくつかトライした   画像のembeddingによるマッチングとテキストのembeddingによるマッチングを結合する 画像のembeddingとテキストのembeddingをconcatしてからマッチングする 1と2を組み合わせてマッチングする   これは，画像のembeddingが強く示唆するアイテム・テキストのembeddingが強く示唆するアイテム・画像とテキストのembeddingがどちらも適度に示唆するアイテムを取り入れることができる     Iterative Neighborhood Blending (INB)  QE/DBAとは少し異なる，embeddingからマッチする商品を検索するパイプラインを作った k-Nearest Neighbor Search:  近隣探索ラリブラリ: faiss (k=51: 50(自身以外)) + 1(自身))   Threshold:  コサイン類似度をコサイン距離(=1-コサイン類似度)に変換し，distance\u0026lt;thresholdを満たす(matches, distances)のペアを取得した 閾値処理を行う場合，1つのクエリに対して少なくとも2つ以上のマッチがあることがコンペで保証されているので，distanceがmin2-thresholdを超えた場合にのみ，二番目に近いマッチを除外する   Neighborhood Blending:  kNNによるサーチとmin2による閾値処理をした後，各アイテムの(matches, similarities)ペアを取得し，グラフを作成する  各ノードはアイテム，エッジの重みは2つのノード間の類似性を示す 近傍のみが繋がっており，閾値条件とmin2条件を満たさないノードは切断される   近傍のアイテムの情報を使いたいので，クエリアイテムのembeddingを改良し，よりクラスタを明確にする  重みとして類似性を持つ近傍のembeddingを加重合計し，それをクエリembeddingに追加する    上図を簡単に説明すると，最初Aは[B,C,D]と繋がっているが，加重合計した結果閾値=0.5の場合，Cとの接続が切れて，Aは[B,D]のみと接続していることがわかる このNBの処理を評価指標の改善が止まるまで繰り返し実行する 最終的なNBのパイプラインは以下になる    About Threhsold Tuning:  調整する閾値は全部で10種類ある  stage1のtext, image, combinationの閾値が3つ stage2の閾値が1つ stage3の閾値が1つ 直接最終結合部分に繋がるstage1のtext, imageの閾値が2つ stage1~3のmin2閾値が3つ   最終的にはstage2,3の閾値を2つ調整するだけでよかった     Visualizations of Embeddings before/after INB  INBの効果を可視化したノートブックが公開されている  (SHOPEE) Embedding Visualizations before/after INB 見るからに各点が凝集されたクラスタを形成していることがわかる      Others  画像に対するCutMix(p=0.1) 画像のaugmentation  horizontal flipのみがよかった   madgrad optimizer: https://github.com/facebookresearch/madgrad 学習データ全体を使った学習  2nd Place Solution 解法はこちらになります: 2nd place solution (matching prediction by GAT \u0026amp; LGB), 2nd Place Solution Code\nSummary  1st stage: 画像・テキスト・画像+テキストデータに対するコサイン類似度を得るためにMetric Learningによるモデルを学習する 2nd stage: 同じlabel groupに属するアイテムのペアかどうかを識別するために\u0026quot;メタ\u0026quot;モデルを学習する  Model  image: 2つのモデル  backbone  nfnet-F0 ViT   loss: CurricularFace optimizer: SAM embeddingの結合: F.normalize(torch.cat([F.normalize(emb1), F.normalize(emb2)], axis=1))   text: 3つのモデル + TF-IDF  indonesian-bert multilingual-bert paraphrase-xlm   image+text: nfnet-F0とindonesian-bertのFC層のembeddingを結合したもの Training Tips:  label groupのサイズに基づいたSample Weighting  1 / (label group size) ** 0.4      Features Graph features  それぞれのアイテムのtop-Kコサイン類似度の平均と分散  K=5, 10, 15, 30, etc   標準化（mean=0, std=1） Pagerank  Others  テキストの長さ #の記号 Levenshtein距離 画像ファイルのサイズ 画像の高さと幅 Query Extension  Ensemble Methodology  ローカルデータでそれぞれのモデルのベストな閾値を計算する 上記閾値でそれぞれのモデルの予測値を差し引く 差し引かれた予測値を合計する 合計値\u0026gt;0の場合，アイテムペアが同じグループに属するとする お互いにエッジを持たないペアを削除する  A-Bはあるが，B-Aがない場合は両方を削除する    Post-Processing  中間中心性が最も高いエッジを再帰的に削除する（Graph-Based）  Others  Performance and Memory Tunings  CuDF, cupy, cugraph: GPUを有効に使うためには大事 ForestInference: 40分かかるCPUでの推論が2分になる   Not worked  Graph-basedの特徴量 固有ベクトルの中心性とjaccard End-to-end model Local feature matching    3rd Place Solution 解法はこちらになります: 3rd Place Solution (Triplet loss, Boosting, Clustering)\nModel  image: 2つのモデル  backbone  efficientnet_b2 ViT (DINO)     text: 2つのモデル (異なるtokenizersとCLIP)  indonesian-bert multilingual-bert   common  loss: TripletLoss (margin=0.1) 各エポック，label_groupsから重複したペアを作成し，各バッチでlabel_groupsから1ペアが挿入される．バッチサイズは128を使っていたため，バッチ毎に64の重複を取得し，ランダムな5点でそれらの各々を比較する 5fold CVで，validationスコアを計算する時は，validation-foldの中から候補を選ぶのではなく，学習サンプル全体から候補を探した  この方法はCV/LBのgapを抑えて，相関を取ることが出来る oofを用意して，2nd-levelの予測に使用する CVから得られた5つのモデルで，テストデータに対して推論を行った より速く推論するためには，validationなしの1つのモデルでテストデータにfitさせること   ArcFaceは上手くいかなかった    Features  複数モデルを用いて，異なる表現方法を作成するのが良い 全てのembeddingsから候補となる近しいものを結合し，ペアが実際に重複しているかどうかにかかわらず、binary targetを使用してペアオブジェクトのサンプルを作成する  3M点のペアがあり，重複率は4%だった   これらに対して，GBM(=CatBoost)を作成し，embedding毎に計算する  pairwise-distances (コサイン類似度，ユークリッド距離など) 両方の点周辺の密度 points ranks   最終的には500個特徴量を作成し，CVやLBを使いながら，重複確率による閾値の候補を出す\u0008 -\u0026gt; LB:0.76+  Post-Processing  クラスタリングを行ったが，embeddingsを使うのではなく，pairwise-distanceを使う 重複推定のためにGBM(=CatBoost)の確率を使い，類似度を求める 凝集クラスタリングのアイデアを採用  各単一点をクラスタとして開始し，平均的なクラスタサイズが閾値に等しくなるまでそれらをマージしていく クラスタが単一の場合，最も近傍のクラスタにマージする -\u0026gt; LB:0.79    Others  最適化するために  half precision(torch AMP)を使って学習と推論を行った 画像モデルの場合，画像の読み込みとリサイズにNVIDIA DALIを使った GBMの推論には，Rapids ForestInference Libraryを使った   上記の方法を使わないと，2時間で全てのモデルを推論することは不可能だった  4th Place Solution 解法はこちらになります: 4th Place Solution\nModel  image: backboneにnfnetとefficientnet  pooling: GeM＆Avg pooling embeddingが大きいほど，LBのスコアがよくなった loss: ArcFace marginの調整が大事だった   text: BERTベース + TF-IDF  TF-IDF: かなり大きいembeddingを生成し，notebook上ではメモリの制約を受けるため，Random Sparse Projectionで次元削減を行った loss: ArcFace marginの調整が大事だった    Ensemble  画像のembedding・テキスト（BERTベース）のembedding・TF-IDFのembeddingを結合して，正規化した これらのベクトル表現のそれぞれに対して，pairwiseコサイン類似度を計算し，3つの行列を作成した  最初に二乗し，その後加重平均を取って行列を結合した    Post-Processing  閾値の調整 Rank2 matching  もし，AがRank2にBを持っており，BはRank2にAを持っている場合，お互いに追加する   Rank2とRank3の違いが大きい場合，Rank2のIDを追加する 少なくとも1つの他とのマッチング（Rank2のコサイン類似度が極端に小さくなければ） Query Expansion マッチしていない行との再マッチング  その他上位解法のリンク  Kaggle ShopeeコンペPrivate LB待機枠＆プチ反省会 5th Place Solution 6th place solution 7th place solution 8th Place Solution Overview Public 16th / Private 10th Solution 11th Place Solution 14th Place Gold - Image Text Decision Boundary 15th place solution Public 13th / Private 16th solution 18th place solution - You really don\u0026rsquo;t need big models 19 place. Voting and similarity chain 26th Place Solution : Effective Cluster Separation and Neighbour Search [31st Place] Object Detection approach Public 39th / Private 37th Solution 48th Place Silver - Simple Baseline Public 56th / Private 57th Solution 62th Place Solution (Stacking Logistic Regression) 72nd place solution  ","date":"2021-05-14","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-shopee-solution/","tags":["Kaggle"],"title":"Kaggle-Shopeeコンペの振り返りとソリューション"},{"content":"はじめに 今まで仕事では，開発環境としてIntelliJを使っていたのですが，最近はVSCodeの人気が高くExtensionsも便利なものが多くあるということで，個人的な作業をする時はVSCodeを使ってみようと思って使っています． そんな中で，タイトルにもあるようにVSCodeからgit pushしようとしたら，\u0026lt;アカウント名\u0026gt;@github.com: Permission denied (publickey).とエラーが出たので，それを解消してVSCodeでgit pushできるようにした備忘録になります．\nエラー原因（SSH接続エラー） Permission denied (publickey)とあり，GitHubにSSH接続するために，公開鍵を登録しておかないといけないのですが，それをしていなかったので，エラーが発生したということになります．\n以下のコマンドを打つことで接続を確認することができます．\nssh -T git@github.com \u0026gt; git@github.com: Permission denied (publickey).  ではどうすればいいかというと，鍵を生成してGithubに登録すればいいということになります．\n公開鍵と秘密鍵の作成 詳細な作成方法はこちらの記事が参考になります．\n簡単に手順を載せておきます．\ncd ~/.ssh ssh-keygen -t rsa -b 4096 -C \u0026quot;\u0026lt;メールアドレス\u0026gt;\u0026quot; -f github_rsa # オプションをいくつか設定して，鍵を生成 # 以下実行結果（一部マスクしてます） Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in github_rsa. Your public key has been saved in github_rsa.pub. The key fingerprint is: SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \u0026lt;メールアドレス\u0026gt; The key's randomart image is: +---[RSA 4096]----+ |=== | |.B o . | |o.. . * . o | |. . . B +. oo .o*| | . o * OSo.oooo*+| | . = + = o ..*..| | E . . o . . ..| | . . | | | +----[SHA256]-----+  鍵の種類をRSAにし，鍵の長さを4096にしています．ファイル名はgithub_rsaと設定しました．\nGithubに生成した公開鍵を登録 cd ~/.ssh ssh-add -K github_rsa # 秘密鍵をssh-agentデーモンに登録 pbcopy \u0026lt; github_rsa.pub # pbcopyコマンドで公開鍵の中身をクリップボードにコピー  この後は，コピーした公開鍵の中身をGithubに登録します．\nGithubのアカウントからSettingsに進み，SSH and GPG keysを選択し，New SSH keyを押して，先程コピーした中身をペーストし，名前を決めて保存します．\nSSH接続確認 保存が完了したら，SSH接続できるか確認するために，以下のコマンドを打って確認します．\nssh -T git@github.com \u0026gt;Hi \u0026lt;ユーザー名\u0026gt;! You've successfully authenticated, but GitHub does not provide shell access.  Remote設定の上書き ここまで来たら後一息で，最後にremoteの設定を上書きします． 以下のような感じでリポジトリ名を書いて，実行すればOK．\ngit remote set-url origin git@github.com:\u0026lt;ユーザー名\u0026gt;/\u0026lt;リポジトリ名\u0026gt;.git  VSCodeからgit push 今までの設定が完了していれば，上記画像の手順でVSCodeの画面から簡単にgitにcommitやpushなどの操作を行うことができます．\nさいごに まだまだVSCode初心者なので，使いやすいExtensionsを取り入れて開発環境をカスタマイズしていきたいと思います！\n参考 初めてのgitは5ステップで完了\nGitHubでssh接続する手順~公開鍵・秘密鍵の生成から~\n","date":"2021-03-26","permalink":"https://masatakashiwagi.github.io/portfolio/post/vscode_git_connect/","tags":["Dev"],"title":"VSCodeとgitを連携してpushできるようにするまで"},{"content":"はじめに Pytorchでモデルを作成していた際に，RuntimeError: CUDA error: device-side assert triggeredが発生して，原因がよくわからなかったので，調べたことをメモしておきます．\nエラー発生の原因 調べてみると，原因としては\n ライブラリのVersionが違う ラベル/クラスの数とネットワークの入出力のshapeが異なる Loss関数の入力が正確でない  などなど\u0026hellip;\nよくあるのが，下2つかなと思います．\nラベル/クラスの数とネットワークの入出力のshapeが異なる 想定しているラベルもしくはクラス数とネットワークの出力のクラス数が異なる場合，この場合はnn.Linear(input, num_class)で合わせてやる必要がある．\nLoss関数の入力が正確でない 僕が遭遇したのはこちらのパターンになります．\n例えば，BCELossを考えた場合，計算するためには値としては0~1を取る必要があります．そのため普通は最終出力にSigmoid, Softmax関数を入れるかと思います．\nそれ以外にもLossの設計で以下のようにしておくと良いかと思います．\nclass BCELoss(nn.Module): def __init__(self): super().__init__() self.bce = nn.BCELoss() def forward(self, input, target): input = torch.where(torch.isnan(input), torch.zeros_like(input), input) input = torch.where(torch.isinf(input), torch.zeros_like(input), input) input = torch.where(input\u0026gt;1, torch.ones_like(input), input) # 1を超える場合には1にする target = target.float() return self.bce(input, target)  他の解決方法 他にも調べていると解決方法としてCUDAの設定を以下にすると良いなどもありましたが，解決するかどうかはよくわからないです．\nCUDA_LAUNCH_BLOCKING=1  おわりに 今回は，Pytorchでのモデル作成時に発生したエラーについて整理しました，モデル作成時にはモデルのIn/OutやLoss関数の定義をきちんと理解し把握しておく必要があると改めて感じました．\n同様のエラーが起きた場合には，この辺りをまずは調べてみるのが良さそうです．\n参考 https://towardsdatascience.com/cuda-error-device-side-assert-triggered-c6ae1c8fa4c3\n","date":"2021-02-01","permalink":"https://masatakashiwagi.github.io/portfolio/post/cuda_error_device-side_assert_triggered/","tags":["Dev"],"title":"RuntimeError: CUDA error: Device-side assert triggeredの解決方法"},{"content":"Kaggle-MoAコンペにTeam 90\u0026rsquo;sで初参加 このブログは2020/9/4~12/1まで開催していたMoAコンペでの取り組みを紹介する内容です． （コンペの詳細な内容については割愛します）\n今回のコンペでは，同世代のメンバーでチームを組んで取り組みました！\nチーム結成の経緯は，Twitterでお互いが90年生まれということを知って，同世代でKaggleチーム組んで戦いたいねーという感じだったと思います． それが少し前のことで当時取り組める良い感じのコンペがなかったのですが，今回テーブルデータのコンペで取り組めそうということで始まりました． チームでの取り組みはとにかく学びが多く，終盤までモチベーションを保つことができたのが大きかったです．\nまた，議論することで理解なども深まっていくので，コンペを通してより成長できたんじゃないかなと思います．\n今回の僕たちのチームでの取り組み方を紹介すると\n1. 情報はSlackで共有 2. 分析方針や実験結果はGithubのissueで管理 3. 毎週末に2時間程度のディスカッション\nといった感じです．\n3番目の週末のディスカッションは強制ではなく，参加可能な人が参加する形式で運用してました． （と言いつつもみんな真面目に毎回参加してました笑） 今回はチームでの取り組み方針の具体的な内容について少しだけ掘り下げます．\n1. 情報はSlackで共有 Slackをどうゆう感じで活用していたのかというと，\nコンペのDiscussionやNotebookの内容について疑問点などを話し合ったり，それ以外にも進め方の相談や雑談などを基本的に行っていました． あとはsubmitする時は一言声をかけるなどのsubmit管理もしていました．\nこうゆうのがあれば良かったなーというところでは，新着のDiscussionやNotebookをKaggleから連携して通知する仕組みを用意しておければ尚良かったのかなと思いました．\n2. 分析方針や実験結果はGithubのissueで管理 Githubをどうゆう感じで活用していたのかというと，\n分析での実験毎に1つのissueを立てて，そこでどうゆう実験をしたのかsubmitした結果のスコアがどうだったのかなどを記録として残してました． また，共通で使える特徴量生成のコードだったり，CVの切り方のコードなどの共有も行ってました．\nその他にはDiscussionの内容を整理したり，情報をまとめるために活用したりしていました．\n3. 毎週末に2時間程度のディスカッション 週末にGoogle meetでオンラインディスカッションでしていて，そこで何をしていたかというと，\n基本的には，今週何をしたのかを各々共有したり，わからない部分を話し合ってどうゆうふうに次進めて行くかなどをチームで考えていました． あとは，次の週でどうゆうことをするかの方向性を決めて終わる感じでした．\nもちろん雑談や仕事での苦労を労ったりもしていました笑\n最終順位 最終順位は4373チーム中34位の銀メダルで、金メダルまであともう少しのところまで行ったので，とても悔しい結果となりました．\n個人的にはInferenceの処理がエラーで通らない状況に最後の3日ぐらいでなって泣きそうになりました． チームメンバーにはweight0の状態で非常に申し訳なかったなと思ってます泣\n学習時に回していたノートブックでは，スコアがチーム内で作ったモデルの中でも上位5つ以内に入っていたので，アンサンブル時には効いてただろうなと思うと尚更です． 個人的な成長としては，テーブルデータに対してNeuralNetが有効に作用する場面について多少理解が深まったと感じています．\n今回のMoAでは，マルチラベルの予測だったので，一度に大量のクラスを予測する場合にはNNが有効でかつGBDT系と比較して計算速度も速いんだなと感じました．\nまた，特徴量的にも交互作用的な部分はNN内部の中間層の組み方などで実現できるので，GBDT系みたく大量に特徴量を用意しなくても対処できるのが大きいのかなと思っています．（今回のケースだとGBDTで大量のモデルを作るとなると速度的な部分で特徴量が膨大になるとかなり厳しい感じでした）\nあとは，NNの実装をPytorchで行ったこともあり，Pytorchの扱い方がわかるようになったのは大きいと思っています．（仕事ではTensorflowだったりするので\u0026hellip;）\nPytorchでの実装に関してはもっと進めて行きたいのとコードの整理も合わせてして行きたいので，次に参加予定のコンペではその辺りも意識して挑めたらなーと思います．\n","date":"2020-12-11","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-moa/","tags":["Kaggle"],"title":"Kaggle-MoAの振り返り"},{"content":"初期化したリストの更新処理でハマってしまった 今回は，初期化したリストを更新した際にハマってしまった失敗があるので，備忘録として残しておきます． 詳しい内容は参考サイトに載っています．（かなりわかりやすいです！）\npythonで決まった形のリストを予め作成しておきたい場合に，以下のようにすることがあると思います．\nshape you want: [[0, 0], [0, 0], [0, 0]] # 要素が2つあるリストが3つ \u0026gt;\u0026gt;\u0026gt; list1 = [[0] * 2] * 3 \u0026gt;\u0026gt;\u0026gt; list1 [[0, 0], [0, 0], [0, 0]]  そして，上記リストを何かしらの値で更新したい場合を考えます． 今回だと，[0][0]の要素を更新するとします．\n\u0026gt;\u0026gt;\u0026gt; list1[0][0] = 3.5 \u0026gt;\u0026gt;\u0026gt; list1 [[3.5, 0], [3.5, 0], [3.5, 0]]  結果は，各リストの0番目の要素が全て更新されています． この原因は，list1 = [[0] * 2] * 3と書くと，要素のリストが全て同じオブジェクトになってしまい，どこかの要素を変更すると全て変わってしまうからです．\n対処法 上記の結果を回避するためにはリスト内包表記を使うと解決することができます． 先程の例の場合，以下のように書くといいです．\n\u0026gt;\u0026gt;\u0026gt; list2 = [[0] * 2 for i in range(3)] \u0026gt;\u0026gt;\u0026gt; list2 [[0, 0], [0, 0], [0, 0]]  内包表記を使うと，リストはそれぞれ異なるオブジェクトとして扱われます． ですので，[0][0]の要素を更新すると，意図した部分だけが更新され問題ありません．\n\u0026gt;\u0026gt;\u0026gt; list2[0][0] = 3.5 \u0026gt;\u0026gt;\u0026gt; list2 [[3.5, 0], [0, 0], [0, 0]]  リストを初期化する際はこれらに注意しておかないと，本来の意図とは違う動きになってしまいます．\nこうゆうミスを気にしたくない場合には，numpyのarrayで作りたいshapeを作成し，その後にリストに変換すれば良いかもしれないです．\n\u0026gt;\u0026gt;\u0026gt; np.zeros((2, 3)).tolist() # 0で初期化 [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]] # 0以外の場合 np.ones((2, 3)).tolist() # 1で初期化 np.full((2, 3), 5).tolist() # 任意の値で初期化  参考 Pythonのリスト（配列）を任意の値・要素数で初期化\n","date":"2020-09-12","permalink":"https://masatakashiwagi.github.io/portfolio/post/list-objects/","tags":["Dev"],"title":"ネストしたリストの更新処理"},{"content":"ポートフォリオ作成 こちらのQiitaの記事でHugoを使って簡単にポートフォリオを作成できるというのを見かけたので，以前まで使っていたpersonal pageを移植しました． 移植した際に少し詰まった部分があるので，Tipsとしてこの記事で紹介します．\nこの記事は以前に使用していたHugo Themeの内容になります\n最初はHugo Theme Cactus Plusというテーマで作成していたのですが，再度作り直してます． （再作成した理由は，少しだけ凝ったテーマを使って見たくなったためです笑） 作り直したテーマはHugo Future Imperfect Slimになります．\nHugoはシンプルなデザインが多いので非常にオススメです．\n基本的な構築方法は上記Qiitaの記事に沿って行っています． 別途追加した要素としては，最初に作成したHugo Theme Cactus Plusと作り直したHugo Future Imperfect Slimそれぞれありますので，この際どちらも紹介します．\n  Hugo Theme Cactus Plus\n メニューの追加設定 Custom-CSSの設定（custom-cssの設定は簡単に設定できますので，今回は割愛します）    Hugo Future Imperfect Slim\n faviconの設定 github.ioでサイトをhostした場合のpath設定    個人的には，1回目に作成したテーマより2回目の方が簡単でした．\nHugo Theme Cactus Plus: メニューの追加設定 Hugo Theme Cactus Plusのテーマでは，デフォルトでAbout/Archive/Tagsの3つがメニューとして存在しています． 今回はそこにProjectsを新しく追加しましたので，その方法を記載します． 実施することとしては，以下の4ステップになります．\n content配下にprojectsディレクトリを作成し，_index.mdファイルを配置する． 記事などのページ情報はcontentで管理します．  ├── content │ ├── about │ ├── posts │ └── projects │ └── _index.md   themes/layouts/partials配下にあるnav.htmlにProjectsのリンクを追記する． これはTagsなどのリンクをコピーして，nameの部分はprojectsに修正すれば大丈夫です． メニューバーにProjectsを表示させるために，この部分を修正する必要があります．\n  themes/layouts/section配下にabout.htmlをコピーして，projects.htmlにrenameする． ここに追加することで，セクションのトップページとして扱われることになります．\n  最後に，コマンドラインでhugoを実行する． hugoコマンドを実行することで，必要なものが自動生成・反映されます．\n  以上でメニューを追加することができます． （他のテーマでは，もう少し簡単にメニュー追加が可能なものもあります）\nHugo Future Imperfect Slim: faviconの設定 faviconを設定する方法は，下記の3ステップになります．\n まず，下記のデフォルトのconfig.tomlの内容のうち，faviconとfaviconVersionを変更します．  [params.meta] description = \u0026quot;A theme by HTML5 UP, ported by Julio Pescador. Slimmed and enhanced by Patrick Collins. Multilingual by StatnMap. Powered by Hugo.\u0026quot; author = \u0026quot;HTML5UP and Hugo\u0026quot; favicon = false \u0026lt;-- trueに変更する svg = true faviconVersion = \u0026quot;1\u0026quot; \u0026lt;-- 1を削除する msColor = \u0026quot;#ffffff\u0026quot; iOSColor = \u0026quot;#ffffff\u0026quot;   config.tomlを修正したら，static配下にfaviconディレクトリを作成する．\n  static/favicon配下にfavicon.icoとfavicon-32x32.pngを配置する． なぜfavicon-32x32かというと？\n layouts/partials/meta.htmlのrel=iconに以下が記載されている  favicon-32x32 favicon-16x16 site.webmanifest    なので，これに合わせて名前を変更するかwebmanifestを新しく作成し，その中に諸々の内容を記載する必要がある．\n  Hugo Future Imperfect Slim: github.ioでサイトをhostした場合のpath設定 今回作成したサイトをgithub.ioでhostした場合に発生した内容です． 各メニューのURLとして，https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/home/などとなって欲しいのですが， https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/portfolio/home/とportfolioが重なってしまうエラーが発生しました．\n上記エラーを回避する方法の紹介になります． config.tomlファイルに各メニューの設定をする箇所があります．\n[menu] [[menu.main]] name = \u0026quot;Home\u0026quot; identifier = \u0026quot;home\u0026quot; url = \u0026quot;/\u0026quot; \u0026lt;-- /を削除する pre = \u0026quot;\u0026lt;i class='fa fa-home'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 1 [[menu.main]] name = \u0026quot;About\u0026quot; identifier = \u0026quot;about\u0026quot; url = \u0026quot;/about/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-id-card'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 2 [[menu.main]] name = \u0026quot;Blog\u0026quot; identifier = \u0026quot;blog\u0026quot; url = \u0026quot;/blog/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-newspaper'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 3  urlの部分で先頭の/を削除することで，上記問題を回避することできます．\n 以上で今回紹介する内容は終了になります．\n参考となる記事などがあまりなかったので，試行錯誤しながら行いました．\nそのため，もっと簡単にする方法が他にもあるかもしれないですので，もし他にあれば，Twitterなどでコメント頂けると大変助かります．\n","date":"2020-07-13","permalink":"https://masatakashiwagi.github.io/portfolio/post/hugo-portfolio/","tags":["Dev"],"title":"Hugoを使ったポートフォリオ作成"},{"content":"はじめに Kaggleを始めて半年ほど経ち，個人的にこの半年で得たものを整理するという意味で「kaggle その2 Advent Calendar 2019」の20日目を担当します．\n簡単な自己紹介として，普段は都内のベンチャー企業で主に製造業のお客さんを相手にデータ分析の仕事と自社製品の開発を8:2ぐらいの割合で担当しています．\n実はJTCから転職して今の会社は2社目で，働いて2年弱になります．\nちなみに前職のJTCでは，マーケティングオートメーションツール導入などのSE的なことをしてました．なので，バリバリデータサイエンスをしていたわけではないです笑\nKaggleは転職した時ぐらいから知って，すぐやり始めたいと思ってたのですが，色々と仕事プライベート共に余裕が無くて満を辞して半年ほど前から本格的に参加し始めました！\nKaggleを始めて得たもの 今回はそんな半年ほど前からKaggleを始めて得たものとして，大きく3つあり，それについて書きます．\n データサイエンス関連の知識 実務へのフィードバック 人との繋がり  それぞれ簡単ですが，書いていきたいと思います．\n1. データサイエンス関連の知識 よく言われていることですが，Kaggleは宝の山であり世界中のデータサイエンティスト・機械学習エンジニアの知恵や情報が特にコードレベルで共有されているのが魅力の一つです．掘れば掘るほど色々と出て来るので，これを活用しない手はないなという印象です．\nその中でも，特に個人的に得て良かった知見としては以下の4つかなと思います．\n1つ目は，\n 特徴量エンジニアリング  ドメイン知識に基づく特徴量エンジニアリングが有効であることはもちろん知っていましたが，それを作る発想であったり組み立て方が非常に勉強になってます． また関連して，どの単位で集約した特徴量を作るか，カテゴリカルデータやカウントデータの扱い，エンコーディングの仕方であったりと特徴量の作り方は非常に参考になってます．    2つ目は，\n パイプライン設計  パイプライン設計はKaggleを始めてから特に意識させられた部分になります．参考になる情報をいくつか上げておきます[1, 2, 3]．    これを意識して良かったこととしては\u0026hellip;\n 特徴量の管理が楽になる 試行錯誤した結果をログという形で後から確認できる 結果の再現性も容易になる 計算回した後は寝てられる笑  他にも色々と良いことはあるので，是非オススメしたいです！後々の再利用のためにも整理しておくと，一から全てを作り出さなくても良いので，有用かと思います．\n3つ目は，\n バリデーションの重要性  モデルの汎化性能を考える上では大事な要素で，Kaggleでは特にpubulic LBで上位に入っていても，バリデーションをきちんとしていないとprivate LBで大きくshake downしてしまう結果になることがよく？あるのかなという印象です．  学習データの結果が良くてもテストデータで全然良くないとなると使い物にならないので、この辺りは実務でも活きてくる部分になります．運用段階で全然使えないモデルが出来上がるのを回避できる方法の1つ．      4つ目は，\n NNをテーブルデータで使う方法  Neural Networkは画像認識の領域で使われてますが，それをテーブルデータに使う方法がKaggleでは見かけます．  テーブルコンペでは，GBDT系のアルゴリズムの方がまだまだ精度的には良いですが，モデルの多様性や特徴量抽出の自動化的な部分でNNモデルも十分に活用できると思ってます．   この辺りはもっとkernelなどで理解して自分の武器にしていきたいなという感じです．ただ，前までは選択肢にもなかった気がするので．様々な手法を見た結果得られた知見かなと．    2. 実務へのフィードバック Kaggleで実施する内容と実務での内容が必ずしも直結するわけではないですが，分析スキルの向上は実務でも大きく活きてます．\n例えば，実務でデータ受領後，EDAを進める中でデータの勘所を掴むのが以前より早くなったのと，何をどうすれば良いかを掴むのが以前よりスピードが上がったと感じてます．それによって，案件を進めていくスピードが上がったので，色々と試行錯誤できる時間を確保できるようになったと思います．\nまた，Kaggleで有効な手法を製品へフィードバックすることも進めているので，自分自身だけでなく会社へも還元できつつあるのかなという感じです．\n一方でKaggleが楽しすぎて，仕事中でもコンペのことが気になって手を動かしたくなったり，休日だいたい費やしてるので出不精になったりしてます笑\nTwitterでも書きましたが，良くも悪くも世界中の人たちと競い合って評価されるので，負けたくない精神は会社でも発揮されてます！\n人との繋がり Kaggleを始めてから，もくもく会や勉強会に参加する頻度が増えたかなと感じてます．大体最後には懇親会があるので，コンペの話や仕事の話で盛り上がって色々と情報交換が出来ていて楽しい限りです．\nあとは，コンペ終了後の反省会に参加することでコンペでの苦しみなどを共有できるのも良いコミュニティーだなと思います．そういった場で社内以外のデータサイエンティスト・機械学習エンジニアの方々とお話しできるのは非常に良い刺激になります．自分が知らないことを知ってる人がめっちゃいるので，勉強になりまくりです．\n前々から社外での繋がりを増やしたいと思ってたところでKaggleという共通の話題があるので，比較的話がしやすい環境ができてKaggle様様です．もっとKaggleでの繋がりを増やして，お互い切磋琢磨できる環境に持っていきたいですね．\nおわりに まだまだ半年しか経っていないですが，濃い経験や知見をKaggleを通して得られているので，これからも継続していきたいです！\n人との繋がり的には，もう少し仕事面でも色々と相談できる関係性を作って，どんなことをやっててどうゆう課題や問題意識があるかとか聞いてみたいです．\nあとは幸いにも，勉強会で知り合った方とコンペでチームを組んで頂けるようになったので，次はチームで参加する楽しみも味わいます！\n最後にKaggleでは、まさに以下の話を体現できるのではと思ってます！\n 頭で理屈をわかったところで，体感を伴わない知識は活用できない（ハリガネサービス）\n P.S. DSBコンペでメダルを獲得します！\n参考  [1] Kaggleで使えるFeather形式を利用した特徴量管理法 [2] データ分析コンペで使っているワイの学習・推論パイプラインを晒します [3] hakubishin3/kaggle_ieee  ","date":"2019-12-20","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-get-something/","tags":["Poem","Kaggle"],"title":"Kaggleを始めて半年経って個人的に得たもの"},{"content":"はじめに 今回は，先日初めて見たファイル形式のExcel Binary Workbook (xlsb)に関して，pythonでcsvにパースする話です．\n.xlsxはよくあるExcelファイルの形式ですが，それのバイナリー形式である.xlsbに関しての話です．（今まで見たことなかった拡張子です笑）\nExcelの闇やExcelとの格闘は色々ありますが，今回はそこをグッと堪えて進めたいと思います笑\n.xlsbとは Weblio辞書によると以下のように記載されています．\n .xlsbとは，Excel 2007で作成したブックを「XML形式でないバイナリブック」として保存する際に用いられる拡張子である．.xlsbでブックを保存した場合はファイル全体がバイナリ形式で保存され，XMLベースである.xlsxなどのファイル形式で保存した場合と比べて，ファイルサイズを数分の1程度に抑えることができる．\n 受け取ったファイルは.xlsb形式でも100MBぐらいあったため，.xlsx形式だとかなり容量が大きく，ファイルを開くと処理が重たくなることが想像できるので，圧縮したのだと考えられます．\nExcelを扱えるpythonライブラリ openpyxl 定番のopenpyxlです．\n上記ページにも記載されてますが，Excelファイルの拡張子である.xlsxを扱うことができます．\n openpyxl is a Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files.\n いつものようにこのライブラリで処理しようとしたところ，下記のようなエラーが発生しました．\nopenpyxl.utils.exceptions.InvalidFileException: openpyxl does not support binary format .xlsb, please convert this file to .xlsx format if you want to open it with openpyxl  もう一度openpyxlの説明を見ると，確かに扱える拡張子はxlsx/xlsm/xltx/xltmとなっているので、.xlsbは扱えないのが分かります．\nそこで，.xlsbの拡張子が扱えるライブラリを調べたところ，pyxlsbというのがあるみたいなので，それを使うことにしました．\npyxlsb pyxlsbは公式の説明にあるように，xlsb形式を扱えるpythonライブラリになります．\n pyxlsb is an Excel 2007-2010 Binary Workbook (xlsb) parser for Python.\n インストールはpipですることができます．\npip install pyxlsb  公式のサンプルコードを記載しておきます．\nimport csv from pyxlsb import open_workbook with open_workbook('Book1.xlsb') as wb: for name in wb.sheets: with wb.get_sheet(name) as sheet, open(name + '.csv', 'w') as f: writer = csv.writer(f) for row in sheet.rows(): writer.writerow([c.v for c in row])  もしpandasのデータフレームに変換したい場合は，参考ページのコードで可能となります．\nただし，時刻変換に関して少し注意が必要なので，記載しておくと公式にもある通り，日付はfloatに変換されてしまうため，convert_date関数を使う必要があります．\n Note that dates will appear as floats. You must use the convert_date(date) method from the corresponding Workbook instance to turn them into datetime.\n なので，元のファイルに時刻が入っている場合には上記変換をコードの中に入れて処理する必要がありますのでご注意下さい．\nprint(wb.convert_date(41235.45578)) \u0026gt;\u0026gt;\u0026gt; datetime.datetime(2012, 11, 22, 10, 56, 19)  今回は個人的に嵌ってしまった.xlsb形式のファイルを扱う方法を紹介しましたが，出来ればデータ分析をするようなデータをExcelファイルで扱いたくないのが本音です．\nもちろん簡単なデータの可視化とか表計算とかExcelが活躍する場面は多々あると思うので，使い分けていきたいとは思います．\n参考 Stack Overflow - Read XLSB File in Pandas Python\n追記 2020-01-05 pandasのversion=1.0.0で.xlsbファイルをロードできるようになったみたいです．\n方法はpd.read_excelの引数でengine=\u0026quot;pyxlsb\u0026quot;と指定するだけです．\n# Returns a DataFrame pd.read_excel(\u0026quot;path_to_file.xlsb\u0026quot;, engine=\u0026quot;pyxlsb\u0026quot;)  参考: https://pandas.pydata.org/docs/user_guide/io.html#io-xlsb\n","date":"2019-10-05","permalink":"https://masatakashiwagi.github.io/portfolio/post/excel_processing_using_python/","tags":["DataScience"],"title":"Excel Binary WorkbookをPythonで処理"}]