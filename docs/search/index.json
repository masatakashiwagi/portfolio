[{"content":"Kaggle-Shopeeコンペの振り返り 2021/03/09~2021/05/11まで開催していたShopeeコンペの振り返りになります．\n2週間程度しか手を動かせなかったですが，久しぶりに参加したので備忘録として記録を残しておきます．\n最終的な結果は179th/2464で銅メダルで，特に凝ったことは何もしていなかったので，妥当かなと思います．\nこのコンペは上位10チーム中7チームが日本人チームで，日本人のレベルの高さを改めて実感できるコンペでした！\n概要 コンペの内容は簡単に言うと，画像とテキスト情報を用いて、2つの画像の類似性を比較し，どのアイテムが同じ商品であるかを予測するコンペになります．\n 開催期間: 2021/03/09 ~ 2021/05/11 参加チーム数: 2464 予測対象: posting_id列にマッチする全てのposting_idを予測する．ただし，posting_idは必ずself-matchし，グループの上限は50個となっている． データ: 投稿ID，画像，商品のタイトル，画像の知覚ハッシュ(perceptual hash)，ラベルグループID 評価指標: F1-Score その他: コードコンペ  My Solution  画像特徴量・テキスト特徴量・画像のphash値をconcatして結果をユニーク化したものを最終的な予測値としました． 何も複雑なことはしていないモデルですが，結果的に銅メダルを取ることができました．  Image Model  eca_nfnet_l0  loss: ArcFace pooling: AdaptiveAvgPool2d scheduler: CosineAnnealingLR loss(criterion): CrossEntropyLoss size: 512*512   eca_nfnet_l1  loss: CurricularFace pooling: MAC scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512   efficientnet_b3  loss: ArcFace pooling: GeM scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 512*512   swin_small_patch4_window7_224  loss: CurricularFace scheduler: CosineAnnealingLR loss(criterion): FocalLoss size: 224*224   common  augmentation by albumentations.  HorizontalFlip VerticalFlip Rotate RandomBrightness   optimizer  Adam      少し工夫したポイント  画像特徴量を抽出する部分で，いくつか工夫した点をあげます．  CNN Image Retrievalを参考にしました．     lossにArcFaceとCurricularFaceを用いた  ArcFaceを使っている人が多かったが，CurricularFaceも使いました．スコア的にはCurricularFaceの方がよかったです．   いくつかのモデルでpooling層にGeMとMACを用いた  Google Landmark Retrieval Challengeで上手くいっていたGeMやMACなどのプーリング手法を用いました．   loss(criterion)にFocalLossを用いた 最終的に得られた特徴量をconcatした後，ZCAWhiteningによる次元削減を行った  有効でなかったもの  一方で，上手くいかなかった内容としては以下になります．  resnext50_32x4d swin_small_patch4_window7_224 with ArcFace CosFace, AdaCos PCA Whitening (worse than ZCAWhitening)    Text Model  paraphrase-xlm-r-multilingual-v1  loss: ArcFace scheduler: linear schedule with warmup loss(criterion): CrossEntropyLoss optimizer: AdamW   TF-IDF  textの方のモデルは特に改良する時間が取れなかったので，ほとんど手を付けれてなかったです．\n transformerとTF-IDFで得られた特徴量それぞれに対して，Cosine Similarityを計算し，テキストのpredictionを作成しました．  最終的な予測値は画像特徴量とテキスト特徴量に加えて，画像のphash値を追加して，ユニークを取った値としています．\n反省  post-processingが全然できていなかった  他の解法を見るに，post-processingでスコアが伸びているので，この部分は結構大事だったんだなと感じています． 6位の解法にもありましたが，今回のコンペでは，label_groupの長さが2以上であることから，「予測した結果のposting_idが1つしかない場合，強制的に似たものを持ってきて，2つにする」というアイデアでLBがかなり上がるみたい   ImageとTextをMulti-modal的にモデルに組み込んで学習することができていなかった グラフ理論全然わかってないです笑 細かい部分  他のoptimizerを試す  SAMとか   Database-side feature augmentation (DBA) / Query Extension (QE)  全然知らなかったので，End-to-end Learning of Deep Visual Representations for Image Retrievalを読んで勉強したい   閾値の調整 テキストモデルの追加     ここからは上位の解法を書きたいと思います．数もそれなりにあるので，載せるのは上位5つにします．最後に上位5つ以外にもDiscussionsに投稿されている解法のリンクを載せておきます．\n1st Place Solution 解法はこちらになります: 1st Place Solution - From Embeddings to Matches\nModel  Image: 2つのモデル  eca_nfnet_l1 * 2   Text: 5つのモデル  xlm-roberta-large xlm-roberta-base cahya/bert-base-indonesian-1.5G indobenchmark/indobert-large-p1 bert-base-multilingual-uncased   loss: ArcFace poolingした後にbatch normalizationとfeature-wise normalizationを行った ArcFaceのチューニングを行った  学習段階で徐々にmarginを大きくした（埋め込み表現のqualityに影響する）  画像に対するmargin: 0.8~1.0 テキストに対するmargin: 0.6~0.8   marginを大きくすると，モデルの収束に問題が出たので，以下を行った  warmup stepsを大きくする cosineheadに対するlearning rateをより大きくする gradient clippingを行う   class-size-adaptive marginも試したが，不均衡性が小さかったため，改善は少しだけだった  image: class_size^-0.1 text: class_size^-0.2     global average poolingの後にFC層を追加するとモデルが悪くなるが，feature-wise normalizationの前にbatch normalizationを追加するとスコアが改善した  Features  Combining Image \u0026amp; Text Matches  マッチングさせる方法をいくつかトライした   画像のembeddingによるマッチングとテキストのembeddingによるマッチングを結合する 画像のembeddingとテキストのembeddingをconcatしてからマッチングする 1と2を組み合わせてマッチングする   これは，画像のembeddingが強く示唆するアイテム・テキストのembeddingが強く示唆するアイテム・画像とテキストのembeddingがどちらも適度に示唆するアイテムを取り入れることができる     Iterative Neighborhood Blending (INB)  QE/DBAとは少し異なる，embeddingからマッチする商品を検索するパイプラインを作った k-Nearest Neighbor Search:  近隣探索ラリブラリ: faiss (k=51: 50(自身以外)) + 1(自身))   Threshold:  コサイン類似度をコサイン距離(=1-コサイン類似度)に変換し，distance\u0026lt;thresholdを満たす(matches, distances)のペアを取得した 閾値処理を行う場合，1つのクエリに対して少なくとも2つ以上のマッチがあることがコンペで保証されているので，distanceがmin2-thresholdを超えた場合にのみ，二番目に近いマッチを除外する   Neighborhood Blending:  kNNによるサーチとmin2による閾値処理をした後，各アイテムの(matches, similarities)ペアを取得し，グラフを作成する  各ノードはアイテム，エッジの重みは2つのノード間の類似性を示す 近傍のみが繋がっており，閾値条件とmin2条件を満たさないノードは切断される   近傍のアイテムの情報を使いたいので，クエリアイテムのembeddingを改良し，よりクラスタを明確にする  重みとして類似性を持つ近傍のembeddingを加重合計し，それをクエリembeddingに追加する    上図を簡単に説明すると，最初Aは[B,C,D]と繋がっているが，加重合計した結果閾値=0.5の場合，Cとの接続が切れて，Aは[B,D]のみと接続していることがわかる このNBの処理を評価指標の改善が止まるまで繰り返し実行する 最終的なNBのパイプラインは以下になる    About Threhsold Tuning:  調整する閾値は全部で10種類ある  stage1のtext, image, combinationの閾値が3つ stage2の閾値が1つ stage3の閾値が1つ 直接最終結合部分に繋がるstage1のtext, imageの閾値が2つ stage1~3のmin2閾値が3つ   最終的にはstage2,3の閾値を2つ調整するだけでよかった     Visualizations of Embeddings before/after INB  INBの効果を可視化したノートブックが公開されている  (SHOPEE) Embedding Visualizations before/after INB 見るからに各点が凝集されたクラスタを形成していることがわかる      Others  画像に対するCutMix(p=0.1) 画像のaugmentation  horizontal flipのみがよかった   madgrad optimizer: https://github.com/facebookresearch/madgrad 学習データ全体を使った学習  2nd Place Solution 解法はこちらになります: 2nd place solution (matching prediction by GAT \u0026amp; LGB), 2nd Place Solution Code\nSummary  1st stage: 画像・テキスト・画像+テキストデータに対するコサイン類似度を得るためにMetric Learningによるモデルを学習する 2nd stage: 同じlabel groupに属するアイテムのペアかどうかを識別するために\u0026quot;メタ\u0026quot;モデルを学習する  Model  image: 2つのモデル  backbone  nfnet-F0 ViT   loss: CurricularFace optimizer: SAM embeddingの結合: F.normalize(torch.cat([F.normalize(emb1), F.normalize(emb2)], axis=1))   text: 3つのモデル + TF-IDF  indonesian-bert multilingual-bert paraphrase-xlm   image+text: nfnet-F0とindonesian-bertのFC層のembeddingを結合したもの Training Tips:  label groupのサイズに基づいたSample Weighting  1 / (label group size) ** 0.4      Features Graph features  それぞれのアイテムのtop-Kコサイン類似度の平均と分散  K=5, 10, 15, 30, etc   標準化（mean=0, std=1） Pagerank  Others  テキストの長さ #の記号 Levenshtein距離 画像ファイルのサイズ 画像の高さと幅 Query Extension  Ensemble Methodology  ローカルデータでそれぞれのモデルのベストな閾値を計算する 上記閾値でそれぞれのモデルの予測値を差し引く 差し引かれた予測値を合計する 合計値\u0026gt;0の場合，アイテムペアが同じグループに属するとする お互いにエッジを持たないペアを削除する  A-Bはあるが，B-Aがない場合は両方を削除する    Post-Processing  中間中心性が最も高いエッジを再帰的に削除する（Graph-Based）  Others  Performance and Memory Tunings  CuDF, cupy, cugraph: GPUを有効に使うためには大事 ForestInference: 40分かかるCPUでの推論が2分になる   Not worked  Graph-basedの特徴量 固有ベクトルの中心性とjaccard End-to-end model Local feature matching    3rd Place Solution 解法はこちらになります: 3rd Place Solution (Triplet loss, Boosting, Clustering)\nModel  image: 2つのモデル  backbone  efficientnet_b2 ViT (DINO)     text: 2つのモデル (異なるtokenizersとCLIP)  indonesian-bert multilingual-bert   common  loss: TripletLoss (margin=0.1) 各エポック，label_groupsから重複したペアを作成し，各バッチでlabel_groupsから1ペアが挿入される．バッチサイズは128を使っていたため，バッチ毎に64の重複を取得し，ランダムな5点でそれらの各々を比較する 5fold CVで，validationスコアを計算する時は，validation-foldの中から候補を選ぶのではなく，学習サンプル全体から候補を探した  この方法はCV/LBのgapを抑えて，相関を取ることが出来る oofを用意して，2nd-levelの予測に使用する CVから得られた5つのモデルで，テストデータに対して推論を行った より速く推論するためには，validationなしの1つのモデルでテストデータにfitさせること   ArcFaceは上手くいかなかった    Features  複数モデルを用いて，異なる表現方法を作成するのが良い 全てのembeddingsから候補となる近しいものを結合し，ペアが実際に重複しているかどうかにかかわらず、binary targetを使用してペアオブジェクトのサンプルを作成する  3M点のペアがあり，重複率は4%だった   これらに対して，GBM(=CatBoost)を作成し，embedding毎に計算する  pairwise-distances (コサイン類似度，ユークリッド距離など) 両方の点周辺の密度 points ranks   最終的には500個特徴量を作成し，CVやLBを使いながら，重複確率による閾値の候補を出す\u0008 -\u0026gt; LB:0.76+  Post-Processing  クラスタリングを行ったが，embeddingsを使うのではなく，pairwise-distanceを使う 重複推定のためにGBM(=CatBoost)の確率を使い，類似度を求める 凝集クラスタリングのアイデアを採用  各単一点をクラスタとして開始し，平均的なクラスタサイズが閾値に等しくなるまでそれらをマージしていく クラスタが単一の場合，最も近傍のクラスタにマージする -\u0026gt; LB:0.79    Others  最適化するために  half precision(torch AMP)を使って学習と推論を行った 画像モデルの場合，画像の読み込みとリサイズにNVIDIA DALIを使った GBMの推論には，Rapids ForestInference Libraryを使った   上記の方法を使わないと，2時間で全てのモデルを推論することは不可能だった  4th Place Solution 解法はこちらになります: 4th Place Solution\nModel  image: backboneにnfnetとefficientnet  pooling: GeM＆Avg pooling embeddingが大きいほど，LBのスコアがよくなった loss: ArcFace marginの調整が大事だった   text: BERTベース + TF-IDF  TF-IDF: かなり大きいembeddingを生成し，notebook上ではメモリの制約を受けるため，Random Sparse Projectionで次元削減を行った loss: ArcFace marginの調整が大事だった    Ensemble  画像のembedding・テキスト（BERTベース）のembedding・TF-IDFのembeddingを結合して，正規化した これらのベクトル表現のそれぞれに対して，pairwiseコサイン類似度を計算し，3つの行列を作成した  最初に二乗し，その後加重平均を取って行列を結合した    Post-Processing  閾値の調整 Rank2 matching  もし，AがRank2にBを持っており，BはRank2にAを持っている場合，お互いに追加する   Rank2とRank3の違いが大きい場合，Rank2のIDを追加する 少なくとも1つの他とのマッチング（Rank2のコサイン類似度が極端に小さくなければ） Query Expansion マッチしていない行との再マッチング  その他上位解法のリンク  Kaggle ShopeeコンペPrivate LB待機枠＆プチ反省会 5th Place Solution 6th place solution 7th place solution 8th Place Solution Overview Public 16th / Private 10th Solution 11th Place Solution 14th Place Gold - Image Text Decision Boundary 15th place solution Public 13th / Private 16th solution 18th place solution - You really don\u0026rsquo;t need big models 19 place. Voting and similarity chain 26th Place Solution : Effective Cluster Separation and Neighbour Search [31st Place] Object Detection approach Public 39th / Private 37th Solution 48th Place Silver - Simple Baseline Public 56th / Private 57th Solution 62th Place Solution (Stacking Logistic Regression) 72nd place solution  ","date":"2021-05-14","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-shopee-solution/","tags":["kaggle"],"title":"Kaggle-Shopeeコンペの振り返りとソリューション"},{"content":"はじめに 今まで仕事では，開発環境としてIntelliJを使っていたのですが，最近はVSCodeの人気が高くExtensionsも便利なものが多くあるということで，個人的な作業をする時はVSCodeを使ってみようと思って使っています． そんな中で，タイトルにもあるようにVSCodeからgit pushしようとしたら，\u0026lt;アカウント名\u0026gt;@github.com: Permission denied (publickey).とエラーが出たので，それを解消してVSCodeでgit pushできるようにした備忘録になります．\nエラー原因（SSH接続エラー） Permission denied (publickey)とあり，GitHubにSSH接続するために，公開鍵を登録しておかないといけないのですが，それをしていなかったので，エラーが発生したということになります．\n以下のコマンドを打つことで接続を確認することができます．\nssh -T git@github.com \u0026gt; git@github.com: Permission denied (publickey).  ではどうすればいいかというと，鍵を生成してGithubに登録すればいいということになります．\n公開鍵と秘密鍵の作成 詳細な作成方法はこちらの記事が参考になります．\n簡単に手順を載せておきます．\ncd ~/.ssh ssh-keygen -t rsa -b 4096 -C \u0026quot;\u0026lt;メールアドレス\u0026gt;\u0026quot; -f github_rsa # オプションをいくつか設定して，鍵を生成 # 以下実行結果（一部マスクしてます） Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in github_rsa. Your public key has been saved in github_rsa.pub. The key fingerprint is: SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \u0026lt;メールアドレス\u0026gt; The key's randomart image is: +---[RSA 4096]----+ |=== | |.B o . | |o.. . * . o | |. . . B +. oo .o*| | . o * OSo.oooo*+| | . = + = o ..*..| | E . . o . . ..| | . . | | | +----[SHA256]-----+  鍵の種類をRSAにし，鍵の長さを4096にしています．ファイル名はgithub_rsaと設定しました．\nGithubに生成した公開鍵を登録 cd ~/.ssh ssh-add -K github_rsa # 秘密鍵をssh-agentデーモンに登録 pbcopy \u0026lt; github_rsa.pub # pbcopyコマンドで公開鍵の中身をクリップボードにコピー  この後は，コピーした公開鍵の中身をGithubに登録します．\nGithubのアカウントからSettingsに進み，SSH and GPG keysを選択し，New SSH keyを押して，先程コピーした中身をペーストし，名前を決めて保存します．\nSSH接続確認 保存が完了したら，SSH接続できるか確認するために，以下のコマンドを打って確認します．\nssh -T git@github.com \u0026gt;Hi \u0026lt;ユーザー名\u0026gt;! You've successfully authenticated, but GitHub does not provide shell access.  Remote設定の上書き ここまで来たら後一息で，最後にremoteの設定を上書きします． 以下のような感じでリポジトリ名を書いて，実行すればOK．\ngit remote set-url origin git@github.com:\u0026lt;ユーザー名\u0026gt;/\u0026lt;リポジトリ名\u0026gt;.git  VSCodeからgit push 今までの設定が完了していれば，上記画像の手順でVSCodeの画面から簡単にgitにcommitやpushなどの操作を行うことができます．\nさいごに まだまだVSCode初心者なので，使いやすいExtensionsを取り入れて開発環境をカスタマイズしていきたいと思います！\n参考 初めてのgitは5ステップで完了\nGitHubでssh接続する手順~公開鍵・秘密鍵の生成から~\n","date":"2021-03-26","permalink":"https://masatakashiwagi.github.io/portfolio/post/vscode_git_connect/","tags":["dev"],"title":"VSCodeとgitを連携してpushできるようにするまで"},{"content":"RuntimeError: CUDA error: device-side assert triggeredの解決方法 Pytorchでモデルを作成していた際に，RuntimeError: CUDA error: device-side assert triggeredが発生して，原因がよくわからなかったので，調べたことをメモしておきます．\nエラー発生の原因 調べてみると，原因としては\n ライブラリのVersionが違う ラベル/クラスの数とネットワークの入出力のshapeが異なる loss関数の入力が正確でない  などなど\u0026hellip;\nよくあるのが，下2つかなと思います．\nラベル/クラスの数とネットワークの入出力のshapeが異なる 想定しているラベルもしくはクラス数とネットワークの出力のクラス数が異なる場合，この場合はnn.Linear(input, num_class)で合わせてやる必要がある．\nloss関数の入力が正確でない 僕が遭遇したのはこちらのパターンになります．\n例えば，BCELossを考えた場合，計算するためには値としては0~1を取る必要があります．そのため普通は最終出力にSigmoid, Softmax関数を入れるかと思います．\nそれ以外にもLossの設計で以下のようにしておくと良いかと思います．\nclass BCELoss(nn.Module): def __init__(self): super().__init__() self.bce = nn.BCELoss() def forward(self, input, target): input = torch.where(torch.isnan(input), torch.zeros_like(input), input) input = torch.where(torch.isinf(input), torch.zeros_like(input), input) input = torch.where(input\u0026gt;1, torch.ones_like(input), input) # 1を超える場合には1にする target = target.float() return self.bce(input, target)  他の解決方法 他にも調べていると解決方法としてCUDAの設定を以下にすると良いなどもありましたが，解決するかどうかはよくわからないです．\nCUDA_LAUNCH_BLOCKING=1  参考 https://towardsdatascience.com/cuda-error-device-side-assert-triggered-c6ae1c8fa4c3\n","date":"2021-02-01","permalink":"https://masatakashiwagi.github.io/portfolio/post/cuda_error_device-side_assert_triggered/","tags":["dev"],"title":"CUDA error: Device-side assert triggeredの解決方法"},{"content":"Kaggle MoAにTeam 90\u0026rsquo;sで初参加 このブログは2020/9/4~12/1まで開催していたMoAコンペでの取り組みを紹介する内容です． （コンペの詳細な内容については割愛します）\n今回のコンペでは，同世代のメンバーでチームを組んで取り組みました！\nチーム結成の経緯は，Twitterでお互いが90年生まれということを知って，同世代でKaggleチーム組んで戦いたいねーという感じだったと思います． それが少し前のことで当時取り組める良い感じのコンペがなかったのですが，今回テーブルデータのコンペで取り組めそうということで始まりました． チームでの取り組みはとにかく学びが多く，終盤までモチベーションを保つことができたのが大きかったです．\nまた，議論することで理解なども深まっていくので，コンペを通してより成長できたんじゃないかなと思います．\n今回の僕たちのチームでの取り組み方を紹介すると\n1. 情報はSlackで共有 2. 分析方針や実験結果はGithubのissueで管理 3. 毎週末に2時間程度のディスカッション\nといった感じです．\n3番目の週末のディスカッションは強制ではなく，参加可能な人が参加する形式で運用してました． （と言いつつもみんな真面目に毎回参加してました笑） 今回はチームでの取り組み方針の具体的な内容について少しだけ掘り下げます．\n1. 情報はSlackで共有 Slackをどうゆう感じで活用していたのかというと，\nコンペのDiscussionやNotebookの内容について疑問点などを話し合ったり，それ以外にも進め方の相談や雑談などを基本的に行っていました． あとはsubmitする時は一言声をかけるなどのsubmit管理もしていました．\nこうゆうのがあれば良かったなーというところでは，新着のDiscussionやNotebookをKaggleから連携して通知する仕組みを用意しておければ尚良かったのかなと思いました．\n2. 分析方針や実験結果はGithubのissueで管理 Githubをどうゆう感じで活用していたのかというと，\n分析での実験毎に1つのissueを立てて，そこでどうゆう実験をしたのかsubmitした結果のスコアがどうだったのかなどを記録として残してました． また，共通で使える特徴量生成のコードだったり，CVの切り方のコードなどの共有も行ってました．\nその他にはDiscussionの内容を整理したり，情報をまとめるために活用したりしていました．\n3. 毎週末に2時間程度のディスカッション 週末にGoogle meetでオンラインディスカッションでしていて，そこで何をしていたかというと，\n基本的には，今週何をしたのかを各々共有したり，わからない部分を話し合ってどうゆうふうに次進めて行くかなどをチームで考えていました． あとは，次の週でどうゆうことをするかの方向性を決めて終わる感じでした．\nもちろん雑談や仕事での苦労を労ったりもしていました笑\n最終順位 最終順位は4373チーム中34位の銀メダルで、金メダルまであともう少しのところまで行ったので，とても悔しい結果となりました．\n個人的にはInferenceの処理がエラーで通らない状況に最後の3日ぐらいでなって泣きそうになりました． チームメンバーにはweight0の状態で非常に申し訳なかったなと思ってます泣\n学習時に回していたノートブックでは，スコアがチーム内で作ったモデルの中でも上位5つ以内に入っていたので，アンサンブル時には効いてただろうなと思うと尚更です． 個人的な成長としては，テーブルデータに対してNeuralNetが有効に作用する場面について多少理解が深まったと感じています．\n今回のMoAでは，マルチラベルの予測だったので，一度に大量のクラスを予測する場合にはNNが有効でかつGBDT系と比較して計算速度も速いんだなと感じました．\nまた，特徴量的にも交互作用的な部分はNN内部の中間層の組み方などで実現できるので，GBDT系みたく大量に特徴量を用意しなくても対処できるのが大きいのかなと思っています．（今回のケースだとGBDTで大量のモデルを作るとなると速度的な部分で特徴量が膨大になるとかなり厳しい感じでした）\nあとは，NNの実装をPytorchで行ったこともあり，Pytorchの扱い方がわかるようになったのは大きいと思っています．（仕事ではTensorflowだったりするので\u0026hellip;）\nPytorchでの実装に関してはもっと進めて行きたいのとコードの整理も合わせてして行きたいので，次に参加予定のコンペではその辺りも意識して挑めたらなーと思います．\n","date":"2020-12-11","permalink":"https://masatakashiwagi.github.io/portfolio/post/kaggle-moa/","tags":["kaggle"],"title":"Kaggle-MoAの振り返り"},{"content":"初期化したリストの更新処理でハマってしまった 今回は，初期化したリストを更新した際にハマってしまった失敗があるので，備忘録として残しておきます． 詳しい内容は参考サイトに載っています．（かなりわかりやすいです！）\npythonで決まった形のリストを予め作成しておきたい場合に，以下のようにすることがあると思います．\nshape you want: [[0, 0], [0, 0], [0, 0]] # 要素が2つあるリストが3つ \u0026gt;\u0026gt;\u0026gt; list1 = [[0] * 2] * 3 \u0026gt;\u0026gt;\u0026gt; list1 [[0, 0], [0, 0], [0, 0]]  そして，上記リストを何かしらの値で更新したい場合を考えます． 今回だと，[0][0]の要素を更新するとします．\n\u0026gt;\u0026gt;\u0026gt; list1[0][0] = 3.5 \u0026gt;\u0026gt;\u0026gt; list1 [[3.5, 0], [3.5, 0], [3.5, 0]]  結果は，各リストの0番目の要素が全て更新されています． この原因は，list1 = [[0] * 2] * 3と書くと，要素のリストが全て同じオブジェクトになってしまい，どこかの要素を変更すると全て変わってしまうからです．\n対処法 上記の結果を回避するためにはリスト内包表記を使うと解決することができます． 先程の例の場合，以下のように書くといいです．\n\u0026gt;\u0026gt;\u0026gt; list2 = [[0] * 2 for i in range(3)] \u0026gt;\u0026gt;\u0026gt; list2 [[0, 0], [0, 0], [0, 0]]  内包表記を使うと，リストはそれぞれ異なるオブジェクトとして扱われます． ですので，[0][0]の要素を更新すると，意図した部分だけが更新され問題ありません．\n\u0026gt;\u0026gt;\u0026gt; list2[0][0] = 3.5 \u0026gt;\u0026gt;\u0026gt; list2 [[3.5, 0], [0, 0], [0, 0]]  リストを初期化する際はこれらに注意しておかないと，本来の意図とは違う動きになってしまいます．\nこうゆうミスを気にしたくない場合には，numpyのarrayで作りたいshapeを作成し，その後にリストに変換すれば良いかもしれないです．\n\u0026gt;\u0026gt;\u0026gt; np.zeros((2, 3)).tolist() # 0で初期化 [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]] # 0以外の場合 np.ones((2, 3)).tolist() # 1で初期化 np.full((2, 3), 5).tolist() # 任意の値で初期化  参考 Pythonのリスト（配列）を任意の値・要素数で初期化\n","date":"2020-09-12","permalink":"https://masatakashiwagi.github.io/portfolio/post/list-objects/","tags":["dev"],"title":"ネストしたリストの更新処理"},{"content":"ポートフォリオ作成 こちらのQiitaの記事でHugoを使って簡単にポートフォリオを作成できるというのを見かけたので，以前まで使っていたpersonal pageを移植しました． 移植した際に少し詰まった部分があるので，Tipsとしてこの記事で紹介します．\nこの記事は以前に使用していたHugo Themeの内容になります\n最初はHugo Theme Cactus Plusというテーマで作成していたのですが，再度作り直してます． （再作成した理由は，少しだけ凝ったテーマを使って見たくなったためです笑） 作り直したテーマはHugo Future Imperfect Slimになります．\nHugoはシンプルなデザインが多いので非常にオススメです．\n基本的な構築方法は上記Qiitaの記事に沿って行っています． 別途追加した要素としては，最初に作成したHugo Theme Cactus Plusと作り直したHugo Future Imperfect Slimそれぞれありますので，この際どちらも紹介します．\n  Hugo Theme Cactus Plus\n メニューの追加設定 Custom-CSSの設定（custom-cssの設定は簡単に設定できますので，今回は割愛します）    Hugo Future Imperfect Slim\n faviconの設定 github.ioでサイトをhostした場合のpath設定    個人的には，1回目に作成したテーマより2回目の方が簡単でした．\nHugo Theme Cactus Plus: メニューの追加設定 Hugo Theme Cactus Plusのテーマでは，デフォルトでAbout/Archive/Tagsの3つがメニューとして存在しています． 今回はそこにProjectsを新しく追加しましたので，その方法を記載します． 実施することとしては，以下の4ステップになります．\n content配下にprojectsディレクトリを作成し，_index.mdファイルを配置する． 記事などのページ情報はcontentで管理します．  ├── content │ ├── about │ ├── posts │ └── projects │ └── _index.md   themes/layouts/partials配下にあるnav.htmlにProjectsのリンクを追記する． これはTagsなどのリンクをコピーして，nameの部分はprojectsに修正すれば大丈夫です． メニューバーにProjectsを表示させるために，この部分を修正する必要があります．\n  themes/layouts/section配下にabout.htmlをコピーして，projects.htmlにrenameする． ここに追加することで，セクションのトップページとして扱われることになります．\n  最後に，コマンドラインでhugoを実行する． hugoコマンドを実行することで，必要なものが自動生成・反映されます．\n  以上でメニューを追加することができます． （他のテーマでは，もう少し簡単にメニュー追加が可能なものもあります）\nHugo Future Imperfect Slim: faviconの設定 faviconを設定する方法は，下記の3ステップになります．\n まず，下記のデフォルトのconfig.tomlの内容のうち，faviconとfaviconVersionを変更します．  [params.meta] description = \u0026quot;A theme by HTML5 UP, ported by Julio Pescador. Slimmed and enhanced by Patrick Collins. Multilingual by StatnMap. Powered by Hugo.\u0026quot; author = \u0026quot;HTML5UP and Hugo\u0026quot; favicon = false \u0026lt;-- trueに変更する svg = true faviconVersion = \u0026quot;1\u0026quot; \u0026lt;-- 1を削除する msColor = \u0026quot;#ffffff\u0026quot; iOSColor = \u0026quot;#ffffff\u0026quot;   config.tomlを修正したら，static配下にfaviconディレクトリを作成する．\n  static/favicon配下にfavicon.icoとfavicon-32x32.pngを配置する． なぜfavicon-32x32かというと？\n layouts/partials/meta.htmlのrel=iconに以下が記載されている  favicon-32x32 favicon-16x16 site.webmanifest    なので，これに合わせて名前を変更するかwebmanifestを新しく作成し，その中に諸々の内容を記載する必要がある．\n  Hugo Future Imperfect Slim: github.ioでサイトをhostした場合のpath設定 今回作成したサイトをgithub.ioでhostした場合に発生した内容です． 各メニューのURLとして，https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/home/などとなって欲しいのですが， https://\u0026lt;アカウント名\u0026gt;.github.io/portfolio/portfolio/home/とportfolioが重なってしまうエラーが発生しました．\n上記エラーを回避する方法の紹介になります． config.tomlファイルに各メニューの設定をする箇所があります．\n[menu] [[menu.main]] name = \u0026quot;Home\u0026quot; identifier = \u0026quot;home\u0026quot; url = \u0026quot;/\u0026quot; \u0026lt;-- /を削除する pre = \u0026quot;\u0026lt;i class='fa fa-home'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 1 [[menu.main]] name = \u0026quot;About\u0026quot; identifier = \u0026quot;about\u0026quot; url = \u0026quot;/about/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-id-card'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 2 [[menu.main]] name = \u0026quot;Blog\u0026quot; identifier = \u0026quot;blog\u0026quot; url = \u0026quot;/blog/\u0026quot; \u0026lt;-- 先頭の/を削除する pre = \u0026quot;\u0026lt;i class='far fa-newspaper'\u0026gt;\u0026lt;/i\u0026gt;\u0026quot; weight = 3  urlの部分で先頭の/を削除することで，上記問題を回避することできます．\n 以上で今回紹介する内容は終了になります．\n参考となる記事などがあまりなかったので，試行錯誤しながら行いました．\nそのため，もっと簡単にする方法が他にもあるかもしれないですので，もし他にあれば，Twitterなどでコメント頂けると大変助かります．\n","date":"2020-07-13","permalink":"https://masatakashiwagi.github.io/portfolio/post/hugo-portfolio/","tags":["dev"],"title":"Hugoを使ったポートフォリオ作成"},{"content":"はじめに 今回は，先日初めて見たファイル形式のExcel Binary Workbook (xlsb)に関して，pythonでcsvにパースする話です．\n.xlsxはよくあるExcelファイルの形式ですが，それのバイナリー形式である.xlsbに関しての話です．（今まで見たことなかった拡張子です笑）\nExcelの闇やExcelとの格闘は色々ありますが，今回はそこをグッと堪えて進めたいと思います笑\n.xlsbとは Weblio辞書によると以下のように記載されています．\n .xlsbとは，Excel 2007で作成したブックを「XML形式でないバイナリブック」として保存する際に用いられる拡張子である．.xlsbでブックを保存した場合はファイル全体がバイナリ形式で保存され，XMLベースである.xlsxなどのファイル形式で保存した場合と比べて，ファイルサイズを数分の1程度に抑えることができる．\n 受け取ったファイルは.xlsb形式でも100MBぐらいあったため，.xlsx形式だとかなり容量が大きく，ファイルを開くと処理が重たくなることが想像できるので，圧縮したのだと考えられます．\nExcelを扱えるpythonライブラリ openpyxl 定番のopenpyxlです．\n上記ページにも記載されてますが，Excelファイルの拡張子である.xlsxを扱うことができます．\n openpyxl is a Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files.\n いつものようにこのライブラリで処理しようとしたところ，下記のようなエラーが発生しました．\nopenpyxl.utils.exceptions.InvalidFileException: openpyxl does not support binary format .xlsb, please convert this file to .xlsx format if you want to open it with openpyxl  もう一度openpyxlの説明を見ると，確かに扱える拡張子はxlsx/xlsm/xltx/xltmとなっているので、.xlsbは扱えないのが分かります．\nそこで，.xlsbの拡張子が扱えるライブラリを調べたところ，pyxlsbというのがあるみたいなので，それを使うことにしました．\npyxlsb pyxlsbは公式の説明にあるように，xlsb形式を扱えるpythonライブラリになります．\n pyxlsb is an Excel 2007-2010 Binary Workbook (xlsb) parser for Python.\n インストールはpipですることができます．\npip install pyxlsb  公式のサンプルコードを記載しておきます．\nimport csv from pyxlsb import open_workbook with open_workbook('Book1.xlsb') as wb: for name in wb.sheets: with wb.get_sheet(name) as sheet, open(name + '.csv', 'w') as f: writer = csv.writer(f) for row in sheet.rows(): writer.writerow([c.v for c in row])  もしpandasのデータフレームに変換したい場合は，参考ページのコードで可能となります．\nただし，時刻変換に関して少し注意が必要なので，記載しておくと公式にもある通り，日付はfloatに変換されてしまうため，convert_date関数を使う必要があります．\n Note that dates will appear as floats. You must use the convert_date(date) method from the corresponding Workbook instance to turn them into datetime.\n なので，元のファイルに時刻が入っている場合には上記変換をコードの中に入れて処理する必要がありますのでご注意下さい．\nprint(wb.convert_date(41235.45578)) \u0026gt;\u0026gt;\u0026gt; datetime.datetime(2012, 11, 22, 10, 56, 19)  今回は個人的に嵌ってしまった.xlsb形式のファイルを扱う方法を紹介しましたが，出来ればデータ分析をするようなデータをExcelファイルで扱いたくないのが本音です．\nもちろん簡単なデータの可視化とか表計算とかExcelが活躍する場面は多々あると思うので，使い分けていきたいとは思います．\n参考 Stack Overflow - Read XLSB File in Pandas Python\n追記 2020-01-05 pandasのversion=1.0.0で.xlsbファイルをロードできるようになったみたいです．\n方法はpd.read_excelの引数でengine=\u0026quot;pyxlsb\u0026quot;と指定するだけです．\n# Returns a DataFrame pd.read_excel(\u0026quot;path_to_file.xlsb\u0026quot;, engine=\u0026quot;pyxlsb\u0026quot;)  参考: https://pandas.pydata.org/docs/user_guide/io.html#io-xlsb\n","date":"2019-10-05","permalink":"https://masatakashiwagi.github.io/portfolio/post/excel_processing_using_python/","tags":["data science"],"title":"Excel Binary WorkbookをPythonで処理"}]