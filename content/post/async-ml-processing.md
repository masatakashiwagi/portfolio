+++
author = "Masataka Kashiwagi"
title = "FastAPI ã¨ RabbitMQ ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã®éåŒæœŸå‡¦ç†"
description = "FastAPI ã¨ RabbitMQ ã‚’ç”¨ã„ã¦æ©Ÿæ¢°å­¦ç¿’ã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å‡¦ç†ã‚’éåŒæœŸåŒ–ã™ã‚‹è©±"
date = 2022-04-24T02:10:25+09:00
draft = false
share = true
showLicense = false
tags = ["Dev", "Machine Learning"]
+++

## ã¯ã˜ã‚ã«

æœ€è¿‘ï¼Œæ©Ÿæ¢°å­¦ç¿’ã‚’ä½¿ã£ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã©ã†ã„ã£ãŸå‡¦ç†ã‚’è¡Œã£ã¦ãƒ¢ãƒ‡ãƒ«ä½œæˆãªã©ã‚’è¡Œã£ã¦ã„ã‚‹ã‹æ°—ã«ãªã£ãŸã®ã§ï¼Œãƒ¢ãƒ‡ãƒ«ä½œæˆæ™‚ã«ã‚ˆãè¡Œã‚ã‚Œã‚‹éåŒæœŸå‡¦ç†ã‚’ FastAPI ã¨ RabbitMQ ã‚’ç”¨ã„ã¦æ¤œè¨¼ã—ãŸãŠè©±ã«ãªã‚Šã¾ã™ï¼

æ©Ÿæ¢°å­¦ç¿’ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ä½œæˆã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆï¼Œãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’è¡Œã†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¯¾ã—ã¦ï¼Œãã®æƒ…å ±ã‚’å—ã‘å–ã£ãŸã¨ã„ã†ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã ã‘ã‚’å…ˆã«è¿”ã—ï¼Œå®Ÿéš›ã®å‡¦ç†ã¯éåŒæœŸã§è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã¨æ€ã„ã¾ã™ï¼ã“ã®å‡¦ç†ã‚’ RabbitMQ ã¨ã„ã† OSS ã® message queuing service ã‚’ç”¨ã„ã¦å®Ÿæ–½ã—ãŸç´¹ä»‹ã«ãªã‚Šã¾ã™ï¼

ã‚ã¨å€‹äººçš„ã« RPC (Remote Procedure Call) ã‚„ Publish/Subscribe ã®ä»•çµ„ã¿ã‚’ç†è§£ã—ãŸã„ã¨ã„ã†æ°—æŒã¡ã‚‚ã‚ã‚Šã¾ã—ãŸï¼

ä»¥ä¸‹ã®ãƒªãƒã‚¸ãƒˆãƒªã«ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãªã©ã‚’ç½®ã„ã¦ã‚ã‚Šã¾ã™ï¼

<iframe class="hatenablogcard" style="width:100%;height:155px;max-width:680px;" title="teamaya/async-processing at main Â· masatakashiwagi/teamaya" src="https://hatenablog-parts.com/embed?url=https://github.com/masatakashiwagi/teamaya/tree/main/async-processing" width="300" height="150" frameborder="0" scrolling="no"></iframe>

æ¦‚è¦ã¨ã—ã¦ï¼Œä»¥ä¸‹ã®ã‚ˆã†ãªæµã‚Œã§å‡¦ç†ã‚’è¦‹ã¦ã„ãã¾ã—ãŸï¼

1. FastAPI ã« json å½¢å¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ POST ã™ã‚‹ï¼ˆãƒ¢ãƒ‡ãƒ«ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æƒ…å ±ï¼‰
2. `/train`ã¨ã„ã†ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ã¾ãšã¯ãƒ‡ãƒ¼ã‚¿ã‚’POSTã™ã‚‹
3. ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒè¡Œã‚ã‚Œãƒ¢ãƒ‡ãƒ«ã‚’ S3 ã«ä¿å­˜ã™ã‚‹
4. ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ãŸã‚‰ï¼Œæ¬¡ã«`/predict`ã¨ã„ã†ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’POSTã™ã‚‹
5. å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ S3 ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ï¼ŒPOST ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ç¢ºç‡ã‚’è¿”ã™

## Message Queuing Serviceã¨ã¯ï¼Ÿ

ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã¯ Producer ã¨å‘¼ã°ã‚Œã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒä½œæˆã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ã‘å–ã‚Šï¼Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæºœã¾ã£ã¦ã„ãä»•çµ„ã¿ã§ã™ï¼Producer å´ã‹ã‚‰è¦‹ã‚‹ã¨ï¼Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é…ä¿¡ã—ã¾ã™ï¼ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã™ã‚‹å½¹å‰²ã¨ã—ã¦ï¼ŒConsumer (Worker) ã¨å‘¼ã°ã‚Œã‚‹åˆ¥ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šï¼ŒConsumer ã¯ Queue ã«æ¥ç¶šã—ï¼Œå‡¦ç†ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ã—ã¾ã™ï¼å‡¦ç†ã—çµ‚ã‚ã£ãŸã‚‰ï¼Œè¿”ä¿¡ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã«é€ä¿¡ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼ï¼ˆQueue ã«å…¥ã‚Œã‚‰ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ï¼ŒConsumer ãŒå–ã‚Šå‡ºã™ã¾ã§ä¿å­˜ã•ã‚Œã¾ã™ï¼‰

ã¾ãŸï¼Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã«ã¯ Exchange ã¨å‘¼ã°ã‚Œã‚‹æ©Ÿèƒ½ãŒã‚ã‚Šï¼Œã©ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã©ã®ã‚ˆã†ã«é€ã‚‹ã‹ã‚’è¨­å®šã™ã‚‹æ©Ÿèƒ½ã‚‚ã‚ã‚Šã¾ã™ï¼ï¼ˆå³å¯†ã«ã¯ï¼ŒProducer ã¯ç›´æ¥ Queue ã«é€ä¿¡ã™ã‚‹ã®ã§ã¯ãªãï¼ŒExchange ã«é€ä¿¡ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ï¼‰

å®Ÿéš›ã«ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã®å½¹å‰²ã‚’æ‹…ã†ã‚‚ã®ã‚’ Broker ã¨å‘¼ã‚“ã ã‚Šã—ã¾ã™ï¼ã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦ [RabbitMQ](https://www.rabbitmq.com/) ã‚„ [Redis](https://redis.io/) ãªã©ãŒã‚ã‚Šï¼Œãƒãƒãƒ¼ã‚¸ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã§ã¯ [Amazon Simple Queue Service (SQS)](https://aws.amazon.com/sqs/) ãŒã‚ã‚Šã¾ã™ï¼

å‚è€ƒ: [What is message queuing?](https://www.cloudamqp.com/blog/what-is-message-queuing.html)

### RabbitMQã‚’ä½¿ã£ãŸå®Ÿè£…

ä»Šå›ã¯ RabbitMQ ã‚’ä½¿ã£ã¦å®Ÿè£…ã—ã¾ã—ãŸï¼RabbitMQ ã¯ OSS ã® Message Broker ã§å‹•ä½œãŒé€Ÿãè»½é‡ã§ï¼Œè¤‡æ•°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ³ã‚°ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼ã„ãã¤ã‹ã®è¨€èªã§å®Ÿè£…å¯èƒ½ã§ã™ãŒï¼Œpython ã§æ‰±ã†å ´åˆã«ã¯ï¼Œ`pika` ã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã“ã¨ã«ãªã‚Šã¾ã™ï¼

ä¾‹ãˆã°ï¼Œ[Celery](https://docs.celeryq.dev/en/stable/) ã®ã‚ˆã†ãªåˆ†æ•£ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã“ã¨ã§éåŒæœŸå‡¦ç†ã‚’ã‚ˆã‚Šç°¡å˜ã«å®Ÿè£…ã§ãã¾ã™ãŒï¼ŒCelery è‡ªä½“ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã¯ã§ããªã„ãŸã‚ï¼ŒRabbitMQ ã‚„ Redis ã®ã‚ˆã†ãª Broker ãŒå¿…è¦ã«ãªã‚Šã¾ã™ï¼ä»Šå›ã¯ã“ã®ã‚ãŸã‚Šã® pub/sub ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã« Celery ã¯ä½¿ã‚ãšã« RabbitMQ ã® python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚‹ pika ã‚’ä½¿ã£ã¦å®Ÿè£…ã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸï¼

RabbitMQ ã¯ docker ã‚³ãƒ³ãƒ†ãƒŠã§ç«‹ã¡ä¸Šã’ã¦ã„ã¦ï¼Œdefinitions.json ã¨ã„ã†å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’äº‹å‰ã«ç”¨æ„ã™ã‚‹ã“ã¨ã§ãã®ã‚¹ã‚­ãƒ¼ãƒã«åŸºã¥ã„ã¦ RabbitMQ ã‚’ç«‹ã¡ä¸Šã’ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•æ™‚ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ï¼

<details>
<summary>definitions.json</summary>

```json
{
  "rabbit_version": "3.9.14",
  "rabbitmq_version": "3.9.14",
  "product_name": "RabbitMQ",
  "product_version": "3.9.14",
  "users": [
    {
      "name": "guest",
      "password": "guest",
      "hashing_algorithm": "rabbit_password_hashing_sha256",
      "tags": "administrator",
      "limits": {}
    }
  ],
  "vhosts": [{ "name": "/" }],
  "permissions": [
    {
      "user": "guest",
      "vhost": "/",
      "configure": ".*",
      "write": ".*",
      "read": ".*"
    }
  ],
  "topic_permissions": [],
  "parameters": [],
  "global_parameters": [],
  "policies": [],
  "queues": [
    {
      "name": "queue.model.train",
      "vhost": "/",
      "durable": true,
      "auto_delete": false,
      "arguments": { "x-queue-type": "classic" }
    },
    {
      "name": "queue.model.predict",
      "vhost": "/",
      "durable": true,
      "auto_delete": false,
      "arguments": { "x-queue-type": "classic" }
    }
  ],
  "exchanges": [],
  "bindings": []
}
```

</details>

RabbitMQã«ã¯ä¸å¯§ãªTutorialsãŒã‚ã‚‹ã®ã§ï¼Œãã‚Œã‚’èª­ã‚€ã¨ç†è§£ãŒé€²ã‚€ã¨æ€ã„ã¾ã™ï¼

## ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

éåŒæœŸå‡¦ç†ã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆã¯å›³ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼Producer/Broker/Consumer ã¨ã‚³ãƒ³ãƒ†ãƒŠã‚’3ã¤ç”¨æ„ã—ã¦ã„ã¾ã™ï¼

å›³ã®å³ä¸‹ã«ã‚ã‚‹ Result Stores ã¯ã‚¿ã‚¹ã‚¯ã®å‡¦ç†çµæœã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã«ãªã‚Šã¾ã™ï¼Result Stores ã«ã¯ PostgreSQL ã‚„ MySQL ãªã©ã®DBã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã—ï¼ŒRedis ã‚‚ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼Redis ã¯ Broker ã¨ã—ã¦ã‚‚ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ï¼Œä¸¡æ–¹ã‚’1ã¤ã§æ‹…ã†ã“ã¨ãŒå¯èƒ½ã§ã™ï¼ä»Šå›ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ S3 ã«ä¿å­˜ã™ã‚‹ã ã‘ã¨ã—ã¦ï¼Œå‡¦ç†çµæœã‚’ DB ã«ä¿å­˜ã—ãŸã‚Šã¯ã—ã¦ã„ãªã„ã§ã™ï¼

- Producer: æ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’è¡Œã†ãŸã‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒã‚¹ãƒˆã™ã‚‹ã‚³ãƒ³ãƒ†ãƒŠï¼ˆ=FastAPIï¼‰
- Broker: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã®å½¹å‰²ã‚’æ‹…ã†ã‚³ãƒ³ãƒ†ãƒŠï¼ˆ=RabbitMQï¼‰
- Consumer: ã‚¿ã‚¹ã‚¯ã‚’å®Ÿéš›ã«å®Ÿè¡Œã™ã‚‹ã‚³ãƒ³ãƒ†ãƒŠ
- Storage: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆ=S3ï¼‰

![ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼](../../img/teamaya-async-img1.png "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼")

docker-compose.yml ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹æˆã«ãªã‚Šã¾ã™ï¼

<details>
<summary>docker-compose.yml</summary>

```dockerfile
version: '3.8'

# Common definition
x-template: &template
  volumes:
      - ~/.gcp:/root/.gcp:cached
      - ~/.aws:/root/.aws:cached
      - ./app:/opt/program:cached
  env_file:
      - .env
  environment:
      TZ: Asia/Tokyo
      LANG: 'ja_JP.UTF-8'
  restart: always
  tty: true

services:
  producer:
    # FastAPI for producer
    container_name: producer
    build:
      context: .
    ports:
      - 5000:5000
    command: ["uvicorn", "main:app", "--reload", "--host", "0.0.0.0", "--port", "5000", "--access-log"]
    depends_on:
      - rabbitmq
    <<: *template

  consumer:
    container_name: consumer
    hostname: consumer
    build:
      context: .
    command: ["python3", "consumer/consumer.py", "--num_threads", "2"]
    depends_on:
      - rabbitmq
    <<: *template

  rabbitmq:
    image: rabbitmq:3.9-management
    container_name: rabbitmq
    hostname: rabbitmq
    restart: always
    volumes:
      # - ./app/rabbitmq/etc:/etc/rabbitmq/rabbitmq
      - ./app/rabbitmq/etc/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
      - ./app/rabbitmq/etc/definitions.json:/etc/rabbitmq/definitions.json
      - ./app/rabbitmq/data:/var/lib/rabbitmq
      - ./app/rabbitmq/logs:/var/log/rabbitmq
      - ~/.aws:/root/.aws:cached
    ports:
      # AMQP protocol port
      - 5672:5672
      # HTTP management UI
      - 15672:15672
    environment:
      TZ: Asia/Tokyo
      LANG: 'ja_JP.UTF-8'
    env_file:
      - .env

# networks:
#   default:
#     external:
#       name: teamaya-network-async
```

</details>

ä»Šå›ã®ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã¯ä»¥ä¸‹ã«ãªã‚Šã¾ã™ï¼å°‘ã—å†—é•·ãªæ§‹æˆã«ãªã£ã¦ã„ã¾ã™ãŒï¼Œ`./app/producer` ã¨ `./app/consumer` é…ä¸‹ã« Producer ã¨ Consumer ã®å‡¦ç†ã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚ã‚Šã¾ã™ï¼

```text
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ consumer
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ consumer.py
â”‚   â”‚   â””â”€â”€ tasks.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ producer
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ producer.py
â”‚   â”‚   â””â”€â”€ schema.py
â”‚   â”œâ”€â”€ rabbitmq
â”‚   â”‚   â””â”€â”€ etc
â”‚   â”‚       â”œâ”€â”€ definitions.json
â”‚   â”‚       â””â”€â”€ rabbitmq.conf
â”‚   â””â”€â”€ test
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ conftest.py
â”‚       â”œâ”€â”€ data
â”‚       â”‚   â””â”€â”€ test_diabetes.csv
â”‚       â””â”€â”€ unit
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ test_tasks.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.lock
â””â”€â”€ requirements.txt
```

## Producerã®å®Ÿè£…

ãã‚Œãã‚Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ã‚’ã—ã¦ãŠãã¨ï¼Œ

- `base.py`: RabbitMQ ã«æ¥ç¶šã™ã‚‹ãŸã‚ã®åˆæœŸåŒ–ã‚„ Consumer ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹ãŸã‚ã®å‡¦ç†ã‚’å®Ÿè£…ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
- `producer.py`: `base.py` ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã¦ï¼Œå€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ã¦é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å®Ÿè¡Œã™ã‚‹ API ã‚’å®Ÿè£…ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
- `schema.py`: ãƒ‡ãƒ¼ã‚¿ã®å…¥å‡ºåŠ›ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
  - FastAPI ã§ã¯å…¥å‡ºåŠ›ã‚’ `Pydantic` ã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¦ Data validation ã‚’è¡Œã„ã¾ã™ï¼å‹ãƒ’ãƒ³ãƒˆã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ã«ãªã‚Šã¾ã™

<details>
<summary>producer.py</summary>

```python
import ast
import uuid

from fastapi import APIRouter
from logger import get_logger

from producer.base import BaseProducer, QueueNames, RepQueueNames
from producer.schema import ApiSchemaPredict, ApiSchemaTrain, ProducerResult

LOGGER = get_logger()

router = APIRouter(prefix='', tags=["producers"])


class ProducerTrain(BaseProducer):
    def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames):
        BaseProducer.__init__(self, queue_name, rep_queue_name)

    def run(self, params: ApiSchemaTrain):
        """Run to send message to train consumer

        Args:
            params (ApiSchemaTrain): schema for train
        """
        model_id = str(uuid.uuid4())
        message = {
            "model_id": model_id,
            "dataset_id": params.dataset_id,
            "features": params.features,
            "target": params.target
        }

        # self.send_message_to_consumer(message)
        LOGGER.info("Produce message for train.")
        response = self.send_message_to_consumer(message)
        response = ast.literal_eval(response.decode())
        LOGGER.info(f"Reply Response from consumer: {response}")

        return ProducerResult(message=response)


class ProducerPredict(BaseProducer):
    def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames):
        BaseProducer.__init__(self, queue_name, rep_queue_name)

    def run(self, params: ApiSchemaPredict):
        """Run to send message to predict consumer

        Args:
            params (ApiSchemaPredict): schema for predict
        """
        message = {
            "model_id": params.model_id,
            "dataset_id": params.dataset_id,
            "input_data": params.input_data
        }

        LOGGER.info("Produce message for predict.")
        response = self.send_message_to_consumer(message)
        response = ast.literal_eval(response.decode())
        LOGGER.info(f"Reply Response from consumer: {response}")

        return ProducerResult(message=response)


@router.post("/train", response_model=ProducerResult, name="train")
async def train(params: ApiSchemaTrain) -> ProducerResult:
    """Train model"""
    return ProducerTrain(queue_name='queue.model.train', rep_queue_name='queue.reply.train').run(params)


@router.post("/predict", response_model=ProducerResult, name="predict")
async def predict(params: ApiSchemaPredict) -> ProducerResult:
    """Predict model"""
    return ProducerPredict(queue_name='queue.model.predict', rep_queue_name='queue.reply.predict').run(params)
```

</details>

`ProducerTrain` ã¨ `ProducerPredict` ã¯ã‚¿ã‚¹ã‚¯å®Ÿè¡Œç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹ã‚­ãƒ¥ãƒ¼ã® `queue_name` ã¨ Consumer å´ã‹ã‚‰ã®è¿”ä¿¡ç”¨ã®ã‚­ãƒ¥ãƒ¼ã§ã‚ã‚‹ `rep_queue_name` ã®2ã¤ã‚’å¼•æ•°ã«å–ã‚Šã¾ã™ï¼å®Ÿéš›ã®å­¦ç¿’ã‚„äºˆæ¸¬å‡¦ç†ã‚’è¡Œã†éƒ¨åˆ†ã¯ Consumer å´ã§å®Ÿè£…ã—ã¦ã„ã¾ã™ï¼

<details>
<summary>base.py</summary>

```python
import json
import os
import uuid
from typing import Literal

import pika
from logger import get_logger

LOGGER = get_logger()

# Possible values as queue name
QueueNames = Literal['queue.model.train', 'queue.model.predict']
RepQueueNames = Literal['queue.reply.train', 'queue.reply.predict']


class BaseProducer:
    def __init__(self, queue_name: QueueNames, rep_queue_name: RepQueueNames):
        self.queue_name = queue_name
        self.rep_queue_name = rep_queue_name
        self.pika_params = pika.ConnectionParameters(
            host="rabbitmq",
            port=os.getenv('RABBITMQ_PORT', 5672),
            connection_attempts=10,
            heartbeat=0
        )
        self.connection = pika.BlockingConnection(self.pika_params)
        self.channel = self.connection.channel()
        LOGGER.info('Pika connection initialized.')

        result = self.channel.queue_declare(queue=self.rep_queue_name, exclusive=True)
        self.callback_queue = result.method.queue
        self.channel.basic_consume(queue=self.callback_queue, on_message_callback=self.on_response, auto_ack=True)

    def on_response(self, ch, method, props, body):
        if self.corr_id == props.correlation_id:
            self.response = body

    def run(self):
        raise NotImplementedError()

    def send_message_to_consumer(self, message: dict):
        """Send message

        Args:
            message (dict): message info
        """
        self.response = None
        self.corr_id = str(uuid.uuid4())
        message_json = json.dumps(message)

        self.channel.basic_publish(
            exchange="",
            routing_key=self.queue_name,
            body=message_json,
            properties=pika.BasicProperties(
                content_type='application/json',
                delivery_mode=2,  # make message persistent
                reply_to=self.callback_queue,
                correlation_id=self.corr_id
            )
        )

        LOGGER.info(f"Sent message. [q] '{self.queue_name}' [x] Body: {message_json=}")

        while self.response is None:
            self.connection.process_data_events()

        self.close()
        return self.response

    def close(self):
        self.channel.close()
        self.connection.close()
```

</details>

- `__init__` é–¢æ•°:
  - RabbitMQ ã®ã‚µãƒ¼ãƒãƒ¼ã¨æ¥ç¶šã™ã‚‹ãŸã‚ã« `pika.BlockingConnection()` ã§ `host` , `port` ãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã‚’è¡Œã†
  - Consumer ã‹ã‚‰ã® Reply ç”¨ã« rep_queue_name ã«æŒ‡å®šã—ãŸã‚­ãƒ¥ãƒ¼åã§ callback_queue ã‚’ä½œæˆ
  - basic_consume ã§ã¯ subscribe ã™ã‚‹ã‚­ãƒ¥ãƒ¼ãŒå­˜åœ¨ã™ã‚Œã°ãã‚Œã‚’å®Ÿè¡Œ

- `send_message_to_consumer` é–¢æ•°:
  - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ json.dump ã—ï¼Œbasic_publish ã® body ã«ã¤ã‚ã¦ Exchange ã«é€ã‚‹

## Consumer ã®å®Ÿè£…

ãã‚Œãã‚Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ã‚’ã—ã¦ãŠãã¨ï¼Œ

- `base.py`: RabbitMQ ã«æ¥ç¶šã—ã¦ã‚­ãƒ¥ãƒ¼ã«ã‚ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ã—ï¼Œå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
  - callback éƒ¨åˆ†ã¯ `tasks.py` ã§å®Ÿè£…ã—ã¦ã„ã¾ã™
- `consumer.py`: ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æ±ºã‚ã‚‹ `num_threads` ã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã«å–ã‚Šï¼Œã‚³ãƒ³ãƒ†ãƒŠä¸Šã§ã¯ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Ÿè¡Œã•ã‚Œã¾ã™
- `tasks.py`: æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚„å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦äºˆæ¸¬ã‚’è¡Œã†å‡¦ç†ã‚’å®Ÿè£…ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
  - callback ãƒ¡ã‚½ãƒƒãƒ‰ã«å®Ÿè¡Œã—ãŸã„å‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã™
  - `if __name__ == "__main__":` ä»¥ä¸‹ã«ã¯ [Continuous Machine Learning (CML)](https://cml.dev/) ã§åˆ©ç”¨ã™ã‚‹ CT ç”¨ã®å‡¦ç†ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ï¼ï¼ˆCML ã«é–¢ã—ã¦ã¯åˆ¥ã§ãƒ–ãƒ­ã‚°ã‚’æ›¸ã“ã†ã¨æ€ã„ã¾ã™ï¼‰

<details>
<summary>tasks.py</summary>

```python
import json
import os
import sys
import traceback
from typing import Any, Dict

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pika
from logger import get_logger
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

from base import BaseConsumer, EvalMetrics, QueueNames

LOGGER = get_logger()
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')
S3_PATH_NAME = os.getenv('S3_PATH_NAME')
S3_MODEL_PATH_NAME = os.getenv('S3_MODEL_PATH_NAME')


class TrainConsumer(BaseConsumer):
    def __init__(self, queue_name: QueueNames):
        BaseConsumer.__init__(self, queue_name)

    def callback(self, ch, method, props, body):
        params = self.body2dict(body)

        payload = {
            'status': 'TASK_RECEIVED',
            'model_id': params['model_id']
        }
        response = json.dumps(payload)

        ch.basic_publish(
            exchange='',
            routing_key=props.reply_to,
            properties=pika.BasicProperties(correlation_id=props.correlation_id),
            body=response
        )
        # ch.basic_ack(delivery_tag=method.delivery_tag)

        self.download_from_s3(S3_BUCKET_NAME, S3_PATH_NAME, 'data/', params['dataset_id'] + '.csv')
        LOGGER.info("Download dataset from S3.")

        dataset_path = 'data/' + params['dataset_id'] + '.csv'
        df = pd.read_csv(dataset_path)
        LOGGER.info("Read csv file and transform to dataframe.")

        try:
            result = train(df, params)

            # save model
            model_path = 'data/model.pkl'
            self.save_model(result['model'], model_path)
            LOGGER.info("Save trained model to local.")

            # upload model to cloud storage
            model_id = params['model_id']
            self.upload_to_s3(S3_BUCKET_NAME, S3_MODEL_PATH_NAME + f'{model_id}/', 'data/', 'model.pkl')
            LOGGER.info("Upload trained model to S3.")
            LOGGER.info("TASK_COMPLETED")
        except Exception as e:
            _, _, tb = sys.exc_info()
            LOGGER.error(
                f"Exception Error: {e} || Type: {str(type(e))} || Traceback Message: {traceback.format_tb(tb)}")
            LOGGER.error("TASK_ERROR")


class PredictConsumer(BaseConsumer):
    def __init__(self, queue_name: QueueNames):
        BaseConsumer.__init__(self, queue_name)

    def callback(self, ch, method, props, body):
        params = self.body2dict(body)
        model_id = params['model_id']
        self.download_from_s3(S3_BUCKET_NAME, S3_MODEL_PATH_NAME + f'{model_id}/', 'data/', 'model.pkl')
        LOGGER.info("Download model file from S3.")

        model_path = 'data/model.pkl'
        model = self.load_model(model_path)
        LOGGER.info("Load model for prediction.")

        try:
            result = predict(model, params)

            payload = {
                'status': 'TASK_COMPLETED',
                'pred_proba': result['pred_proba']
            }
            response = json.dumps(payload)
        except Exception as e:
            _, _, tb = sys.exc_info()
            LOGGER.error(
                f"Exception Error: {e} || Type: {str(type(e))} || Traceback Message: {traceback.format_tb(tb)}")

            payload = {
                'status': 'TASK_ERROR',
                'pred_proba': None
            }
            response = json.dumps(payload)

        ch.basic_publish(
            exchange='',
            routing_key=props.reply_to,
            properties=pika.BasicProperties(correlation_id=props.correlation_id),
            body=response
        )
        # ch.basic_ack(delivery_tag=method.delivery_tag)


def train(df: pd.DataFrame, params: dict) -> Dict[str, Any]:
    """Train machine learning model (RandomForestRegressor)

    Args:
        df (pd.DataFrame): dataset for training model
        params (dict): parameters for training
    """
    features = params['features']
    target = params['target']
    X, y = df[features], df[target].values

    # train/test split
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

    LOGGER.info("Start model training.")
    # machine learning model: RandomForestRegressor
    reg_model = RandomForestRegressor(max_depth=3, random_state=42, n_estimators=100)
    reg_model.fit(X_train, y_train)
    LOGGER.info("Model fit for training.")

    # evaluate model
    pred = reg_model.predict(X_valid)

    # evaluate metrics
    eval_metrics = EvalMetrics()
    rmse = eval_metrics.rmse_score(y_valid, pred)
    LOGGER.info("Evaluate metrics=RMSE for valid dataset : %.3f" % rmse)
    LOGGER.info("Finish model training.")

    result = {
        'y_pred': pred,
        'y_true': y_valid,
        'metrics': {'rmse': rmse},
        'model': reg_model
    }

    return result


def predict(model: object, params: dict) -> Dict[str, Any]:
    """Prediction for dataset using trained model

    Args:
        model (object): trained model
        params (dict): parameters for prediction

    Returns:
        float: predict probability
    """
    input_data = params['input_data']
    pred_proba = model.predict(pd.DataFrame([input_data]))
    result = {
        'pred_proba': pred_proba[0]
    }

    return result
```

</details>

- å­¦ç¿’ãƒ‘ãƒ¼ãƒˆ
  - ä»Šå›ï¼Œæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯ä½•ã§ã‚‚ã‚ˆã‹ã£ãŸã®ã§ï¼ŒRandomForest ã§å›å¸°ã‚’è¡Œã†å‡¦ç†ã«ã—ã¦ã„ã¾ã™
  - å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ S3 ã«ä¿å­˜ã—ã¦ã„ã‚‹ã®ã§ï¼Œã“ã®å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã¯ `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã«è‡ªèº«ã§åˆ©ç”¨ã—ã¦ã„ã‚‹ AWS ã®ãƒã‚±ãƒƒãƒˆæƒ…å ±ãªã©ã‚’è¼‰ã›ã¦ä¸‹ã•ã„

```bash
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')
S3_PATH_NAME = os.getenv('S3_PATH_NAME')
S3_MODEL_PATH_NAME = os.getenv('S3_MODEL_PATH_NAME')
```

ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚ã®ãƒ¡ã‚¿æƒ…å ±ã‚‚ DB ã«æ®‹ã—ã¦ãŠãã®ãŒè‰¯ã„ã¨æ€ã„ã¾ã™ãŒï¼Œä»Šå›ã¯ãã®éƒ¨åˆ†ã¯å®Ÿè£…ã—ã¦ã„ãªã„ã§ã™ğŸ™

- äºˆæ¸¬ãƒ‘ãƒ¼ãƒˆ
  - S3 ã«ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ï¼Œä¸ãˆã‚‰ãŸãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã¾ã™
  - ãƒ¢ãƒ‡ãƒ« ID ã¯å­¦ç¿’æ™‚ã«ç™ºè¡Œã•ã‚ŒãŸ UUID ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦è²¼ã‚Šä»˜ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã™ãŒï¼Œå‡ºåŠ›ã•ã‚ŒãŸãƒ­ã‚°ã‹ã‚‰æ‹¾ã†ã®ã§ã¡ã‚‡ã£ã¨ã„ã‘ã¦ãªã„ã§ã™ãŒãƒ¢ãƒƒã‚¯ãªã®ã§ã”å‹˜å¼ã‚’...

<details>
<summary>consumer.py</summary>

```python
from concurrent.futures import ThreadPoolExecutor

import click

import tasks


@click.command()
@click.option("--num_threads", type=int, help='the number of threads', default=1)
@click.option("--max_workers", type=int, help='the number of max workers', default=None)
def main(num_threads: int, max_workers: int):
    # Consumer execution
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for _ in range(num_threads):
            for task in [
                tasks.TrainConsumer(queue_name='queue.model.train'),
                tasks.PredictConsumer(queue_name='queue.model.predict')
            ]:
                executor.submit(task.run)


if __name__ == "__main__":
    main()
```

</details>

å¼•æ•°ã«æŒ‡å®šã—ãŸã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã«å¿œã˜ã¦ Consumer ãŒè¤‡æ•°ç«‹ã¡ä¸ŠãŒã‚Šã¾ã™ï¼

## å®Ÿè¡Œçµæœ

`docker compose up` ã§ã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ã—ã¦ï¼Œ`http://localhost:5000/docs` ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ Swagger ã«ã‚ˆã‚‹è¡¨ç¤ºãŒã•ã‚Œã¾ã™ï¼FastAPI ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ OpenAPI ã‚’è‡ªå‹•ç”Ÿæˆã—ã¦ãã‚Œï¼ŒSwagger ã‚„ ReDoc ã§è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼

ã“ã®è¾ºã¯å€‹äººçš„ã«ã¨ã¦ã‚‚ä¾¿åˆ©ã ãªã¨æ€ã£ã¦ã„ã¦ï¼Œãƒ‡ãƒ¼ã‚¿ã‚’ç°¡å˜ã« GET/POST ã™ã‚‹ã“ã¨ã§å‹•ä½œã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ãã¾ã™ï¼

- Swagger ã®ç”»é¢

![Swagger ã®ç”»é¢](../../img/teamaya-async-img2.png "Swagger")

- ReDoc ã®ç”»é¢

![ReDoc ã®ç”»é¢](../../img/teamaya-async-img3.png "ReDoc")

### å­¦ç¿’ç·¨

`/train` ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ POST ã—ã¾ã™ï¼äº‹å‰ã« S3 ã«ä¿å­˜ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ `dataset_id` ã«ï¼Œä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ï¼ˆèª¬æ˜å¤‰æ•°ï¼‰ã‚’ `features` ã«ï¼Œç›®çš„å¤‰æ•°ã‚’ `target` ã«æŒ‡å®šã—ã¾ã™ï¼

```bash
curl -X 'POST' \
  'http://localhost:5000/train' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "dataset_id": "diabetes",
  "features": ["age", "bmi", "bp", "s1", "s2", "s3", "s4", "s5", "s6"],
  "target": "target"
}'
```

å‡ºåŠ›ã•ã‚Œã‚‹ãƒ­ã‚°ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ï¼

- 4è¡Œç›®ã¯ Producer ãŒã‚­ãƒ¥ãƒ¼ã«å¯¾ã—ã¦é€ä¿¡ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆConsumer ã«æ¸¡ã—ãŸã„æƒ…å ±ï¼‰
- 7è¡Œç›®ï¼ˆä¸­ç•¥å¾Œ1è¡Œç›®ï¼‰ã¯ Consumer ã‹ã‚‰ã® Reply ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- æœ€å¾Œã® Consumer ã‹ã‚‰ã®ãƒ­ã‚°ã¯å­¦ç¿’å‡¦ç†ã‚’å®Ÿè¡Œä¸­ã«å‡ºåŠ›ã•ã‚Œã‚‹ãƒ­ã‚°

```text
producer  | [2022-04-24 15:49:09] [ INFO] Created channel=1
producer  | [2022-04-24 15:49:09] [ INFO] Pika connection initialized.
producer  | [2022-04-24 15:49:09] [ INFO] Produce message for train.
producer  | [2022-04-24 15:49:09] [ INFO] Sent message. [q] 'queue.model.train' [x] Body: message_json='{"model_id": "c7632288-442d-44c5-9102-31ccda2af6b7", "dataset_id": "diabetes", "features": ["age", "bmi", "bp", "s1", "s2", "s3", "s4", "s5", "s6"], "target": "target"}'
consumer  | [2022-04-24 15:49:09] [ INFO] Convert message to dict type.
~ä¸­ç•¥~
producer  | [2022-04-24 15:49:09] [ INFO] Reply Response from consumer: {'status': 'TASK_RECEIVED', 'model_id': 'c7632288-442d-44c5-9102-31ccda2af6b7'}
producer  | INFO:     172.24.0.1:57222 - "POST /train HTTP/1.1" 200 OK
rabbitmq  | 2022-04-24 06:49:09.367236+00:00 [info] <0.5900.0> closing AMQP connection <0.5900.0> (172.24.0.4:46266 -> 172.24.0.2:5672, vhost: '/', user: 'guest')
consumer  | [2022-04-24 15:49:09] [ INFO] Found credentials in shared credentials file: ~/.aws/credentials
consumer  | [2022-04-24 15:49:09] [ INFO] Download dataset from S3.
consumer  | [2022-04-24 15:49:09] [ INFO] Read csv file and transform to dataframe.
consumer  | [2022-04-24 15:49:09] [ INFO] Start model training.
consumer  | [2022-04-24 15:49:09] [ INFO] Model fit for training.
consumer  | [2022-04-24 15:49:09] [ INFO] Evaluate metrics=RMSE for valid dataset : 53.039
consumer  | [2022-04-24 15:49:09] [ INFO] Finish model training.
consumer  | [2022-04-24 15:49:09] [ INFO] Save trained model to local.
consumer  | [2022-04-24 15:49:10] [ INFO] Upload trained model to S3.
consumer  | [2022-04-24 15:49:10] [ INFO] TASK_COMPLETED
```

### äºˆæ¸¬ç·¨

`/predict` ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ POST ã—ã¾ã™ï¼`model_id` ã‚’å…ƒã«å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ S3 ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼`input_data` ã«ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã§ç‰¹å¾´é‡ã¨ãã®å€¤ã¨ã„ã†çµ„ã§æ¸¡ã—ã¾ã™ï¼<br>
â€» `model_id` ã¯å­¦ç¿’ç·¨ã®ãƒ­ã‚°å‡ºåŠ›ã«ã‚ã‚‹ `model_id` ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼

```bash
curl -X 'POST' \
  'http://localhost:5000/predict' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "model_id": "c7632288-442d-44c5-9102-31ccda2af6b7",
  "dataset_id": "diabetes",
  "input_data": {
    "age": 0.038076,
    "bmi": 0.061696,
    "bp": 0.021872,
    "s1": -0.044223,
    "s2": -0.034821,
    "s3": -0.043401,
    "s4": -0.002592,
    "s5": 0.019908,
    "s6": -0.017646
  }
}'
```

å‡ºåŠ›ã•ã‚Œã‚‹ãƒ­ã‚°ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ï¼

- 4è¡Œç›®ã¯ Producer ãŒã‚­ãƒ¥ãƒ¼ã«å¯¾ã—ã¦é€ä¿¡ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆConsumer ã«æ¸¡ã—ãŸã„æƒ…å ±ï¼‰
- 5è¡Œç›®ã® Consumer ã‹ã‚‰ã®ãƒ­ã‚°ã¯äºˆæ¸¬å‡¦ç†ã‚’å®Ÿè¡Œä¸­ã«å‡ºåŠ›ã•ã‚Œã‚‹ãƒ­ã‚°
- ï¼ˆä¸­ç•¥å¾Œ1è¡Œç›®ï¼‰ã¯ Consumer ã‹ã‚‰ã® Reply ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ï¼Œäºˆæ¸¬çµæœãŒå…¥ã£ã¦ã„ã¾ã™

```text
producer  | [2022-04-24 18:00:16] [ INFO] Created channel=1
producer  | [2022-04-24 18:00:16] [ INFO] Pika connection initialized.
producer  | [2022-04-24 18:00:16] [ INFO] Produce message for predict.
producer  | [2022-04-24 18:00:16] [ INFO] Sent message. [q] 'queue.model.predict' [x] Body: message_json='{"model_id": "c7632288-442d-44c5-9102-31ccda2af6b7", "dataset_id": "diabetes", "input_data": {"age": 0.038076, "bmi": 0.061696, "bp": 0.021872, "s1": -0.044223, "s2": -0.034821, "s3": -0.043401, "s4": -0.002592, "s5": 0.019908, "s6": -0.017646}}'
consumer  | [2022-04-24 18:00:16] [ INFO] Convert message to dict type.
consumer  | [2022-04-24 18:00:16] [ INFO] Download model file from S3.
consumer  | [2022-04-24 18:00:16] [ INFO] Load model for prediction.
~ä¸­ç•¥~
producer  | [2022-04-24 18:00:16] [ INFO] Reply Response from consumer: {'status': 'TASK_COMPLETED', 'pred_proba': 208.6445780005619}
rabbitmq  | 2022-04-24 09:00:16.565636+00:00 [info] <0.8348.0> closing AMQP connection <0.8348.0> (172.24.0.4:46664 -> 172.24.0.2:5672, vhost: '/', user: 'guest')
producer  | INFO:     172.24.0.1:57624 - "POST /predict HTTP/1.1" 200 OK
```

## ãŠã‚ã‚Šã«

FastAPI ã¨ RabbitMQ ã‚’ç”¨ã„ã¦ WebAPI å½¢å¼ã§ï¼Œæ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã®éåŒæœŸå‡¦ç†ã‚’è¡Œã†æ¤œè¨¼ã‚’ã—ã¾ã—ãŸï¼éåŒæœŸå‡¦ç†ã ã£ãŸã‚Šï¼ŒRPC ã‚„ Pub/Sub ã®ä»•çµ„ã¿ã‚’å°‘ã—ã¯ç†è§£ã§ããŸã‹ãªã¨æ€ã„ã¾ã™ï¼

ä»Šå›ã¯ DB ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ãŸã‚Š DB å‘¨ã‚Šã®å‡¦ç†ã¯å®Ÿè£…ã—ã¦ã„ãªã„ã®ã§ï¼Œã“ã®è¾ºã‚‚æ™‚é–“ãŒã‚ã‚Œã°å®Ÿè£…ã§ãã‚Œã°ã¨æ€ã„ã¾ã™...

éåŒæœŸå‡¦ç†ã‚’è¡Œã†ä¸Šã§ãƒ¡ã‚¤ãƒ³ã®å½¹å‰²ã‚’æœãŸã—ãŸ RabbitMQ ã«ã¤ã„ã¦ã‚‚ã‚³ãƒ¡ãƒ³ãƒˆã™ã‚‹ã¨ï¼ŒOSS ã§ç°¡å˜ã«éåŒæœŸå‡¦ç†ã‚’è¡Œãˆã‚‹ä¾¿åˆ©ãªæŠ€è¡“ã ã¨æ€ã„ã¾ã™ï¼Producer/Exchange/Queue/Consumer ã®é–¢ä¿‚æ€§ã‚‚ Tutorials ã®å›³ãªã©ã§ã‚¤ãƒ¡ãƒ¼ã‚¸ã—ã‚„ã™ããªã‚‹ã®ã§ï¼Œã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ãªãŒã‚‰æ¯”è¼ƒçš„å®¹æ˜“ã«å®Ÿè£…ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸï¼<br>
ä¸€æ–¹ã§ï¼ŒConsumer ã‹ã‚‰ Producer ã« Reply ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹å ´åˆã«ï¼Œã©ã®ã‚ˆã†ã«å®Ÿè£…ã™ã‚Œã°ã„ã„ã‹ãŒåˆ†ã‹ã‚Šã¥ã‚‰ãï¼Œå€‹äººçš„ã«ã¯ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆã§ã—ãŸï¼

ã‚ã¨ï¼Œãªã«ã’ã« FastAPI ã‚‚ã»ã¨ã‚“ã©ä½¿ã£ãŸã“ã¨ãªã‹ã£ãŸã®ã§è‰¯ã„å‹‰å¼·ã«ãªã‚Šã¾ã—ãŸï¼

æœ€å¾Œã«ä»Šå¾Œã‚„ã‚ŠãŸã„ã“ã¨ã«ã¤ã„ã¦åˆ—æŒ™ã—ã¦ãŠãã¨...

- AWS SQS ã‚’ä½¿ã£ãŸéåŒæœŸå‡¦ç†ã®å®Ÿè£…
- Celery ã‚’ä½¿ã£ãŸéåŒæœŸå‡¦ç†ã®å®Ÿè£…
- Broker ã¨ã—ã¦ Redis ã‚’ç”¨ã„ãŸå®Ÿè£…
- DB ã‚’ä½¿ã£ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
- etc...

## å‚è€ƒ

- [FastAPI](https://fastapi.tiangolo.com/)
- [RabbitMQ](https://www.rabbitmq.com/)
  - [RabbitMQ Tutorials](https://www.rabbitmq.com/getstarted.html)
- [Asynchronous message-based communication](https://docs.microsoft.com/en-us/dotnet/architecture/microservices/architect-microservice-container-applications/asynchronous-message-based-communication)
- [Deep Learning: Scaling your neural networks with containerization and a message broker](https://medium.com/@si.allen/part-1-deep-learning-scaling-your-neural-networks-with-containerization-and-a-message-broker-d9c872a8345b)
- [katanaml/katana-skipper](https://github.com/katanaml/katana-skipper)
